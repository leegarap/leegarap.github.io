[
  
  {
    "title"    : "Improving Electronic Conductance Calculations with a Monte Carlo Approach",
    "category" : "Project",
    "url"      : "/project/2023/08/20/Electronic-Transport-Monte-Carlo/",
    "date"     : "August 20, 2023",
    "excerpt"  : "This is a part of research I am current working with Ethan Lister under supervision of Dr. Baruch Feldman at the University of Washington.\n\nThis post covers an overview of the research in informal language.\n\nThis research had been presented in UW ...",
    "content"  : "This is a part of research I am current working with Ethan Lister under supervision of Dr. Baruch Feldman at the University of Washington.\n\nThis post covers an overview of the research in informal language.\n\nThis research had been presented in UW Undergraduate Research Symposium 2023. The abstract for the presentation can be found at this link\n\n\nAbstract\n\n\nFor investigations into nanoscopic properties of matter, computational simulation is a key tool. In particular, simulations of the electronic configuration and conductance of nanoscale devices facilitate the continued miniaturization of semiconductor devices used in integrated circuits and computer processors. However, due to the importance of quantum mechanics at these scales, accurate calculations can be highly costly. In our research we consider improvements to one such parallelizable electronic transport code, TRANSEC. We seek to better understand how a Monte Carlo technique, combined with a special polynomial expansion, may improve the scaling of TRANSEC’s computing time with the size of the simulation. We apply the Monte Carlo technique within a simplified tight-binding model of a TRANSEC calculation. We test how the Monte Carlo technique facilitates the calculation of the electronic transmission probability, given an initial Hamiltonian energy matrix along with absorbing boundary conditions (referred to as complex absorbing potentials, or CAPs). We expect that the results of this study may provide algorithms which allow for faster calculation times when integrated at scale into TRANSEC. Improvements to computing time for determining parameters such as nanoscopic conduction helps to advance computer modeling of nanoscopic structures (such as transistors, interconnect, or molecular electronics), which could benefit semiconductor technology.\n\nBackground\n\nWe consider electronic conductance from the first principal level which can formulated as\n\n $I = \\frac{2e}{h} \\int_{-\\infty}^{\\infty} T(E) [f(E - \\mu_{L}) - f(E- \\mu_{R})] dE$\n\nwhere $f$ is a Fermi Distribution, $h$ is Plack’s constant, e is electronic charge. $T(E)$ is a transition probability, which is our main interest. $T(E) \\sim |G^r(E)|^2$ defined by\n\n\n $G^{r}(E) =  [E \\mathbf{1} - H_{op} - i\\eta]^{-1}$\n\n\nIn this project, we are interested in $T(E)$.\n\nDr. Feldman, our research mentor, employed a real space grid to solve for $G^r(E)$ and calculate $T(E)$. The method exhibits several advantages, including, but not limited to, being iterative, parallelizable, and straightforward.\n\nMethodology\n\nAt the symposium, we presented the approximation of $T(E)$ using the Monte Carlo method, Faber Polynomial Expansion, and the Iterative method. We applied the polynomial expansion and the iterative method separately.\n\n\n\n\nFirstly, note that, from literature,\n\n$T(E) = tr(G^{r}(E) \\Gamma_R  G^{a}(E) \\Gamma_L)$\n\nBy defining, $ S = \\sqrt{\\Gamma_R} G^r(E) \\sqrt{\\Gamma_L}$, we can reformulate above as,\n\n$T(E) = tr(S^{\\dagger} S)$\n\n\nThe idea to apply Monte is the trace trick.\nLet $\\psi_k = e^{\\theta_k i}$ be a wave function of unit amplitude with random phase, $\\theta_k$, where $n$ is a number of sample.\nConsider that $E[\\psi_k] = 0$ and $Cov(\\psi_k) = I$\nThen, by trace trick, $E[\\psi_k^T S^T S \\psi_k] = 0^T S^T S 0 + tr( S^{\\dagger} S I ) = T(E)$.\nFollowing from the Law of Large Number,\n\n$T(E) \\approx \\frac{1}{n}\\sum_{k = 1}^{n}\\psi_k^T S^T S \\psi_k$\n\n\nIn Bracket notation,\n\n$T(E) \\approx \\frac{1}{n}\\sum_{k = 1}^{n}  \\braket{\\psi_k|S^{\\dagger} S| \\psi_k}$\n\n\nWith the approximation, in addition to that one multiplication factor does not longer scale with the dimension of S (but a sample of monte carlo), each $\\braket{\\psi_k | S^{\\dagger} S | \\psi_k}$ can be computed in parallel.\n\nNow, we consider the computation of retarded Green function, $G^r(E) = [E1 - H_op - i \\eta]^{-1}$.\nFaber polynomial\n\nWe employ the Faber Polynomial Expansion of a matrix function to approximate its inverse.\nThe Faber Polynomial is generated by,\n\n    $\\frac{W(s) \\Phi&#39;(s)}{\\Phi(s) - z} = \\sum_{k = 0}^{\\infty} \\frac{1}{s^{k+1}} F_k(z, W)$\n\nwhere $F_n(z, W)$ is a generalized Faber Polynomial, and  \\Phi(s) = s + %\\alpha_0  + \\alpha_1 s^{-1} + \\alpha_2 s^{-2} + … is a conformal mapping from an exterior of a disk to an exterior of a simply-connected domain %containing $z$.\nConsider by taking $W(s) = 1$, $\\phi(s) = E \\mathbf{1}$, $z = H_{op} - \\eta i$, we can formulate,\n\n$\\frac{1}{ E \\mathbf{1} - H_{op}- i\\eta} =  \\frac{1}{\\Phi&#39;(E)} \\sum_{k =1}^{\\infty} \\frac{1}{s^{k+1}} F_k(H_{op}+ i \\eta)$\n\n\nAnd by truncating the summation term, we approximate $G^r(E)$.\n\nIterative Methods\n\nAs an alternative to polynomial expansion, we can approximate $G^r(E)$ using an iterative method. Leveraging the benefits of the Monte Carlo technique, we can express it as follows:\n\n$\\braket{\\Psi_k|S^{\\dagger} S| \\Psi_k} = || S \\ket{\\Psi_k}||^2_2$\n\n\nSuppose that $y = S \\ket{\\Psi_k}$. We want to compute the square of the norm of $y$.\nThus, we can formulate,\n\n$ (\\sqrt{\\Gamma_L}^{-1}(E \\mathbf{1} - H_{op} - i\\eta)\\sqrt{\\Gamma_R}^{-1}) y = \\ket{\\Psi_k}$\n\nThat is, we obtained a system of linear equation, i.e. $Ax = b$. For large dimension of $A$, iterative methods can be used to numerically solve for $y$.\n\nWe previously use Quasi-Minimal Residual method to solve for $y$.\n\nBy combining either Polynomial Expansion or iterative methods to the Monte Carlo method, we can efficiently approximate $T(E)$.\n\nCurrent Progress\n\nAlhough iterative methods theoretically solve for $y$ rapidly, practical implementation is impeded by numerical floating point errors, which hinder the convergence of $y$ towards the actual solution. Regarding the Faber Polynomial, we employed conformal mapping in the shape of an ellipse, a tuning process that is challenging to adjust to meet the necessary requirements for polynomial convergence.\n\nReference\nRoi Baer et al. “Ab initio study of the alternating current impedance\nof a molecular junction”. In: The Journal of Chemical Physics 120.7\n(Feb. 2004), pp. 3387–3396. issn: 0021-9606. doi:\n10.1063/1.1640611. eprint:\nhttps://pubs.aip.org/aip/jcp/article-\npdf/120/7/3387/10858645/3387_1_online.pdf. url:\nhttps://doi.org/10.1063/1.1640611.\n\nBaruch Feldman et al. “Real-space method for highly parallelizable\nelectronic transport calculations”. In: Phys. Rev. B 90 (3 July 2014),\np. 035445. doi: 10.1103/PhysRevB.90.035445. url:\nhttps://link.aps.org/doi/10.1103/PhysRevB.90.035445.\n\nYouhong Huang, Donald J. Kouri, and David K. Hoffman. “General,\nenergy-separable Faber polynomial representation of operator\nfunctions: Theory and application in quantum scattering”. In: The\nJournal of Chemical Physics 101.12 (Dec. 1994), pp. 10493–10506.\nissn: 0021-9606. doi: 10.1063/1.468481. eprint:\nhttps://pubs.aip.org/aip/jcp/article-\npdf/101/12/10493/15360236/10493_1_online.pdf. url:\nhttps://doi.org/10.1063/1.468481.\n\nToshiaki Iitaka et al. “Calculating the linear response functions of\nnoninteracting electrons with a time-dependent Schr ̈odinger\nequation”. English. In: Physical Review E 56.1 SUPPL. B (July 1997),\npp. 1222–1229. issn: 2470-0045. doi: 10.1103/physreve.56.1222.\nWanchaloem Wunkaew, Ewan Lister, Baruch Feldman (University of Washington, Seattle, WA)Improving Electronic Conductance Calculations with a Monte Carlo ApproachMay 19th, 2023 19 / 20\n"
} ,
  
  {
    "title"    : "Pricing American Options Using Machine Learning",
    "category" : "Project",
    "url"      : "/project/2023/06/06/American-Option-ML.md/",
    "date"     : "June 6, 2023",
    "excerpt"  : "Wanchaloem Wunkaew \nleegarap@uw.edu \nUniversity of Washington, Seattle, WA\n\nThis is an individual final project for CFRM 421/521: Machine Learning for Finance class at the University of Washington.\n\nNote that this project is not peer-reviewed and ...",
    "content"  : "Wanchaloem Wunkaew \nleegarap@uw.edu \nUniversity of Washington, Seattle, WA\n\nThis is an individual final project for CFRM 421/521: Machine Learning for Finance class at the University of Washington.\n\nNote that this project is not peer-reviewed and that the project is for educational purpose. Please use it as your own risk.\n\nThe jupyter notebook of this project is accessible at this link\n\nAbstract \n\nWe proprose machine learning methods for regression including Linear Regression, Polynomial Regression, Support Vector Regressor Ridge and Lasso Regression, Random Forest Regressor, K-Nearest Neighbors Regression, Multi-layer perceptons, and Convolutional Neural Network to price American options. The methods are trained and tested on SPDR S&amp;amp;P 500 ETF Trust call options, from 2021 to 2022.  The result shows that Convolutional Neural Network and the Random Forest performs better than the Binomial Tree, which we use as a benchmark, in the term of testing Root Mean Squared Errors.\n\nIntroduction\n\nOption pricing is one of fields in financial engineering. The formalization of option pricing methods, such as the Black-Scholes equation, has greatly impactedthe field of financial economics. Among various types of options, the American Option is a distinct financial asset that grants its holder the right to buy or sell the underlying asset at any point up to, and including, its maturity date. Unlike European options, American options do not have a closed-form solution, so it requires the use of numerical methods for their pricing. The Binomial Tree and Monte Carlo simulations are two such numerical methods capable of pricing these options. One notable limitation in all option pricing methods is the unrealistic assumptions of underlying asset price models. For instance, the Geometric Brownian motion model, which is assumed in the Black-Scholes equation, does not account for heteroskedasticity and the non-normal log return. In addition, some parameters, such as $\\sigma$ in the binomial tree, in these tradtional methods/models are hard to estimate.\n\nIn this project, we employ a range of machine learning regression models, including linear regression, polynomial regression, ridge regression, lasso regression, Support Vector Regressor, Random Forest Regressor, K-Nearest Neighbor regressor, Multilayer Perceptron regressor, and Convolutional Neural Network, to price American options. We anticipate that these models may unveil relationships between inputs (such as the strike price and stock prices from 8 days prior) and the output (the option price). As such, we expect these models to either outperform or match the performance of the traditional Binomial Tree model, which we have selected as our research benchmark. Additionally, we believe that some of these models can rectify the flaws of traditional models as outlined above.\n\nWe will divide this paper into distinct sections. In the next section, we will discuss our dataset and the Binomial Tree, which serves as our benchmark. The third section will be devoted to training machine learning models on this dataset. In the fourth section, we will summarize and discuss our findings. Lastly, in the fifth section, we will draw conclusions based on our results.\n\nData Preparation and Benchmark model\n\nWe will employ the SPDR S&amp;amp;P 500 ETF Trust option chains from Q1 2020-Q4 2022 for our analysis. This data, which consists of more than three million options traded in markets, was downloads from Kaggle.  The dataset encompasses a wealth of information, including but not restricted to, the closing option price, the closing strike price, underlying asset price, bid and ask prices, and implied volatility. Despite the fact that the dataset includes put option data, our study will only concentrate on call options.\n\nThe features incorporated in this project consist of: strike price, dividend yield, risk-free rate, the time until the option’s maturity, historical volatility, and the underlying asset (adjusted closed) prices from seven days prior (including the closed price on the date that the option is traded). A majority of these features encompass parameters used to price options in the Binomial Tree model. Note that we assume no transaction fee.\n\nThe historical volatility was calculated by the standard deviation of the logarithmic return of the underlying asset over the five years preceding the date each option was observed. The risk-free rate was obtained from that Fama-French guy website. The stock prices were obtained from Yahoo Finance via yfinance as showed below, while the dividend yield was estimated from  Ycharts.\n\nDespite the dataset’s size with more than three million option data entries, we will randomly sample 100,000 options for training and testing our models due to time limitation. Also, we will choose only first 10000 of the traning data for cross validation.\n\nThe output of the model is solely the corresponding call option price, denoted as “’ [C_LAST]’’ within the dataset.\n\n%matplotlib inline\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pandas.plotting import scatter_matrix\n\n\n# get data from https://www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022\ndf = pd.read_csv(&quot;./spy_2020_2022.csv&quot;, low_memory=False)\n\n\n# Randomly chose 100000 samples for traning, validating, and testing\ndf = df.sample(100000,random_state = 42)\n\n\nThe columns of the option data are shown below:\n\ndf.columns\n\n\nIndex([&#39;[QUOTE_UNIXTIME]&#39;, &#39; [QUOTE_READTIME]&#39;, &#39; [QUOTE_DATE]&#39;,\n       &#39; [QUOTE_TIME_HOURS]&#39;, &#39; [UNDERLYING_LAST]&#39;, &#39; [EXPIRE_DATE]&#39;,\n       &#39; [EXPIRE_UNIX]&#39;, &#39; [DTE]&#39;, &#39; [C_DELTA]&#39;, &#39; [C_GAMMA]&#39;, &#39; [C_VEGA]&#39;,\n       &#39; [C_THETA]&#39;, &#39; [C_RHO]&#39;, &#39; [C_IV]&#39;, &#39; [C_VOLUME]&#39;, &#39; [C_LAST]&#39;,\n       &#39; [C_SIZE]&#39;, &#39; [C_BID]&#39;, &#39; [C_ASK]&#39;, &#39; [STRIKE]&#39;, &#39; [P_BID]&#39;,\n       &#39; [P_ASK]&#39;, &#39; [P_SIZE]&#39;, &#39; [P_LAST]&#39;, &#39; [P_DELTA]&#39;, &#39; [P_GAMMA]&#39;,\n       &#39; [P_VEGA]&#39;, &#39; [P_THETA]&#39;, &#39; [P_RHO]&#39;, &#39; [P_IV]&#39;, &#39; [P_VOLUME]&#39;,\n       &#39; [STRIKE_DISTANCE]&#39;, &#39; [STRIKE_DISTANCE_PCT]&#39;],\n      dtype=&#39;object&#39;)\n\n\n# load option data\n#df = pd.read_csv(&quot;./spy_20_21.csv&quot;)\n#df = df.iloc[:,1:]\ndf[&#39; [QUOTE_DATE]&#39;] = pd.to_datetime(df[&#39; [QUOTE_DATE]&#39;], format = &#39; %Y-%m-%d&#39;)\ndf.set_index(&quot; [QUOTE_DATE]&quot;, inplace = True)\ndf = df[[&#39; [UNDERLYING_LAST]&#39;,&#39; [EXPIRE_DATE]&#39;,&#39; [C_IV]&#39;,&#39; [C_LAST]&#39;,&#39; [STRIKE]&#39;]]\ndf.columns = [&#39;underlying_last&#39;,&#39;maturity&#39;,&#39;implied_vol&#39;,&#39;call_last&#39;, &#39;K&#39;]\n\n# load stock data\n#import yfinance as yf\n# spy = yf.download(&#39;SPY&#39;, &#39;2010-01-01&#39;, &#39;2023-02-01&#39;)\n# spy.to_csv(&#39;spy.csv&#39;)\n\nspy = pd.read_csv(&#39;./spy.csv&#39;)\nspy[&#39;Date&#39;] = pd.to_datetime(spy[&#39;Date&#39;], format = &#39;%Y-%m-%d&#39;)\ndf[&#39;maturity&#39;]=pd.to_datetime(df[&#39;maturity&#39;], format = &#39; %Y-%m-%d&#39;)\nspy.set_index(&#39;Date&#39;, inplace = True)\n# Choose only data and adjust close\nspy = spy[[&#39;Adj Close&#39;]]\n\n# Computing log return\nspy = pd.DataFrame((np.log(spy[&#39;Adj Close&#39;].shift(-1)) - np.log(spy[&#39;Adj Close&#39;])).dropna())\nspy.columns = [&#39;ret&#39;]\n\n\nCompute historical volatility $q$ by finding the std of the log return 1825 days (~5 years) in back in the past.\n\nn_days_hist = 365 * 5\ndate_ls = list()\nvol_ls = list()\nspy_array = spy[&#39;ret&#39;].to_numpy()\nfor i in range(n_days_hist,len(spy_array)):\n    date_ls.append(spy.index[i])\n    vol_ls.append(np.std(spy_array[i-n_days_hist:i+1])*np.sqrt(252))\nhist_vol_df = pd.DataFrame({&#39;Date&#39;: date_ls, &quot;hist_vol&quot;:vol_ls})\nhist_vol_df.set_index(&#39;Date&#39;, inplace = True)\n\n\ndf = df.join(hist_vol_df)\n\n\nDividend Yields are obtained and approximated from ychart website.\n\n# Dividend Yield\n# estimates from https://ycharts.com/companies/SPY/dividend_yield\n# 2022 1.34%\n# 2021 1.5\n# 2020 1.7\n\nd_yield = {2020: 1.7/100, 2021: 1.5/100, 2022: 1.34/100}\ny_list = list()\nfor ind in df.index:\n    y_list.append(d_yield[ind.year])\ndf[&#39;q&#39;] = y_list\n\n\nWe obtain risk-free rate $r$ from Kenneth French website (Fama/French 3 Factors).\n\nWe approximate the risk-free on a specific date by chooing the risk on the first day of a month corresponding to the option data.\n\n# Interest rate\n# Note that the csv file below was preprocessed by removing unnecessary rows and columns that broke the read_csv\nmf = pd.read_csv(&quot;F-F_Research_Data_Factors.CSV&quot;).iloc[:,:2]\nmf.columns = [&#39;Date&#39;,&#39;r&#39;]\nmf[&#39;r&#39;] /= 100\nmf[&#39;Date&#39;] = pd.to_datetime(mf[&#39;Date&#39;], format = &#39;%Y%m&#39;)                \nmf.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      r\n    \n  \n  \n    \n      0\n      1926-07-01\n      0.0296\n    \n    \n      1\n      1926-08-01\n      0.0264\n    \n    \n      2\n      1926-09-01\n      0.0036\n    \n    \n      3\n      1926-10-01\n      -0.0324\n    \n    \n      4\n      1926-11-01\n      0.0253\n    \n  \n\n\n\nmf = pd.read_csv(&quot;F-F_Research_Data_Factors.CSV&quot;)\nmf = mf[[&quot;Unnamed: 0&quot;,&#39;RF&#39;]]\nmf.columns = [&#39;Date&#39;,&#39;r&#39;]\n\nr_years = mf[&#39;Date&#39;].apply(lambda x: int(str(x)[:4]))\nr_months =  mf[&#39;Date&#39;].apply(lambda x: int(str(x)[4:]))\n\nmf[&#39;year&#39;] = r_years\nmf[&#39;month&#39;] = r_months\nmf.drop([&#39;Date&#39;],axis = 1,inplace = True)\nmf = mf[2020 &amp;lt;= mf[&#39;year&#39;]]\nr_list = list()\nfor ind in tqdm(df.index):\n    r_list.append(mf[(mf[&#39;year&#39;] == ind.year) &amp;amp; (mf[&#39;month&#39;] == ind.month)][&#39;r&#39;].to_numpy()[0])\ndf[&#39;r&#39;] = r_list\n\n\n100%|█████████████████████████████████| 100000/100000 [00:42&amp;lt;00:00, 2368.09it/s]\n\n\nr_list = list()\nr_i = 0\n\nfor ind in tqdm(df.index):\n    while (mf.iloc[r_i,1] !=ind.year) or (mf.iloc[r_i,2]  != ind.month):\n        r_i+=1\n    r_list.append(mf.iloc[r_i,0])\ndf[&#39;r&#39;] = r_list\n\n\n100%|████████████████████████████████| 100000/100000 [00:05&amp;lt;00:00, 18277.71it/s]\n\n\nNow, we compute the time until maturity $T-t$ in a year.\nThis is computed by dividing a number of days between the day that the option data was observed and it maturity by 365.\n\nWe will denote this feature as $T$.\n\n# Time til maturity\ndd_list = list()\nfor days in (df[&#39;maturity&#39;]-df.index):\n    dd_list.append(days.days/365)\ndf[&#39;T&#39;] = dd_list\n\n\nA scatter matrix which conclues the dataset are shown below:\n\nscatter_matrix(df[[&quot;underlying_last&quot;,&quot;call_last&quot;, &quot;K&quot;,&quot;hist_vol&quot;,&quot;q&quot;,&quot;r&quot;,&quot;T&quot;]],figsize = (15,15))\nplt.show()\n\n\n\n\nBenchmark: American option\n\nIn the next subsection, we will discuss the traditional method of pricing American option: Binomial Tree.\nGiven an initial stock, in a next step, the stock price will either go up or go down, under some pre-defined multiplicative factors: u and d.\n\\(u = e^{\\sigma \\Delta t}\\)\n\\(d = \\frac{1}{u}\\)\nu and d depends solely on the volatility, so we used historical volatility to estimate this $\\sigma$.\n\nFor stock (or index) price with continuous dividend, the risk-neutral probability is computed by\n\\(\\hat{p} = \\frac{e^{(r-q) \\Delta t} - d}{u -d}\\)\n\nThe price for American option at node i can be computed by\n\\(f_i = \\max (e^{-r \\Delta T} (\\hat{p} f_{iu} + (1- \\hat{p}) f_{id}, (s_i - K)^+)\\)\n\nwhere the payoff at the leaf nodes i are $ (s_i - K)^+$.\n\nThe prices of options, as observed from the market at a given time, represent the values that investors expect those options to have under the circumstances. Theorically, if we have all the necessary parameters to price an option, we can construct option prices in the market. For binomial tree, most of the parameters such as $r$, $q$,  and $S_0$ can be observed or estimated except for the $\\sigma$ which is hard to estimate. One way is to construct that quantity by inversely solving the model given the option price and all other paramemters. This implied volaitlity is the volatility of the underlying asset that the market expects. When pricing option, this quantity is unknown but can be estimated. Due to volatitility smile, in many traditional model, it is hard to use the quantity in the model. Since this implied volatility can be used to reconstruct exact price of an individual option. We will not use this quantity in our model.\n\nTo estimate the volatility, werely on historical volatility of the underlying asset price. By analyzing the historical price movements of the asset, we can make an estimation of $\\sigma$.\n\n# American Option pricing using binomial tree\n# adapted from Kevin Mooney (see reference)\n\ndef american_call_price(S0, K, sigma, t, r = 0, q = 0, N = 3 ):\n\n    #delta t\n    t = t / (N - 1)\n    u = np.exp(sigma * np.sqrt(t))\n    d = 1/u\n\n    p = (np.exp((r-q) * t) - d) / (u - d)\n    stock_prices = np.zeros( (N, N) )\n    call_prices = np.zeros( (N, N) )\n\n    stock_prices[0,0] = S0\n    M = 0\n    for i in range(1, N ):\n        M = i + 1\n        stock_prices[i, 0] = d * stock_prices[i-1, 0]\n        for j in range(1, M ):\n            stock_prices[i, j] = u * stock_prices[i - 1, j - 1]\n    expiration = stock_prices[-1,:] - K\n    expiration = np.exp(-q*t *(N-1))*stock_prices[-1,:] - K\n    expiration.shape = (expiration.size, )\n    expiration = np.where(expiration &amp;gt;= 0, expiration, 0)\n    call_prices[-1,:] =  expiration\n\n    # backward computing value\n    for i in range(N - 2,-1,-1):\n        for j in range(i + 1):\n            # American Payoff\n            call_prices[i,j] = np.max([np.exp(-r * t) * ((1-p) * call_prices[i+1,j] + p * call_prices[i+1,j+1]),\n                                      np.max([stock_prices[i, j] - K,0])])         \n    return call_prices[0,0]\n\n\nWe use 10-step tree for option pricing.\n\n# American Option\nN = 10\n\nbm_list = list()\nfor i in tqdm(range(len(df))):\n    current_row = df.iloc[i,:]\n\n    S0 = current_row[&#39;underlying_last&#39;]\n    K = current_row[&#39;K&#39;]\n    sigma = current_row[&#39;hist_vol&#39;]\n    r = current_row[&#39;r&#39;]\n    q = current_row[&#39;q&#39;]\n    T = current_row[&#39;T&#39;]\n    bm_list.append(american_call_price(S0, K, sigma = sigma, t = T, r = r, q = q, N = N ))\ndf[&#39;bm&#39;] = bm_list\n\n\n  0%|                                    | 126/100000 [00:00&amp;lt;01:19, 1255.55it/s]/var/folders/6r/96ncs6hd5plcz0t1d7spstzr0000gn/T/ipykernel_47931/145875048.py:11: RuntimeWarning: invalid value encountered in double_scalars\n  p = (np.exp((r-q) * t) - d) / (u - d)\n100%|█████████████████████████████████| 100000/100000 [01:12&amp;lt;00:00, 1377.00it/s]\n\n\ndf = df[df[&#39;call_last&#39;] != &quot; &quot;]\ndf[&#39;call_last&#39;] = np.double(df[&#39;call_last&#39;])\ndf.dropna(inplace = True)\n\n\nThe table for option data are shown below.\nNote that we are not going to use all the columns in the table.\n\ndf\n\n\n\n\n\n  \n    \n      \n      underlying_last\n      maturity\n      implied_vol\n      call_last\n      K\n      hist_vol\n      q\n      r\n      T\n      bm\n    \n    \n      [QUOTE_DATE]\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      324.87\n      2020-09-30\n      0.199590\n      33.40\n      300.0\n      0.128190\n      0.0170\n      0.13\n      0.745205\n      47.199011\n    \n    \n      2020-01-02\n      324.87\n      2020-03-31\n      0.557560\n      100.30\n      215.0\n      0.128190\n      0.0170\n      0.13\n      0.243836\n      114.648588\n    \n    \n      2020-01-02\n      324.87\n      2020-01-27\n      0.130940\n      9.09\n      316.0\n      0.128190\n      0.0170\n      0.13\n      0.068493\n      11.905250\n    \n    \n      2020-01-02\n      324.87\n      2020-06-30\n      0.286710\n      70.85\n      255.0\n      0.128190\n      0.0170\n      0.13\n      0.493151\n      81.585331\n    \n    \n      2020-01-02\n      324.87\n      2020-01-31\n      0.118620\n      6.17\n      321.5\n      0.128190\n      0.0170\n      0.13\n      0.079452\n      8.142697\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-12-30\n      382.44\n      2023-01-27\n      0.169750\n      0.38\n      415.0\n      0.189685\n      0.0134\n      0.33\n      0.076712\n      1.201606\n    \n    \n      2022-12-30\n      382.44\n      2024-01-19\n      0.167740\n      1.86\n      515.0\n      0.189685\n      0.0134\n      0.33\n      1.054795\n      27.581163\n    \n    \n      2022-12-30\n      382.44\n      2023-01-03\n      \n      0.00\n      332.0\n      0.189685\n      0.0134\n      0.33\n      0.010959\n      51.526183\n    \n    \n      2022-12-30\n      382.44\n      2024-01-19\n      0.193650\n      0.16\n      670.0\n      0.189685\n      0.0134\n      0.33\n      1.054795\n      0.458140\n    \n    \n      2022-12-30\n      382.44\n      2023-01-20\n      0.681010\n      0.01\n      685.0\n      0.189685\n      0.0134\n      0.33\n      0.057534\n      0.000000\n    \n  \n\n96924 rows × 10 columns\n\n\nThe measure for quantiative models that are widely used in machine learning is the root mean square error.\nAs we use the binomial model as a benchmark, we will compute the RMSE for the benchmark binomial model.\n\n# MSE\nfrom sklearn.metrics import mean_squared_error\n\n\nnp.sqrt(mean_squared_error(df[&#39;call_last&#39;], df[&#39;bm&#39;]))\n\n\n51.301668827531735\n\n\nThe RMSE for the benchmark model is ~51 which is high.\n\nThe statistics for the absolute error is shown below. The median error is around 3.3.\n\nplt.hist(np.abs(df[&#39;bm&#39;]-df[&#39;call_last&#39;]),bins = 50, density = True)\nplt.title(&quot;A histogram of error (in absolute difference) of Binomial Model&quot;)\nplt.ylabel(&quot;density&quot;)\nplt.xlabel(&quot;Error&quot;)\n\n\nText(0.5, 0, &#39;Error&#39;)\n\n\n\n\nMachine Learning Model\n\nIn addition to the (closed) underlying price, days-to-maturity, historical volatility, dividend yield, interest rate and strike price, we will also use the adjusted closed stock prices 8 days lag as inputs.\n\nTn = 8\nspy = pd.read_csv(&#39;./spy.csv&#39;)\nspy[&#39;Date&#39;] = pd.to_datetime(spy[&#39;Date&#39;], format = &#39;%Y-%m-%d&#39;)\nspy.set_index(&quot;Date&quot;,inplace= True)\nspy = spy[[&#39;Adj Close&#39;]]\nspy.rename(columns={&quot;Adj Close&quot;: &quot;t0&quot;}, inplace = True)\nspy_use = spy[spy.index &amp;gt;= df.index[0]]\n\nimport warnings\nwarnings.filterwarnings(&quot;ignore&quot;)\nfor i in range(1,Tn+1):\n    spy_use[&#39;t-&#39;+str(i)] = spy[&#39;t0&#39;].shift(i).iloc[-len(spy_use[&#39;t0&#39;]):]\ndf = df.join(spy_use)\n\n\n# choose only numerical\ndf.drop(columns = [&#39;maturity&#39;,&#39;implied_vol&#39;,&#39;bm&#39;,&#39;underlying_last&#39;],inplace = True)\n\n\nThe first 5 rows of  final prepared dataset is shown below:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      call_last\n      K\n      hist_vol\n      q\n      r\n      T\n      t0\n      t-1\n      t-2\n      t-3\n      t-4\n      t-5\n      t-6\n      t-7\n      t-8\n    \n  \n  \n    \n      2020-01-02\n      33.40\n      300.0\n      0.12819\n      0.017\n      0.13\n      0.745205\n      308.517456\n      305.658936\n      304.918213\n      306.608582\n      306.684631\n      305.060669\n      305.051178\n      304.585846\n      303.256348\n    \n    \n      2020-01-02\n      100.30\n      215.0\n      0.12819\n      0.017\n      0.13\n      0.243836\n      308.517456\n      305.658936\n      304.918213\n      306.608582\n      306.684631\n      305.060669\n      305.051178\n      304.585846\n      303.256348\n    \n    \n      2020-01-02\n      9.09\n      316.0\n      0.12819\n      0.017\n      0.13\n      0.068493\n      308.517456\n      305.658936\n      304.918213\n      306.608582\n      306.684631\n      305.060669\n      305.051178\n      304.585846\n      303.256348\n    \n    \n      2020-01-02\n      70.85\n      255.0\n      0.12819\n      0.017\n      0.13\n      0.493151\n      308.517456\n      305.658936\n      304.918213\n      306.608582\n      306.684631\n      305.060669\n      305.051178\n      304.585846\n      303.256348\n    \n    \n      2020-01-02\n      6.17\n      321.5\n      0.12819\n      0.017\n      0.13\n      0.079452\n      308.517456\n      305.658936\n      304.918213\n      306.608582\n      306.684631\n      305.060669\n      305.051178\n      304.585846\n      303.256348\n    \n  \n\n\n\nThe original data is timestamped with the time that each option is observed. The chronological order of the data may have an effect on the model, so it is important to take the order into account. For this option pricing project, we assume that the chronological order does not have a significant effect on the models. We make an assumption that the most chornological effects are contained in the features such as historical volatility and the stock price lags.\n\nWe shuffle and split 80% for traning set, 16% for test set, 4% for validation sets.\n\nfrom sklearn.model_selection import train_test_split\nimport datetime\n# We split 80% for traning set, 16% for test set, 4% for validation sets\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns = [&#39;call_last&#39;]),df[&#39;call_last&#39;], test_size=0.2)\nX_test, X_valid, y_test, y_valid = train_test_split(X_test,y_test, test_size=0.2)\n\n\n# If you believe the order affect, use the code below\n#X_train = df[df.index &amp;lt;= datetime.datetime(2022,9,1)].copy()\n#y_train = X_train[[&#39;call_last&#39;]]\n#X_train = X_train.drop(columns = [&#39;call_last&#39;])\n\n#X_test = df[df.index &amp;gt; datetime.datetime(2022,9,1)].copy()\n#y_test = X_test[[&#39;call_last&#39;]]\n#X_test = X_test.drop(columns = [&#39;call_last&#39;])\n\n#X_valid = X_test.iloc[:1000,:]\n#y_valid = y_test.iloc[:1000]\n\n#X_test = X_test.iloc[1000:,:]\n#y_test = y_test.iloc[1000:]\n\n\nAs we obtained the dataset, we compute the option price based on American binomial tree model on the test set in order to compare it to other models. Since a number of the test set is small, we can apply the tree for large number of steps. In this case, we use 30 steps.\n\nBenchmark (Binomial Tree)\n\nbm_list = list()\nfor i in tqdm(range(len(X_test))):\n    current_row = X_test.iloc[i,:]\n\n    S0 = current_row[&#39;t0&#39;]\n    K = current_row[&#39;K&#39;]\n    sigma = current_row[&#39;hist_vol&#39;]\n    r = current_row[&#39;r&#39;]\n    q = current_row[&#39;q&#39;]\n    T = current_row[&#39;T&#39;]\n    bm_list.append(american_call_price(S0, K, sigma = sigma, t = T, r = r, q = q, N = 30 ))\n\n\n100%|████████████████████████████████████| 15508/15508 [01:25&amp;lt;00:00, 180.46it/s]\n\n\nnp.sqrt(mean_squared_error(y_test,bm_list))\n\n\n48.219430763590715\n\n\nThe root mean squared error for the binomial model is 48.219430763590715.\n\nLinear Regression\n\nWe use multiple linear regression with stdard scaled input.\n\nfrom sklearn.linear_model import LinearRegression,SGDRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nlin_reg = Pipeline([(&quot;std_scaler&quot;, StandardScaler()),\n                     (&quot;LinReg&quot;, LinearRegression())])\nlin_reg.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;LinReg&amp;#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;LinReg&amp;#x27;, LinearRegression())])StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\nThe root mean squared errors for training and testing set are shown below:\n\nnp.sqrt(mean_squared_error(y_train,lin_reg.predict(X_train)))\n\n\n38.43095625260875\n\n\nnp.sqrt(mean_squared_error(y_test,lin_reg.predict(X_test)))\n\n\n37.97049534873681\n\n\nThe coefficients and intercept for the model are shown below:\n\nlin_reg[&quot;LinReg&quot;].coef_\n\n\narray([-37.65425523,   2.524246  ,   1.51641187,  -0.54682013,\n        13.45049636,   5.76041445,   3.59107429,   3.32804942,\n         3.28542487,  -0.09017812,  -0.74559882,  -1.94913388,\n         0.6794828 ,   9.51963281])\n\n\nlin_reg[&quot;LinReg&quot;].intercept_\n\n\n31.72985981248147\n\n\nPolynomial Regression\n\nFor the polynomial regression, we only consider the polynomial of degree 2 with standard scaled input. We have 14 features, and the polynomial features are going to be large.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\n\nPolyReg = Pipeline([(&quot;std_scaler&quot;, StandardScaler()),\n          (&quot;poly_feature&quot;, PolynomialFeatures(degree=2, include_bias=False)),\n         (&quot;LinReg&quot;,  LinearRegression())])\n\n\nPolyReg.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;poly_feature&amp;#x27;, PolynomialFeatures(include_bias=False)),\n                (&amp;#x27;LinReg&amp;#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;poly_feature&amp;#x27;, PolynomialFeatures(include_bias=False)),\n                (&amp;#x27;LinReg&amp;#x27;, LinearRegression())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures(include_bias=False)LinearRegressionLinearRegression()\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Training Loss\nnp.sqrt(mean_squared_error(y_train,PolyReg.predict(X_train)))\n\n\n36.17182222027887\n\n\n# Testing Loss\nnp.sqrt(mean_squared_error(y_test,PolyReg.predict(X_test)))\n\n\n36.06896946882215\n\n\nRidge and Lasso Regression\n\nFor Ridge and Lasso Regression, we also find the best hyperparameters $\\alpha$ using Grid Search.\nWe perform cross validation only on the first 10000 training data with 3-fold cross validation.\n\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\n\n\nalpha = (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)\nridge_reg = Pipeline([(&quot;std_scaler&quot;, StandardScaler()),\n         (&quot;ridge&quot;,  Ridge())])\nparameters = {&#39;ridge__alpha&#39;: alpha}\nrr = GridSearchCV(estimator=ridge_reg,param_grid = parameters, cv = 3)\nrr.fit(X_train[:10000], y_train[:10000])\n\n\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;ridge&amp;#x27;, Ridge())]),\n             param_grid={&amp;#x27;ridge__alpha&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;ridge&amp;#x27;, Ridge())]),\n             param_grid={&amp;#x27;ridge__alpha&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;ridge&amp;#x27;, Ridge())])StandardScalerStandardScaler()RidgeRidge()\n\nrr.best_estimator_\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;ridge&amp;#x27;, Ridge(alpha=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;ridge&amp;#x27;, Ridge(alpha=10))])StandardScalerStandardScaler()RidgeRidge(alpha=10)\n\nWe found that $\\alpha = 10$ is the best alpha.\nWe then fit this best estimator to the whole dataset.\n\nbest_ridge = rr.best_estimator_\nbest_ridge.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;ridge&amp;#x27;, Ridge(alpha=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;ridge&amp;#x27;, Ridge(alpha=10))])StandardScalerStandardScaler()RidgeRidge(alpha=10)\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Training Loss\nnp.sqrt(mean_squared_error(y_train,best_ridge.predict(X_train)))\n\n\n38.43095969174494\n\n\n# Testing Loss\nnp.sqrt(mean_squared_error(y_test,best_ridge.predict(X_test)))\n\n\n37.97019255417378\n\n\nWe did the same to Lasso.\n\nalpha = (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)\nlasso_reg = Pipeline([(&quot;std_scaler&quot;, StandardScaler()),\n         (&quot;lasso&quot;,  Lasso())])\nparameters = {&#39;lasso__alpha&#39;: alpha}\nlr = GridSearchCV(estimator=lasso_reg, param_grid  = parameters, cv = 3)\nlr.fit(X_train[:10000], y_train[:10000])\n\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;lasso&amp;#x27;, Lasso())]),\n             param_grid={&amp;#x27;lasso__alpha&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;lasso&amp;#x27;, Lasso())]),\n             param_grid={&amp;#x27;lasso__alpha&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;lasso&amp;#x27;, Lasso())])StandardScalerStandardScaler()LassoLasso()\n\nlr.best_estimator_\nbest_lasso = lr.best_estimator_\nbest_lasso\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;lasso&amp;#x27;, Lasso(alpha=0.01))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;lasso&amp;#x27;, Lasso(alpha=0.01))])StandardScalerStandardScaler()LassoLasso(alpha=0.01)\n\nbest_lasso.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;lasso&amp;#x27;, Lasso(alpha=0.01))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;lasso&amp;#x27;, Lasso(alpha=0.01))])StandardScalerStandardScaler()LassoLasso(alpha=0.01)\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Training Loss\nnp.sqrt(mean_squared_error(y_train,best_lasso.predict(X_train)))\n\n\n38.431561232353104\n\n\n# Testing Loss\nnp.sqrt(mean_squared_error(y_test,best_lasso.predict(X_test)))\n\n\n37.969146928057036\n\n\nThe best $\\alpha$ is 0.01.\n\nWe can use Lasso for feature selection as it tries to minimize unimportant features coefficients to 0.\n\nlr.best_estimator_[&#39;lasso&#39;].coef_\n\n\narray([-37.64057787,   2.49377087,   1.45139028,  -0.55584586,\n        13.43936913,   5.72526309,   3.68045696,   3.23670677,\n         2.1807758 ,   0.        ,  -0.        ,  -0.        ,\n         0.        ,   8.50465773])\n\n\nX_train.columns\n\n\nIndex([&#39;K&#39;, &#39;hist_vol&#39;, &#39;q&#39;, &#39;r&#39;, &#39;T&#39;, &#39;t0&#39;, &#39;t-1&#39;, &#39;t-2&#39;, &#39;t-3&#39;, &#39;t-4&#39;, &#39;t-5&#39;,\n       &#39;t-6&#39;, &#39;t-7&#39;, &#39;t-8&#39;],\n      dtype=&#39;object&#39;)\n\n\nInterestingly many of the lags of stock prices are not important.\n\nSupport Vector Regressor\n\nFor the linear, we tune the hyperparameter C and $\\epsilon$ for linear SVR.\nThe other setting are the same as in previous.\n\nfrom sklearn.svm import LinearSVR\n\nlsvr = Pipeline([(&quot;std_scaler&quot;, StandardScaler()),\n                 (&#39;svr&#39;,LinearSVR())])\nparameters = {&#39;svr__epsilon&#39;:(0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000),\n             &#39;svr__C&#39;: (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)}\nlsvr_gs = GridSearchCV(estimator=lsvr,param_grid = parameters, cv = 3)\nlsvr_gs.fit(X_train[:10000], y_train[:10000])\n\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;svr&amp;#x27;, LinearSVR())]),\n             param_grid={&amp;#x27;svr__C&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,\n                                    1000, 5000, 10000),\n                         &amp;#x27;svr__epsilon&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;svr&amp;#x27;, LinearSVR())]),\n             param_grid={&amp;#x27;svr__C&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,\n                                    1000, 5000, 10000),\n                         &amp;#x27;svr__epsilon&amp;#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;svr&amp;#x27;, LinearSVR())])StandardScalerStandardScaler()LinearSVRLinearSVR()\n\nlsvr_gs.best_estimator_\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, LinearSVR(C=10, epsilon=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, LinearSVR(C=10, epsilon=10))])StandardScalerStandardScaler()LinearSVRLinearSVR(C=10, epsilon=10)\n\nbest_lsvr = lsvr_gs.best_estimator_\n\n\nbest_lsvr.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, LinearSVR(C=10, epsilon=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, LinearSVR(C=10, epsilon=10))])StandardScalerStandardScaler()LinearSVRLinearSVR(C=10, epsilon=10)\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Training Loss\nnp.sqrt(mean_squared_error(y_train,best_lsvr.predict(X_train)))\n\n\n38.547851070665025\n\n\n# Testing Loss\nnp.sqrt(mean_squared_error(y_test,best_lsvr.predict(X_test)))\n\n\n38.03050061254464\n\n\nThe tuned c and epsilon are 10 and 10 respectively.\n\nNow, we consider nonlinear SVR,in addtion to $\\epsilon$ and $C$, we include a type of kernel as a hyperparamter. We choose between rbf and sigmoid. Note that we reduce search space for $\\epsilon$ and $C$ to accelerate the computing time.\n\nfrom sklearn.svm import SVR\n\n\nnlsvr = Pipeline([(&quot;std_scaler&quot;, StandardScaler()),\n                 (&#39;svr&#39;,SVR())])\nparameters = {&#39;svr__kernel&#39;:(&#39;rbf&#39;, &#39;sigmoid&#39;),\n              &#39;svr__epsilon&#39;:(0.001, 0.01,0.1,1,100,5000),\n             &#39;svr__C&#39;: (0.001, 0.01,0.1,1,100,5000)\n              }\nnlsvr_gs = GridSearchCV(estimator=nlsvr,param_grid = parameters, cv = 3,n_jobs = -1)\nnlsvr_gs.fit(X_train[:10000].to_numpy(), y_train[:10000].to_numpy().ravel())\n\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;svr&amp;#x27;, SVR())]),\n             n_jobs=-1,\n             param_grid={&amp;#x27;svr__C&amp;#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),\n                         &amp;#x27;svr__epsilon&amp;#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),\n                         &amp;#x27;svr__kernel&amp;#x27;: (&amp;#x27;rbf&amp;#x27;, &amp;#x27;sigmoid&amp;#x27;)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;svr&amp;#x27;, SVR())]),\n             n_jobs=-1,\n             param_grid={&amp;#x27;svr__C&amp;#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),\n                         &amp;#x27;svr__epsilon&amp;#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),\n                         &amp;#x27;svr__kernel&amp;#x27;: (&amp;#x27;rbf&amp;#x27;, &amp;#x27;sigmoid&amp;#x27;)})estimator: PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()), (&amp;#x27;svr&amp;#x27;, SVR())])StandardScalerStandardScaler()SVRSVR()\n\nbest_nlsvr = nlsvr_gs.best_estimator_\nbest_nlsvr\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, SVR(C=100, epsilon=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, SVR(C=100, epsilon=1))])StandardScalerStandardScaler()SVRSVR(C=100, epsilon=1)\n\nbest_nlsvr.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, SVR(C=100, epsilon=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;svr&amp;#x27;, SVR(C=100, epsilon=1))])StandardScalerStandardScaler()SVRSVR(C=100, epsilon=1)\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Training Loss\nnp.sqrt(mean_squared_error(y_train,best_nlsvr.predict(X_train)))\n\n\n37.808085641917536\n\n\n# Testing Loss\nnp.sqrt(mean_squared_error(y_test,best_nlsvr.predict(X_test)))\n\n\n37.92867379033997\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nThere are so many parameters, so we use random search instead of grid search. We randomly sample hyperparameters uniformly from the parameters lists for 100 samples. And we use first 10000 shuffled data entries for this tuning.\n\nrf_rg = Pipeline([(&#39;std_scaler&#39;, StandardScaler()),\n                     (&#39;rf&#39;, RandomForestRegressor())])\nparameters = {&#39;rf__n_estimators&#39;: (50,100,300,400,500),\n             &#39;rf__max_depth&#39;:(None, 8,32,64,128),\n             &#39;rf__ccp_alpha&#39;:(0,0.00000001,0.00001,0.001),\n             &#39;rf__bootstrap&#39;: [True, False]}\nrf_gs =RandomizedSearchCV(estimator=rf_rg,param_distributions = parameters, cv = 3,n_jobs = -1,n_iter = 100)\n\n\nrf_gs.fit(X_train[:10000].to_numpy(), y_train[:10000].to_numpy().ravel())\n\n\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                             (&amp;#x27;rf&amp;#x27;, RandomForestRegressor())]),\n                   n_iter=160, n_jobs=-1,\n                   param_distributions={&amp;#x27;rf__bootstrap&amp;#x27;: [True, False],\n                                        &amp;#x27;rf__ccp_alpha&amp;#x27;: (0, 1e-08, 1e-05,\n                                                          0.001),\n                                        &amp;#x27;rf__max_depth&amp;#x27;: (None, 8, 32, 64, 128),\n                                        &amp;#x27;rf__n_estimators&amp;#x27;: (50, 100, 300, 400,\n                                                             500)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                             (&amp;#x27;rf&amp;#x27;, RandomForestRegressor())]),\n                   n_iter=160, n_jobs=-1,\n                   param_distributions={&amp;#x27;rf__bootstrap&amp;#x27;: [True, False],\n                                        &amp;#x27;rf__ccp_alpha&amp;#x27;: (0, 1e-08, 1e-05,\n                                                          0.001),\n                                        &amp;#x27;rf__max_depth&amp;#x27;: (None, 8, 32, 64, 128),\n                                        &amp;#x27;rf__n_estimators&amp;#x27;: (50, 100, 300, 400,\n                                                             500)})estimator: PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;rf&amp;#x27;, RandomForestRegressor())])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor()\n\nbest_rf = rf_gs.best_estimator_\n\n\nbest_rf\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;rf&amp;#x27;,\n                 RandomForestRegressor(ccp_alpha=0, max_depth=8,\n                                       n_estimators=400))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;rf&amp;#x27;,\n                 RandomForestRegressor(ccp_alpha=0, max_depth=8,\n                                       n_estimators=400))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)\n\nbest_rf.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;rf&amp;#x27;,\n                 RandomForestRegressor(ccp_alpha=0, max_depth=8,\n                                       n_estimators=400))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;rf&amp;#x27;,\n                 RandomForestRegressor(ccp_alpha=0, max_depth=8,\n                                       n_estimators=400))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)\n\nThe root mean squared errors for training and testing set are shown below:\n\nThe root mean squared errors for training and testing set are shown below:# Training Loss\nnp.sqrt(mean_squared_error(y_train,best_rf.predict(X_train)))\n\n\n31.349066580782836\n\n\n#Testing RMSE\nnp.sqrt(mean_squared_error(y_test,best_rf.predict(X_test)))\n\n\n32.90546500222834\n\n\nWith Random Forest, we can see feature importance:\n\nbest_rf[&#39;rf&#39;].feature_importances_\n\n\narray([0.51617093, 0.04662939, 0.00114048, 0.00432427, 0.13790168,\n       0.04360533, 0.0377016 , 0.02562464, 0.03328482, 0.01493993,\n       0.05282551, 0.00782497, 0.01813578, 0.05989066])\n\n\nX_train.columns\n# Strike Price and Time to maturity seem to be the most important\n\n\nIndex([&#39;K&#39;, &#39;hist_vol&#39;, &#39;q&#39;, &#39;r&#39;, &#39;T&#39;, &#39;t0&#39;, &#39;t-1&#39;, &#39;t-2&#39;, &#39;t-3&#39;, &#39;t-4&#39;, &#39;t-5&#39;,\n       &#39;t-6&#39;, &#39;t-7&#39;, &#39;t-8&#39;],\n      dtype=&#39;object&#39;)\n\n\nThe best hyperparamters are ccp_alpha=0, max_depth=8, n_estimators=400, and bootstrap = False.\n\nK-Nearest Neighbors\n\nIn this model, we only tune a number of neighbors as a hyperparameter.\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nknn_rg = Pipeline([(&#39;std_scaler&#39;, StandardScaler()),\n                  (&#39;knn&#39;,KNeighborsRegressor())])\nparameters = {&#39;knn__n_neighbors&#39;: np.arange(5,100,5)}\n\n\nknn_gs = GridSearchCV(estimator=knn_rg,param_grid = parameters, cv = 3,n_jobs = -1)\nknn_gs.fit(X_train[:10000], y_train[:10000])\n\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor())]),\n             n_jobs=-1,\n             param_grid={&amp;#x27;knn__n_neighbors&amp;#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,\n       90, 95])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                                       (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor())]),\n             n_jobs=-1,\n             param_grid={&amp;#x27;knn__n_neighbors&amp;#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,\n       90, 95])})estimator: PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\nbest_knn = knn_gs.best_estimator_\nbest_knn\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor(n_neighbors=15))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor(n_neighbors=15))])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor(n_neighbors=15)\n\nbest_knn.fit(X_train, y_train)\n\n\n\nPipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor(n_neighbors=15))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&amp;#x27;std_scaler&amp;#x27;, StandardScaler()),\n                (&amp;#x27;knn&amp;#x27;, KNeighborsRegressor(n_neighbors=15))])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor(n_neighbors=15)\n\nThe root mean squared errors for training and testing set are shown below:\n\nnp.sqrt(mean_squared_error(y_train,best_knn.predict(X_train)))\n\n\n31.92967572127278\n\n\nnp.sqrt(mean_squared_error(y_test,best_knn.predict(X_test)))\n\n\n32.912277382176526\n\n\nThe best number of the neigbors is 15.\n\nMulti-layer perceptron\n\nFor neutral network, we make experiments on three models, which are multilayer perceptron (with dropout and normalization, with relu as an activation function for the hidden layers and identity for the output layer), multilayer perceptron with tuned hyperparameters, and Convolutional Neural network.\n\nfrom tensorflow.keras import Sequential, layers\nimport tensorflow as tf\n\n\n2023-05-31 17:15:39.270253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.BatchNormalization(),\n     tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(30, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.BatchNormalization(),\n     tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(30, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.BatchNormalization(),\n     tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(20, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.BatchNormalization(),\n     tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.BatchNormalization(),\n     tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation= None)\n])\n\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=50,restore_best_weights=True)\nmodel.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;)\nhistory = model.fit(X_train, y_train, epochs=1000,\n                    validation_data=(X_valid, y_valid),callbacks=[callback])\n\n\nEpoch 1/1000\n2424/2424 [==============================] - 8s 2ms/step - loss: 2348.1985 - val_loss: 1538.4142\nEpoch 2/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1858.8630 - val_loss: 1519.5778\nEpoch 3/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1826.4266 - val_loss: 1481.3267\nEpoch 4/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1821.6594 - val_loss: 1479.6355\nEpoch 5/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1815.7463 - val_loss: 1452.8606\nEpoch 6/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1803.6791 - val_loss: 1457.6923\nEpoch 7/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1796.4032 - val_loss: 1453.8772\nEpoch 8/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1796.3320 - val_loss: 1446.6718\nEpoch 9/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1788.8895 - val_loss: 1437.1597\nEpoch 10/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1762.4237 - val_loss: 1441.7910\nEpoch 11/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1789.3596 - val_loss: 1461.8259\nEpoch 12/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1782.5536 - val_loss: 1455.1730\nEpoch 13/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1772.4648 - val_loss: 1450.3599\nEpoch 14/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1753.4039 - val_loss: 1453.5958\nEpoch 15/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1778.0695 - val_loss: 1447.4929\n...\nEpoch 106/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1699.8079 - val_loss: 1426.6115\nEpoch 107/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1737.1375 - val_loss: 1446.8939\nEpoch 108/1000\n2424/2424 [==============================] - 4s 2ms/step - loss: 1710.6053 - val_loss: 1421.2778\n\n\nmodel.summary()\n\n\nModel: &quot;sequential&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization (BatchN  (None, 14)               56        \n ormalization)                                                   \n\n dropout (Dropout)           (None, 14)                0         \n\n dense (Dense)               (None, 30)                450       \n\n batch_normalization_1 (Batc  (None, 30)               120       \n hNormalization)                                                 \n\n dropout_1 (Dropout)         (None, 30)                0         \n\n dense_1 (Dense)             (None, 30)                930       \n\n batch_normalization_2 (Batc  (None, 30)               120       \n hNormalization)                                                 \n\n dropout_2 (Dropout)         (None, 30)                0         \n\n dense_2 (Dense)             (None, 20)                620       \n\n batch_normalization_3 (Batc  (None, 20)               80        \n hNormalization)                                                 \n\n dropout_3 (Dropout)         (None, 20)                0         \n\n dense_3 (Dense)             (None, 10)                210       \n\n batch_normalization_4 (Batc  (None, 10)               40        \n hNormalization)                                                 \n\n dropout_4 (Dropout)         (None, 10)                0         \n\n dense_4 (Dense)             (None, 1)                 11        \n\n=================================================================\nTotal params: 2,637\nTrainable params: 2,429\nNon-trainable params: 208\n_________________________________________________________________\n\n\nThe learning curve is shown below:\n\nplt.plot(history.history[&#39;loss&#39;],label = &quot;loss&quot;)\nplt.plot(history.history[&#39;val_loss&#39;], label = &quot;val_loss&quot;)\nplt.legend()\nplt.ylabel(&quot;MSE&quot;)\nplt.xlabel(&quot;Epoch&quot;)\n\n\nText(0.5, 0, &#39;Epoch&#39;)\n\n\n\n\nThe root mean squared errors for training and testing set are shown below:\n\n#trainig RMSE\nnp.sqrt(mean_squared_error(y_train,model.predict(X_train)))\n\n\n2424/2424 [==============================] - 2s 714us/step\n\n\n\n\n\n36.21230441027238\n\n\n#testing RMSE\nnp.sqrt(mean_squared_error(y_test,model.predict(X_test)))\n\n\n485/485 [==============================] - 0s 675us/step\n\n\n\n\n\n35.71456726735933\n\n\nThe model seems to be underfitting.\n\nHyperparamter-tuned MLP\n\nWe use keras tuner library to tune the hyperparameters of the network. The hyperparameter space is shown below.\n\nimport keras_tuner as kt\n\ndef build_model(hp):\n    n_hidden = hp.Int(&quot;n_hidden&quot;, min_value=1, max_value=8)\n    n_neurons = hp.Int(&quot;n_neurons&quot;, min_value=1, max_value=100)\n    learning_rate = hp.Float(&quot;learning_rate&quot;, min_value=1e-4, max_value=1e-2,\n                             sampling=&quot;log&quot;)\n    l2_rate = hp.Float(&quot;l2&quot;, min_value=1e-4, max_value=100,\n                             sampling=&quot;log&quot;)\n    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Normalization(input_shape=X_train.shape[1:]))\n    for _ in range(n_hidden):\n        model.add(tf.keras.layers.Dense(n_neurons, activation=&quot;relu&quot;,kernel_initializer=&quot;he_normal&quot;,kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n    model.add(tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n    model.compile(loss=&quot;mse&quot;, optimizer=optimizer)\n    return model\n\n\nrandom_search_tuner = kt.RandomSearch(\n    build_model, objective=&quot;val_loss&quot;, max_trials=20, seed=42)\nrandom_search_tuner.search(X_train[:5000], y_train[:5000], epochs=150,\n                           validation_data=(X_valid, y_valid))\n\n\n\nTrial 20 Complete [00h 00m 53s]\nval_loss: 1461.2305908203125\n\nBest val_loss So Far: 1451.614013671875\nTotal elapsed time: 00h 15m 31s\nINFO:tensorflow:Oracle triggered exit\n\n\nrandom_search_tuner.get_best_hyperparameters()[0].values\n\n\n{&#39;n_hidden&#39;: 7,\n &#39;n_neurons&#39;: 15,\n &#39;learning_rate&#39;: 0.0006237028864858578,\n &#39;l2&#39;: 0.0003065801184974072}\n\n\nThe best hyperparameters we found are 7 hidden layers with 15 neurons each, 0.0006237028864858578 learning rate, and l2 = 0.0003065801184974072 for the l2 regularization.\n\nbest_nn = random_search_tuner.get_best_models()[0]\n\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=100,restore_best_weights=True)\nhistory_best_nn = best_nn.fit(X_train, y_train, epochs=1000,\n                    validation_data=(X_valid, y_valid),callbacks=[callback])\n\nEpoch 1/1000\n2424/2424 [==============================] - 6s 1ms/step - loss: 1364.6182 - val_loss: 1498.1863\nEpoch 2/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1363.1213 - val_loss: 1462.6725\nEpoch 3/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1354.9094 - val_loss: 1493.1799\nEpoch 4/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1349.7559 - val_loss: 1452.7467\nEpoch 5/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1347.2516 - val_loss: 1499.4685\nEpoch 6/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1341.6652 - val_loss: 1437.6245\nEpoch 7/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1340.0056 - val_loss: 1423.4025\nEpoch 8/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1340.5402 - val_loss: 1513.4679\nEpoch 9/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1340.7520 - val_loss: 1569.9033\n...\nEpoch 556/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1242.6877 - val_loss: 1332.1714\nEpoch 557/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1260.6245 - val_loss: 1501.7150\nEpoch 558/1000\n2424/2424 [==============================] - 3s 1ms/step - loss: 1245.4034 - val_loss: 1347.1127\n\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Traing RMSE\nnp.sqrt(mean_squared_error(y_train,best_nn.predict(X_train)))\n\n\n2424/2424 [==============================] - 2s 597us/step\n\n34.78459569413211\n\n\n# Testing RMSE\nnp.sqrt(mean_squared_error(y_test,best_nn.predict(X_test)))\n\n\n485/485 [==============================] - 0s 612us/step\n\n34.821059741172114\n\n\nThe learning curve is shown below.\n\nplt.plot(history_best_nn.history[&#39;loss&#39;],label = &quot;loss&quot;)\nplt.plot(history_best_nn.history[&#39;val_loss&#39;], label = &quot;val_loss&quot;)\nplt.legend()\nplt.ylabel(&quot;MSE&quot;)\nplt.xlabel(&quot;Epoch&quot;)\n\n\nText(0.5, 0, &#39;Epoch&#39;)\n\n\n\n\nConvolutional Neural Network (CNN)\n\nThe structure of the convolutional network is shown below.\n\nmodel_conv = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(32,2,activation = &#39;relu&#39;,input_shape = (14,1,), kernel_initializer=&quot;he_normal&quot;,padding = &#39;same&#39;),\n    tf.keras.layers.Conv1D(32,3,activation = &#39;relu&#39;, kernel_initializer=&quot;he_normal&quot;,padding = &#39;same&#39;),\n    tf.keras.layers.Conv1D(32,3,activation = &#39;relu&#39;, kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.Dense(64, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.Dense(32, activation=&quot;relu&quot;,\n                          kernel_initializer=&quot;he_normal&quot;),\n    tf.keras.layers.Dense(1, activation= None)\n])\n\n\n\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=50,restore_best_weights=True)\nmodel_conv.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;)\nhistory_cov = model_conv.fit(X_train, y_train, epochs=1000,\n                    validation_data=(X_valid, y_valid),callbacks=[callback])\n\n\nEpoch 1/1000\n2424/2424 [==============================] - 7s 2ms/step - loss: 1632.2096 - val_loss: 1460.4231\nEpoch 2/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1373.3662 - val_loss: 1505.1824\nEpoch 3/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1366.6722 - val_loss: 1474.0969\nEpoch 4/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1346.9469 - val_loss: 1475.6388\nEpoch 5/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1334.1947 - val_loss: 1547.0897\nEpoch 6/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1323.7288 - val_loss: 1439.5723\nEpoch 7/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1310.3964 - val_loss: 1432.5355\nEpoch 8/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1294.4939 - val_loss: 1449.5792\nEpoch 9/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1290.2340 - val_loss: 1363.4564\n...\nEpoch 244/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1013.4796 - val_loss: 1174.7893\nEpoch 245/1000\n2424/2424 [==============================] - 5s 2ms/step - loss: 1002.9050 - val_loss: 1177.7996\n\n\nmodel_conv.summary()\n\n\nModel: &quot;sequential_1&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv1d (Conv1D)             (None, 14, 32)            96        \n\n conv1d_1 (Conv1D)           (None, 14, 32)            3104      \n\n conv1d_2 (Conv1D)           (None, 12, 32)            3104      \n\n flatten (Flatten)           (None, 384)               0         \n\n dense_8 (Dense)             (None, 128)               49280     \n\n dense_9 (Dense)             (None, 64)                8256      \n\n dense_10 (Dense)            (None, 32)                2080      \n\n dense_11 (Dense)            (None, 1)                 33        \n\n=================================================================\nTotal params: 65,953\nTrainable params: 65,953\nNon-trainable params: 0\n_________________________________________________________________\n\n\nThe learning curve is shown below.\n\nplt.plot(history_cov.history[&#39;loss&#39;],label = &quot;loss&quot;)\nplt.plot(history_cov.history[&#39;val_loss&#39;], label = &quot;val_loss&quot;)\nplt.legend()\nplt.ylabel(&quot;MSE&quot;)\nplt.xlabel(&quot;Epoch&quot;)\n\n\nText(0.5, 0, &#39;Epoch&#39;)\n\n\n\n\nThe root mean squared errors for training and testing set are shown below:\n\n# Training RMSE\nnp.sqrt(mean_squared_error(y_train,model_conv.predict(X_train)))\n\n\n2424/2424 [==============================] - 2s 920us/step\n\n\n\n\n\n31.66131974960281\n\n\n# Testing RMSE\nnp.sqrt(mean_squared_error(y_test,model_conv.predict(X_test)))\n\n\n485/485 [==============================] - 0s 814us/step\n\n\n\n\n\n32.372653521773714\n\n\nResult\n\nThe Root Mean Square Error for each machine learning models are shown below, together with the chosen hyperparamters.\n\nres = pd.DataFrame({&quot;Model&quot;: [&quot;Binomial Tree&quot;,&#39;Linear Regression&#39;,&quot;Polynomial Regression&quot;,&quot;Ridge Regression&quot;,&quot;Lasso Regression&quot;,&quot;Linear SVR&quot;,&quot;Nonlinear SVR&quot;,&quot;Random Forest&quot;,&quot;KNN&quot;,&quot;Neural Network&quot;, &quot;Neural Network (tuned)&quot;, &quot; Convolutional Neutral Network&quot;],\n                   &quot;RMSE(training)&quot;:[None,\n38.43095625260875,\n36.17182222027887,\n38.43095969174494,\n38.431561232353104,\n38.547851070665025,\n37.808085641917536,\n31.349066580782836,\n31.92967572127278,\n36.21230441027238,\n34.78459569413211,\n31.66131974960281\n],\n                &quot;RMSE(testing)&quot;:\n                   [48.219430763590715,\n                    37.97049534873681,\n36.06896946882215,\n37.97019255417378,\n37.969146928057036,\n38.03050061254464,\n37.92867379033997,\n32.90546500222834,\n32.912277382176526,\n35.71456726735933,\n34.821059741172114,\n32.372653521773714\n],\n&quot;Hyperparameters&quot; :\n        [&quot;29 steps&quot;,&quot;&quot;,\n&quot;degree = 2&quot;,\n&quot;alpha = 10&quot;,\n&quot;alpha = 0.01&quot;,\n&quot;C=10, epsilon=10&quot;,\n&quot;C=100, epsilon=1&quot;,\n&quot;ccp_alpha=0, max_depth=8, n_estimators=400, bootstrap = False&quot;,\n         &quot;neighbors = 15&quot;,\n        &quot;consider model summary above&quot;,\n        &quot;consider model summary above&quot;,\n        &quot;consider model summary above&quot;]    \n                   })\nres\n\n\n\n\n\n  \n    \n      \n      Model\n      RMSE(training)\n      RMSE(testing)\n      Hyperparameters\n    \n  \n  \n    \n      0\n      Binomial Tree\n      NaN\n      48.219431\n      29 steps\n    \n    \n      1\n      Linear Regression\n      38.430956\n      37.970495\n      \n    \n    \n      2\n      Polynomial Regression\n      36.171822\n      36.068969\n      degree = 2\n    \n    \n      3\n      Ridge Regression\n      38.430960\n      37.970193\n      alpha = 10\n    \n    \n      4\n      Lasso Regression\n      38.431561\n      37.969147\n      alpha = 0.01\n    \n    \n      5\n      Linear SVR\n      38.547851\n      38.030501\n      C=10, epsilon=10\n    \n    \n      6\n      Nonlinear SVR\n      37.808086\n      37.928674\n      C=100, epsilon=1\n    \n    \n      7\n      Random Forest\n      31.349067\n      32.905465\n      ccp_alpha=0, max_depth=8, n_estimators=400, bo...\n    \n    \n      8\n      KNN\n      31.929676\n      32.912277\n      neighbors = 25\n    \n    \n      9\n      Neural Network\n      36.212304\n      35.714567\n      consider model summary above\n    \n    \n      10\n      Neural Network (tuned)\n      34.784596\n      34.821060\n      consider model summary above\n    \n    \n      11\n      Convolutional Neutral Network\n      31.661320\n      32.372654\n      consider model summary above\n    \n  \n\n\n\nConsider the RMSE for the testing set, the lower RMSE, the more accurate the model to compute option price. By this sole metric, all the machine learning model we used in this project performs better than the binomial tree.\nBy ranking the models by this value, we have that the Convolutional Neural Network performs the best, the random forest is the second best, and the K-Nearest Neighbors is the third.\n\nThe statistics for the absolute error for the best three models and the binomial tree are shown below.\n\nres_error = pd.concat([pd.DataFrame({&quot;binomial abs_error&quot;: np.abs(np.array(bm_list) -y_test.to_numpy().flatten())}).describe(),\n                        np.abs(model_conv.predict(X_test).ravel() - y_test).describe(),\n          np.abs((best_rf.predict(X_test).ravel()-y_test)).describe(),\n                       np.abs(best_knn.predict(X_test).ravel() - y_test ).describe(),],axis = 1)\n\n\nres_error.columns = [&quot;Binomial Tree Absolute Error&quot;,&quot;CNN Absolute Error&quot;,&quot;Random Forest Absolute Error&quot;,&quot;KNN Absolute Error&quot;]\nres_error\n\n\n485/485 [==============================] - 0s 908us/step\n\n\n\n\n\n  \n    \n      \n      Binomial Tree Absolute Error\n      CNN Absolute Error\n      Random Forest Absolute Error\n      KNN Absolute Error\n    \n  \n  \n    \n      count\n      15508.000000\n      15508.000000\n      15508.000000\n      15508.000000\n    \n    \n      mean\n      22.192882\n      16.865260\n      17.565473\n      17.039533\n    \n    \n      std\n      42.810135\n      27.633330\n      27.825774\n      28.158896\n    \n    \n      min\n      0.000000\n      0.000103\n      0.000018\n      0.000000\n    \n    \n      25%\n      0.749303\n      0.927289\n      1.313031\n      0.710667\n    \n    \n      50%\n      7.469478\n      4.053897\n      5.481253\n      4.335000\n    \n    \n      75%\n      17.896939\n      22.537766\n      22.226330\n      21.908333\n    \n    \n      max\n      369.079773\n      350.456573\n      338.299600\n      341.098000\n    \n  \n\n\n\nBy considering the median for the absolute errors for each model, we can see that those of the machine learing models are smaller than that from the benchmark model. This implies that for at least half of the dataset, the absolute error from the machine learing models are smaller than that from the benchmark.\nHowever, if we consider other quantitle, we can see that there are not much different. But for most part, the machine learning models have smaller errors.\n\nStock Price versus Predicted Call Price\n\nBy the RMSE on the testing set, we choose the CNN to be the best model.\nWe visualize $C_0$ as a function of strike price $K$ and the stock price $S_0$.\n\nres_pt = pd.concat([X_test[[&#39;t0&#39;]].set_index(np.arange(0,len(X_test))).rename(columns = {&quot;t0&quot;:&quot;stock price&quot;}),\n                    X_test[[&#39;K&#39;]].set_index(np.arange(0,len(X_test))).rename(columns = {&quot;K&quot;:&quot;K&quot;}),\n            pd.DataFrame({&quot;Call Price from the dataset&quot;: y_test.to_numpy().ravel()}),\n           pd.DataFrame({&quot;Call Price from Binomial Tree&quot;: bm_list}),\n           pd.DataFrame({&quot;Call Price from CNN&quot;: model_conv.predict(X_test).ravel()}),\n          pd.DataFrame({&quot;lower bound&quot;:\n              [np.max([lb,0]) for lb in X_test[&#39;t0&#39;] - X_test[&#39;K&#39;]*np.exp(-X_test[&#39;r&#39;] *X_test[&#39;T&#39;])]})], axis = 1)\n\n\n485/485 [==============================] - 1s 1ms/step\n\n\n# Percentage of call price greater than 120\nnp.sum(y_test.to_numpy() &amp;gt;= 120)/len(y_test)\n\n\n0.06970595821511479\n\n\nfig,axes = plt.subplots(1,3, figsize = (21,8),subplot_kw={&#39;projection&#39;: &#39;3d&#39;})\naxes[0].scatter(res_pt[&#39;stock price&#39;], res_pt[&quot;K&quot;],res_pt[&quot;Call Price from the dataset&quot;], s= 1)\naxes[0].set_xlabel(&quot;$S_0$&quot;)\naxes[0].set_ylabel(&quot;$K$&quot;)\naxes[0].set_zlabel(&quot;$C_0$&quot;)\naxes[0].view_init(elev=20., azim=145, roll=0)\naxes[0].set_title(&quot;True&quot;)\naxes[0].set_zlim([0,320])\n\naxes[1].scatter(res_pt[&#39;stock price&#39;], res_pt[&quot;K&quot;],res_pt[&quot;Call Price from Binomial Tree&quot;], s= 1)\naxes[1].set_xlabel(&quot;$S_0$&quot;)\naxes[1].set_ylabel(&quot;$K$&quot;)\naxes[1].set_zlabel(&quot;$C_0 $&quot;)\naxes[1].view_init(elev=20., azim=145, roll=0)\naxes[1].set_title(&quot;Binomial Tree&quot;)\naxes[1].set_zlim([0,320])\n\n\naxes[2].scatter(res_pt[&#39;stock price&#39;], res_pt[&quot;K&quot;],res_pt[&quot;Call Price from CNN&quot;], s= 1)\naxes[2].set_xlabel(&quot;$S_0$&quot;)\naxes[2].set_ylabel(&quot;$K$&quot;)\naxes[2].set_zlabel(&quot;$C_0 $&quot;)\naxes[2].set_title(&quot;CNN&quot;)\naxes[2].view_init(elev=20., azim=145, roll=0)\naxes[2].set_zlim([0,320])\n\n\nplt.show()\n\n\n\n\nBy considering only two factors, $S_0$ and $K$, we can see some paterrns in the corresponding option price. All the three plots look similar, indicating the accuracy of the binomial tree and the Convolutional Neural Network to price options. However, in the convolutional neural network, we can see that the call prices are dense around the small values similar to the actual price, however it did not well capture high call prices. For the benchmark, the prices are more sparse compared to both actual price and the CNN.\n\nNow, we visualize an option price as a function of stock price, fixing other variable.\n\nWe fix other variable by choosing those variables from the dataset.\n\nThen, we compare an option price as a function of $S_0$ from Binomial Tree and the best model.\n\nRemark that, the machine learning models require additional parameters which are stock prices 8 days in the past. We cannot fix these stock prices; otherwise, the underlying price is unrealistic (eg. stock price at 0 is 500, while those at -1, -2, are around 200).\n\nFor each $S_0$, we find relevant $S_{-1}, S_{-2}$, …, in the dataset.\n\nWe also compute a upperbound and lowerbound as shown below.\n\nThe upper and lower bound for American option (for non-dividend) can be computed by\n\\(\\max(S_t - K e^{-r (T-t)},0)\\leq C_t \\leq S_t\\)\n\n# Choose only S_0_x (rounded) that we use to train model\nS_0_x = np.unique(np.round(spy_use[&#39;t0&#39;]) )\nbinomial_vs_s0 = [american_call_price(xs0, sample_fixed[&#39;K&#39;][0], sigma = sample_fixed[&#39;hist_vol&#39;][0], t = sample_fixed[&#39;T&#39;][0], r = sample_fixed[&#39;r&#39;][0], q = sample_fixed[&#39;q&#39;][0], N = 30 ) for xs0 in S_0_x ]\nspy_extended = spy_use.copy()\nspy_extended[&#39;round_t0&#39;] = np.round(spy_use[&#39;t0&#39;])\ns0_dataset = []\nfor xs0 in S_0_x:\n    s0_dataset.append(np.concatenate([sample_fixed.drop(columns = [&quot;call_last&quot;,&#39;t0&#39;]).to_numpy()[0,:5], spy_extended[spy_extended[&#39;round_t0&#39;] == xs0].sample(1).iloc[0,:9].ravel()]))\ns0_dataset = np.array(s0_dataset)\ncnn_vs_s0 = model_conv.predict(s0_dataset).ravel()\n\n\n7/7 [==============================] - 0s 1ms/step\n\n\nmax_bound = S_0_x\nmin_bound = [np.max([xs0 - sample_fixed[&#39;K&#39;][0] * np.exp(-sample_fixed[&#39;r&#39;][0]*sample_fixed[&#39;T&#39;][0]),0]) for xs0 in S_0_x ]\n\n\nplt.plot(S_0_x ,binomial_vs_s0,label = &#39;binomial&#39;)\nplt.plot(S_0_x ,cnn_vs_s0,label = &#39;CNN&#39;)\nplt.plot(S_0_x ,max_bound,label = &#39;max bound&#39;)\nplt.plot(S_0_x ,min_bound,label = &#39;min bound&#39;,ls = &#39;dashed&#39;)\nplt.ylim([0,100])\nplt.legend()\n\n\n&amp;lt;matplotlib.legend.Legend at 0x7f844f12e160&amp;gt;\n\n\n\n\nThe plot aligns with the previous plot, suggesting consistency. However, it appears that the CNN model does not accurately capture the high call prices. The generated call prices are observed to be outside the lower bound for high underlying price, creating the potential for arbitrage opportunities. Therefore, it is crucial to exercise caution and conduct a thorough examination of the model before implementing it. Additionally, fine-tuning or training the CNN model with a larger and more comprehensive dataset could potentially improve its performance and yield a better model.\n\nConclusion\n\nIn this project, we have utilized machine learning models to price American options based on the SPY dataset. Our findings indicate that all of the machine learning models outperform the traditional binomial model for both the testing and training sets. Among these models, CNN, random forest, and KNN show promise, as their testing and training losses are relatively low.\n\nHowever, it is worth noting that even though CNN performs the best overall, it struggles with accurately computing high call prices. Therefore, further tuning and dataset preparation might be necessary to enhance its performance in this regard.\n\nIn conclusion, machine learning models demonstrate the ability to capture the relationship between financial information and option pricing. Through the utilization of the RMSE metric, these models outperform the traditional benchmark model. Nonetheless, additional tunin\n\nAcknoledgement\n\nThanks all people who gathered the data and publicly publish them online. Thank you Dr. Kevin Lu for giving suggestions on American option pricing models.\n\nReferences\n\nCulkin, Robert, and Sanjiv R. Das. Machine Learning in Finance: The Case of Deep Learning for Option Pricing, 2 Aug. 2017, srdas.github.io/Papers/BlackScholesNN.pdf.\n\nFrench, Kenneth R. Kenneth R. French - Data Library, mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html. Accessed 3 June 2023.\n\nGraupe, Kyle. “$spy Option Chains - Q1 2020 - Q4 2022.” Kaggle, 14 Mar. 2023,\nwww.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022.\n\nLu, Kevin. “Machine Learning for Finance Lectures.” CFRM 421:Machine Learning for Finance . Seattle, The University of Washington.\n\nLu, Kevin. “Introduction to Financial Markets Lectures.” CFRM 415: Introduction to Financial Markets. Seattle, The University of Washington.\n\nMooney, Kevin, director. Implementing the Binomial Option Pricing Model in Python, YouTube, 15 Feb. 2021, https://www.youtube.com/watch?v=d7wa16RNRCI. Accessed 5 June 2023.\n\nSPY Dividend Yield, ycharts.com/companies/SPY/dividend_yield. Accessed 3 June 2023.\n"
} 
  
]
