<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>


<meta name="description" content="Wanchaloem Wunkaew 
leegarap@uw.edu 
University of Washington, Seattle, WA

This is an individual final project for CFRM 421/521: Machine Learning for Finance class at the University of Washington.

Note that this project is not peer-reviewed and that the project is for educational purpose. Please use it as your own risk.

The jupyter notebook of this project is accessible at this link

Abstract 

We proprose machine learning methods for regression including Linear Regression, Polynomial Regression, Support Vector Regressor Ridge and Lasso Regression, Random Forest Regressor, K-Nearest Neighbors Regression, Multi-layer perceptons, and Convolutional Neural Network to price American options. The methods are trained and tested on SPDR S&amp;P 500 ETF Trust call options, from 2021 to 2022.  The result shows that Convolutional Neural Network and the Random Forest performs better than the Binomial Tree, which we use as a benchmark, in the term of testing Root Mean Squared Errors.

Introduction

Option pricing is one of fields in financial engineering. The formalization of option pricing methods, such as the Black-Scholes equation, has greatly impactedthe field of financial economics. Among various types of options, the American Option is a distinct financial asset that grants its holder the right to buy or sell the underlying asset at any point up to, and including, its maturity date. Unlike European options, American options do not have a closed-form solution, so it requires the use of numerical methods for their pricing. The Binomial Tree and Monte Carlo simulations are two such numerical methods capable of pricing these options. One notable limitation in all option pricing methods is the unrealistic assumptions of underlying asset price models. For instance, the Geometric Brownian motion model, which is assumed in the Black-Scholes equation, does not account for heteroskedasticity and the non-normal log return. In addition, some parameters, such as $\sigma$ in the binomial tree, in these tradtional methods/models are hard to estimate.

In this project, we employ a range of machine learning regression models, including linear regression, polynomial regression, ridge regression, lasso regression, Support Vector Regressor, Random Forest Regressor, K-Nearest Neighbor regressor, Multilayer Perceptron regressor, and Convolutional Neural Network, to price American options. We anticipate that these models may unveil relationships between inputs (such as the strike price and stock prices from 8 days prior) and the output (the option price). As such, we expect these models to either outperform or match the performance of the traditional Binomial Tree model, which we have selected as our research benchmark. Additionally, we believe that some of these models can rectify the flaws of traditional models as outlined above.

We will divide this paper into distinct sections. In the next section, we will discuss our dataset and the Binomial Tree, which serves as our benchmark. The third section will be devoted to training machine learning models on this dataset. In the fourth section, we will summarize and discuss our findings. Lastly, in the fifth section, we will draw conclusions based on our results.

Data Preparation and Benchmark model

We will employ the SPDR S&amp;P 500 ETF Trust option chains from Q1 2020-Q4 2022 for our analysis. This data, which consists of more than three million options traded in markets, was downloads from Kaggle.  The dataset encompasses a wealth of information, including but not restricted to, the closing option price, the closing strike price, underlying asset price, bid and ask prices, and implied volatility. Despite the fact that the dataset includes put option data, our study will only concentrate on call options.

The features incorporated in this project consist of: strike price, dividend yield, risk-free rate, the time until the option’s maturity, historical volatility, and the underlying asset (adjusted closed) prices from seven days prior (including the closed price on the date that the option is traded). A majority of these features encompass parameters used to price options in the Binomial Tree model. Note that we assume no transaction fee.

The historical volatility was calculated by the standard deviation of the logarithmic return of the underlying asset over the five years preceding the date each option was observed. The risk-free rate was obtained from that Fama-French guy website. The stock prices were obtained from Yahoo Finance via yfinance as showed below, while the dividend yield was estimated from  Ycharts.

Despite the dataset’s size with more than three million option data entries, we will randomly sample 100,000 options for training and testing our models due to time limitation. Also, we will choose only first 10000 of the traning data for cross validation.

The output of the model is solely the corresponding call option price, denoted as “’ [C_LAST]’’ within the dataset.

%matplotlib inline


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
from pandas.plotting import scatter_matrix


# get data from https://www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022
df = pd.read_csv("./spy_2020_2022.csv", low_memory=False)


# Randomly chose 100000 samples for traning, validating, and testing
df = df.sample(100000,random_state = 42)


The columns of the option data are shown below:

df.columns


Index(['[QUOTE_UNIXTIME]', ' [QUOTE_READTIME]', ' [QUOTE_DATE]',
       ' [QUOTE_TIME_HOURS]', ' [UNDERLYING_LAST]', ' [EXPIRE_DATE]',
       ' [EXPIRE_UNIX]', ' [DTE]', ' [C_DELTA]', ' [C_GAMMA]', ' [C_VEGA]',
       ' [C_THETA]', ' [C_RHO]', ' [C_IV]', ' [C_VOLUME]', ' [C_LAST]',
       ' [C_SIZE]', ' [C_BID]', ' [C_ASK]', ' [STRIKE]', ' [P_BID]',
       ' [P_ASK]', ' [P_SIZE]', ' [P_LAST]', ' [P_DELTA]', ' [P_GAMMA]',
       ' [P_VEGA]', ' [P_THETA]', ' [P_RHO]', ' [P_IV]', ' [P_VOLUME]',
       ' [STRIKE_DISTANCE]', ' [STRIKE_DISTANCE_PCT]'],
      dtype='object')


# load option data
#df = pd.read_csv("./spy_20_21.csv")
#df = df.iloc[:,1:]
df[' [QUOTE_DATE]'] = pd.to_datetime(df[' [QUOTE_DATE]'], format = ' %Y-%m-%d')
df.set_index(" [QUOTE_DATE]", inplace = True)
df = df[[' [UNDERLYING_LAST]',' [EXPIRE_DATE]',' [C_IV]',' [C_LAST]',' [STRIKE]']]
df.columns = ['underlying_last','maturity','implied_vol','call_last', 'K']

# load stock data
#import yfinance as yf
# spy = yf.download('SPY', '2010-01-01', '2023-02-01')
# spy.to_csv('spy.csv')

spy = pd.read_csv('./spy.csv')
spy['Date'] = pd.to_datetime(spy['Date'], format = '%Y-%m-%d')
df['maturity']=pd.to_datetime(df['maturity'], format = ' %Y-%m-%d')
spy.set_index('Date', inplace = True)
# Choose only data and adjust close
spy = spy[['Adj Close']]

# Computing log return
spy = pd.DataFrame((np.log(spy['Adj Close'].shift(-1)) - np.log(spy['Adj Close'])).dropna())
spy.columns = ['ret']


Compute historical volatility $q$ by finding the std of the log return 1825 days (~5 years) in back in the past.

n_days_hist = 365 * 5
date_ls = list()
vol_ls = list()
spy_array = spy['ret'].to_numpy()
for i in range(n_days_hist,len(spy_array)):
    date_ls.append(spy.index[i])
    vol_ls.append(np.std(spy_array[i-n_days_hist:i+1])*np.sqrt(252))
hist_vol_df = pd.DataFrame({'Date': date_ls, "hist_vol":vol_ls})
hist_vol_df.set_index('Date', inplace = True)


df = df.join(hist_vol_df)


Dividend Yields are obtained and approximated from ychart website.

# Dividend Yield
# estimates from https://ycharts.com/companies/SPY/dividend_yield
# 2022 1.34%
# 2021 1.5
# 2020 1.7

d_yield = {2020: 1.7/100, 2021: 1.5/100, 2022: 1.34/100}
y_list = list()
for ind in df.index:
    y_list.append(d_yield[ind.year])
df['q'] = y_list


We obtain risk-free rate $r$ from Kenneth French website (Fama/French 3 Factors).

We approximate the risk-free on a specific date by chooing the risk on the first day of a month corresponding to the option data.

# Interest rate
# Note that the csv file below was preprocessed by removing unnecessary rows and columns that broke the read_csv
mf = pd.read_csv("F-F_Research_Data_Factors.CSV").iloc[:,:2]
mf.columns = ['Date','r']
mf['r'] /= 100
mf['Date'] = pd.to_datetime(mf['Date'], format = '%Y%m')                
mf.head()





  
    
      
      Date
      r
    
  
  
    
      0
      1926-07-01
      0.0296
    
    
      1
      1926-08-01
      0.0264
    
    
      2
      1926-09-01
      0.0036
    
    
      3
      1926-10-01
      -0.0324
    
    
      4
      1926-11-01
      0.0253
    
  



mf = pd.read_csv("F-F_Research_Data_Factors.CSV")
mf = mf[["Unnamed: 0",'RF']]
mf.columns = ['Date','r']

r_years = mf['Date'].apply(lambda x: int(str(x)[:4]))
r_months =  mf['Date'].apply(lambda x: int(str(x)[4:]))

mf['year'] = r_years
mf['month'] = r_months
mf.drop(['Date'],axis = 1,inplace = True)
mf = mf[2020 &lt;= mf['year']]
r_list = list()
for ind in tqdm(df.index):
    r_list.append(mf[(mf['year'] == ind.year) &amp; (mf['month'] == ind.month)]['r'].to_numpy()[0])
df['r'] = r_list


100%|█████████████████████████████████| 100000/100000 [00:42&lt;00:00, 2368.09it/s]


r_list = list()
r_i = 0

for ind in tqdm(df.index):
    while (mf.iloc[r_i,1] !=ind.year) or (mf.iloc[r_i,2]  != ind.month):
        r_i+=1
    r_list.append(mf.iloc[r_i,0])
df['r'] = r_list


100%|████████████████████████████████| 100000/100000 [00:05&lt;00:00, 18277.71it/s]


Now, we compute the time until maturity $T-t$ in a year.
This is computed by dividing a number of days between the day that the option data was observed and it maturity by 365.

We will denote this feature as $T$.

# Time til maturity
dd_list = list()
for days in (df['maturity']-df.index):
    dd_list.append(days.days/365)
df['T'] = dd_list


A scatter matrix which conclues the dataset are shown below:

scatter_matrix(df[["underlying_last","call_last", "K","hist_vol","q","r","T"]],figsize = (15,15))
plt.show()




Benchmark: American option

In the next subsection, we will discuss the traditional method of pricing American option: Binomial Tree.
Given an initial stock, in a next step, the stock price will either go up or go down, under some pre-defined multiplicative factors: u and d.
\(u = e^{\sigma \Delta t}\)
\(d = \frac{1}{u}\)
u and d depends solely on the volatility, so we used historical volatility to estimate this $\sigma$.

For stock (or index) price with continuous dividend, the risk-neutral probability is computed by
\(\hat{p} = \frac{e^{(r-q) \Delta t} - d}{u -d}\)

The price for American option at node i can be computed by
\(f_i = \max (e^{-r \Delta T} (\hat{p} f_{iu} + (1- \hat{p}) f_{id}, (s_i - K)^+)\)

where the payoff at the leaf nodes i are $ (s_i - K)^+$.

The prices of options, as observed from the market at a given time, represent the values that investors expect those options to have under the circumstances. Theorically, if we have all the necessary parameters to price an option, we can construct option prices in the market. For binomial tree, most of the parameters such as $r$, $q$,  and $S_0$ can be observed or estimated except for the $\sigma$ which is hard to estimate. One way is to construct that quantity by inversely solving the model given the option price and all other paramemters. This implied volaitlity is the volatility of the underlying asset that the market expects. When pricing option, this quantity is unknown but can be estimated. Due to volatitility smile, in many traditional model, it is hard to use the quantity in the model. Since this implied volatility can be used to reconstruct exact price of an individual option. We will not use this quantity in our model.

To estimate the volatility, werely on historical volatility of the underlying asset price. By analyzing the historical price movements of the asset, we can make an estimation of $\sigma$.

# American Option pricing using binomial tree
# adapted from Kevin Mooney (see reference)

def american_call_price(S0, K, sigma, t, r = 0, q = 0, N = 3 ):

    #delta t
    t = t / (N - 1)
    u = np.exp(sigma * np.sqrt(t))
    d = 1/u

    p = (np.exp((r-q) * t) - d) / (u - d)
    stock_prices = np.zeros( (N, N) )
    call_prices = np.zeros( (N, N) )

    stock_prices[0,0] = S0
    M = 0
    for i in range(1, N ):
        M = i + 1
        stock_prices[i, 0] = d * stock_prices[i-1, 0]
        for j in range(1, M ):
            stock_prices[i, j] = u * stock_prices[i - 1, j - 1]
    expiration = stock_prices[-1,:] - K
    expiration = np.exp(-q*t *(N-1))*stock_prices[-1,:] - K
    expiration.shape = (expiration.size, )
    expiration = np.where(expiration &gt;= 0, expiration, 0)
    call_prices[-1,:] =  expiration

    # backward computing value
    for i in range(N - 2,-1,-1):
        for j in range(i + 1):
            # American Payoff
            call_prices[i,j] = np.max([np.exp(-r * t) * ((1-p) * call_prices[i+1,j] + p * call_prices[i+1,j+1]),
                                      np.max([stock_prices[i, j] - K,0])])         
    return call_prices[0,0]


We use 10-step tree for option pricing.

# American Option
N = 10

bm_list = list()
for i in tqdm(range(len(df))):
    current_row = df.iloc[i,:]

    S0 = current_row['underlying_last']
    K = current_row['K']
    sigma = current_row['hist_vol']
    r = current_row['r']
    q = current_row['q']
    T = current_row['T']
    bm_list.append(american_call_price(S0, K, sigma = sigma, t = T, r = r, q = q, N = N ))
df['bm'] = bm_list


  0%|                                    | 126/100000 [00:00&lt;01:19, 1255.55it/s]/var/folders/6r/96ncs6hd5plcz0t1d7spstzr0000gn/T/ipykernel_47931/145875048.py:11: RuntimeWarning: invalid value encountered in double_scalars
  p = (np.exp((r-q) * t) - d) / (u - d)
100%|█████████████████████████████████| 100000/100000 [01:12&lt;00:00, 1377.00it/s]


df = df[df['call_last'] != " "]
df['call_last'] = np.double(df['call_last'])
df.dropna(inplace = True)


The table for option data are shown below.
Note that we are not going to use all the columns in the table.

df





  
    
      
      underlying_last
      maturity
      implied_vol
      call_last
      K
      hist_vol
      q
      r
      T
      bm
    
    
      [QUOTE_DATE]
      
      
      
      
      
      
      
      
      
      
    
  
  
    
      2020-01-02
      324.87
      2020-09-30
      0.199590
      33.40
      300.0
      0.128190
      0.0170
      0.13
      0.745205
      47.199011
    
    
      2020-01-02
      324.87
      2020-03-31
      0.557560
      100.30
      215.0
      0.128190
      0.0170
      0.13
      0.243836
      114.648588
    
    
      2020-01-02
      324.87
      2020-01-27
      0.130940
      9.09
      316.0
      0.128190
      0.0170
      0.13
      0.068493
      11.905250
    
    
      2020-01-02
      324.87
      2020-06-30
      0.286710
      70.85
      255.0
      0.128190
      0.0170
      0.13
      0.493151
      81.585331
    
    
      2020-01-02
      324.87
      2020-01-31
      0.118620
      6.17
      321.5
      0.128190
      0.0170
      0.13
      0.079452
      8.142697
    
    
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
    
    
      2022-12-30
      382.44
      2023-01-27
      0.169750
      0.38
      415.0
      0.189685
      0.0134
      0.33
      0.076712
      1.201606
    
    
      2022-12-30
      382.44
      2024-01-19
      0.167740
      1.86
      515.0
      0.189685
      0.0134
      0.33
      1.054795
      27.581163
    
    
      2022-12-30
      382.44
      2023-01-03
      
      0.00
      332.0
      0.189685
      0.0134
      0.33
      0.010959
      51.526183
    
    
      2022-12-30
      382.44
      2024-01-19
      0.193650
      0.16
      670.0
      0.189685
      0.0134
      0.33
      1.054795
      0.458140
    
    
      2022-12-30
      382.44
      2023-01-20
      0.681010
      0.01
      685.0
      0.189685
      0.0134
      0.33
      0.057534
      0.000000
    
  

96924 rows × 10 columns


The measure for quantiative models that are widely used in machine learning is the root mean square error.
As we use the binomial model as a benchmark, we will compute the RMSE for the benchmark binomial model.

# MSE
from sklearn.metrics import mean_squared_error


np.sqrt(mean_squared_error(df['call_last'], df['bm']))


51.301668827531735


The RMSE for the benchmark model is ~51 which is high.

The statistics for the absolute error is shown below. The median error is around 3.3.

plt.hist(np.abs(df['bm']-df['call_last']),bins = 50, density = True)
plt.title("A histogram of error (in absolute difference) of Binomial Model")
plt.ylabel("density")
plt.xlabel("Error")


Text(0.5, 0, 'Error')




Machine Learning Model

In addition to the (closed) underlying price, days-to-maturity, historical volatility, dividend yield, interest rate and strike price, we will also use the adjusted closed stock prices 8 days lag as inputs.

Tn = 8
spy = pd.read_csv('./spy.csv')
spy['Date'] = pd.to_datetime(spy['Date'], format = '%Y-%m-%d')
spy.set_index("Date",inplace= True)
spy = spy[['Adj Close']]
spy.rename(columns={"Adj Close": "t0"}, inplace = True)
spy_use = spy[spy.index &gt;= df.index[0]]

import warnings
warnings.filterwarnings("ignore")
for i in range(1,Tn+1):
    spy_use['t-'+str(i)] = spy['t0'].shift(i).iloc[-len(spy_use['t0']):]
df = df.join(spy_use)


# choose only numerical
df.drop(columns = ['maturity','implied_vol','bm','underlying_last'],inplace = True)


The first 5 rows of  final prepared dataset is shown below:

df.head()





  
    
      
      call_last
      K
      hist_vol
      q
      r
      T
      t0
      t-1
      t-2
      t-3
      t-4
      t-5
      t-6
      t-7
      t-8
    
  
  
    
      2020-01-02
      33.40
      300.0
      0.12819
      0.017
      0.13
      0.745205
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      100.30
      215.0
      0.12819
      0.017
      0.13
      0.243836
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      9.09
      316.0
      0.12819
      0.017
      0.13
      0.068493
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      70.85
      255.0
      0.12819
      0.017
      0.13
      0.493151
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      6.17
      321.5
      0.12819
      0.017
      0.13
      0.079452
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
  



The original data is timestamped with the time that each option is observed. The chronological order of the data may have an effect on the model, so it is important to take the order into account. For this option pricing project, we assume that the chronological order does not have a significant effect on the models. We make an assumption that the most chornological effects are contained in the features such as historical volatility and the stock price lags.

We shuffle and split 80% for traning set, 16% for test set, 4% for validation sets.

from sklearn.model_selection import train_test_split
import datetime
# We split 80% for traning set, 16% for test set, 4% for validation sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['call_last']),df['call_last'], test_size=0.2)
X_test, X_valid, y_test, y_valid = train_test_split(X_test,y_test, test_size=0.2)


# If you believe the order affect, use the code below
#X_train = df[df.index &lt;= datetime.datetime(2022,9,1)].copy()
#y_train = X_train[['call_last']]
#X_train = X_train.drop(columns = ['call_last'])

#X_test = df[df.index &gt; datetime.datetime(2022,9,1)].copy()
#y_test = X_test[['call_last']]
#X_test = X_test.drop(columns = ['call_last'])

#X_valid = X_test.iloc[:1000,:]
#y_valid = y_test.iloc[:1000]

#X_test = X_test.iloc[1000:,:]
#y_test = y_test.iloc[1000:]


As we obtained the dataset, we compute the option price based on American binomial tree model on the test set in order to compare it to other models. Since a number of the test set is small, we can apply the tree for large number of steps. In this case, we use 30 steps.

Benchmark (Binomial Tree)

bm_list = list()
for i in tqdm(range(len(X_test))):
    current_row = X_test.iloc[i,:]

    S0 = current_row['t0']
    K = current_row['K']
    sigma = current_row['hist_vol']
    r = current_row['r']
    q = current_row['q']
    T = current_row['T']
    bm_list.append(american_call_price(S0, K, sigma = sigma, t = T, r = r, q = q, N = 30 ))


100%|████████████████████████████████████| 15508/15508 [01:25&lt;00:00, 180.46it/s]


np.sqrt(mean_squared_error(y_test,bm_list))


48.219430763590715


The root mean squared error for the binomial model is 48.219430763590715.

Linear Regression

We use multiple linear regression with stdard scaled input.

from sklearn.linear_model import LinearRegression,SGDRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler


lin_reg = Pipeline([("std_scaler", StandardScaler()),
                     ("LinReg", LinearRegression())])
lin_reg.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;LinReg&#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;LinReg&#x27;, LinearRegression())])StandardScalerStandardScaler()LinearRegressionLinearRegression()

The root mean squared errors for training and testing set are shown below:

np.sqrt(mean_squared_error(y_train,lin_reg.predict(X_train)))


38.43095625260875


np.sqrt(mean_squared_error(y_test,lin_reg.predict(X_test)))


37.97049534873681


The coefficients and intercept for the model are shown below:

lin_reg["LinReg"].coef_


array([-37.65425523,   2.524246  ,   1.51641187,  -0.54682013,
        13.45049636,   5.76041445,   3.59107429,   3.32804942,
         3.28542487,  -0.09017812,  -0.74559882,  -1.94913388,
         0.6794828 ,   9.51963281])


lin_reg["LinReg"].intercept_


31.72985981248147


Polynomial Regression

For the polynomial regression, we only consider the polynomial of degree 2 with standard scaled input. We have 14 features, and the polynomial features are going to be large.

from sklearn.preprocessing import PolynomialFeatures
from scipy.stats import randint
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV


PolyReg = Pipeline([("std_scaler", StandardScaler()),
          ("poly_feature", PolynomialFeatures(degree=2, include_bias=False)),
         ("LinReg",  LinearRegression())])


PolyReg.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;poly_feature&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;LinReg&#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;poly_feature&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;LinReg&#x27;, LinearRegression())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures(include_bias=False)LinearRegressionLinearRegression()

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,PolyReg.predict(X_train)))


36.17182222027887


# Testing Loss
np.sqrt(mean_squared_error(y_test,PolyReg.predict(X_test)))


36.06896946882215


Ridge and Lasso Regression

For Ridge and Lasso Regression, we also find the best hyperparameters $\alpha$ using Grid Search.
We perform cross validation only on the first 10000 training data with 3-fold cross validation.

from sklearn.linear_model import LinearRegression,Ridge,Lasso


alpha = (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)
ridge_reg = Pipeline([("std_scaler", StandardScaler()),
         ("ridge",  Ridge())])
parameters = {'ridge__alpha': alpha}
rr = GridSearchCV(estimator=ridge_reg,param_grid = parameters, cv = 3)
rr.fit(X_train[:10000], y_train[:10000])




GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;ridge&#x27;, Ridge())]),
             param_grid={&#x27;ridge__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;ridge&#x27;, Ridge())]),
             param_grid={&#x27;ridge__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge())])StandardScalerStandardScaler()RidgeRidge()

rr.best_estimator_



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])StandardScalerStandardScaler()RidgeRidge(alpha=10)

We found that $\alpha = 10$ is the best alpha.
We then fit this best estimator to the whole dataset.

best_ridge = rr.best_estimator_
best_ridge.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])StandardScalerStandardScaler()RidgeRidge(alpha=10)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_ridge.predict(X_train)))


38.43095969174494


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_ridge.predict(X_test)))


37.97019255417378


We did the same to Lasso.

alpha = (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)
lasso_reg = Pipeline([("std_scaler", StandardScaler()),
         ("lasso",  Lasso())])
parameters = {'lasso__alpha': alpha}
lr = GridSearchCV(estimator=lasso_reg, param_grid  = parameters, cv = 3)
lr.fit(X_train[:10000], y_train[:10000])



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso())])StandardScalerStandardScaler()LassoLasso()

lr.best_estimator_
best_lasso = lr.best_estimator_
best_lasso



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])StandardScalerStandardScaler()LassoLasso(alpha=0.01)

best_lasso.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])StandardScalerStandardScaler()LassoLasso(alpha=0.01)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_lasso.predict(X_train)))


38.431561232353104


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_lasso.predict(X_test)))


37.969146928057036


The best $\alpha$ is 0.01.

We can use Lasso for feature selection as it tries to minimize unimportant features coefficients to 0.

lr.best_estimator_['lasso'].coef_


array([-37.64057787,   2.49377087,   1.45139028,  -0.55584586,
        13.43936913,   5.72526309,   3.68045696,   3.23670677,
         2.1807758 ,   0.        ,  -0.        ,  -0.        ,
         0.        ,   8.50465773])


X_train.columns


Index(['K', 'hist_vol', 'q', 'r', 'T', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',
       't-6', 't-7', 't-8'],
      dtype='object')


Interestingly many of the lags of stock prices are not important.

Support Vector Regressor

For the linear, we tune the hyperparameter C and $\epsilon$ for linear SVR.
The other setting are the same as in previous.

from sklearn.svm import LinearSVR

lsvr = Pipeline([("std_scaler", StandardScaler()),
                 ('svr',LinearSVR())])
parameters = {'svr__epsilon':(0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000),
             'svr__C': (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)}
lsvr_gs = GridSearchCV(estimator=lsvr,param_grid = parameters, cv = 3)
lsvr_gs.fit(X_train[:10000], y_train[:10000])



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, LinearSVR())]),
             param_grid={&#x27;svr__C&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,
                                    1000, 5000, 10000),
                         &#x27;svr__epsilon&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, LinearSVR())]),
             param_grid={&#x27;svr__C&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,
                                    1000, 5000, 10000),
                         &#x27;svr__epsilon&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, LinearSVR())])StandardScalerStandardScaler()LinearSVRLinearSVR()

lsvr_gs.best_estimator_



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])StandardScalerStandardScaler()LinearSVRLinearSVR(C=10, epsilon=10)

best_lsvr = lsvr_gs.best_estimator_


best_lsvr.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])StandardScalerStandardScaler()LinearSVRLinearSVR(C=10, epsilon=10)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_lsvr.predict(X_train)))


38.547851070665025


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_lsvr.predict(X_test)))


38.03050061254464


The tuned c and epsilon are 10 and 10 respectively.

Now, we consider nonlinear SVR,in addtion to $\epsilon$ and $C$, we include a type of kernel as a hyperparamter. We choose between rbf and sigmoid. Note that we reduce search space for $\epsilon$ and $C$ to accelerate the computing time.

from sklearn.svm import SVR


nlsvr = Pipeline([("std_scaler", StandardScaler()),
                 ('svr',SVR())])
parameters = {'svr__kernel':('rbf', 'sigmoid'),
              'svr__epsilon':(0.001, 0.01,0.1,1,100,5000),
             'svr__C': (0.001, 0.01,0.1,1,100,5000)
              }
nlsvr_gs = GridSearchCV(estimator=nlsvr,param_grid = parameters, cv = 3,n_jobs = -1)
nlsvr_gs.fit(X_train[:10000].to_numpy(), y_train[:10000].to_numpy().ravel())



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, SVR())]),
             n_jobs=-1,
             param_grid={&#x27;svr__C&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__epsilon&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__kernel&#x27;: (&#x27;rbf&#x27;, &#x27;sigmoid&#x27;)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, SVR())]),
             n_jobs=-1,
             param_grid={&#x27;svr__C&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__epsilon&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__kernel&#x27;: (&#x27;rbf&#x27;, &#x27;sigmoid&#x27;)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, SVR())])StandardScalerStandardScaler()SVRSVR()

best_nlsvr = nlsvr_gs.best_estimator_
best_nlsvr



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])StandardScalerStandardScaler()SVRSVR(C=100, epsilon=1)

best_nlsvr.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])StandardScalerStandardScaler()SVRSVR(C=100, epsilon=1)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_nlsvr.predict(X_train)))


37.808085641917536


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_nlsvr.predict(X_test)))


37.92867379033997


Random Forest

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV


There are so many parameters, so we use random search instead of grid search. We randomly sample hyperparameters uniformly from the parameters lists for 100 samples. And we use first 10000 shuffled data entries for this tuning.

rf_rg = Pipeline([('std_scaler', StandardScaler()),
                     ('rf', RandomForestRegressor())])
parameters = {'rf__n_estimators': (50,100,300,400,500),
             'rf__max_depth':(None, 8,32,64,128),
             'rf__ccp_alpha':(0,0.00000001,0.00001,0.001),
             'rf__bootstrap': [True, False]}
rf_gs =RandomizedSearchCV(estimator=rf_rg,param_distributions = parameters, cv = 3,n_jobs = -1,n_iter = 100)


rf_gs.fit(X_train[:10000].to_numpy(), y_train[:10000].to_numpy().ravel())



RandomizedSearchCV(cv=3,
                   estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                             (&#x27;rf&#x27;, RandomForestRegressor())]),
                   n_iter=160, n_jobs=-1,
                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],
                                        &#x27;rf__ccp_alpha&#x27;: (0, 1e-08, 1e-05,
                                                          0.001),
                                        &#x27;rf__max_depth&#x27;: (None, 8, 32, 64, 128),
                                        &#x27;rf__n_estimators&#x27;: (50, 100, 300, 400,
                                                             500)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,
                   estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                             (&#x27;rf&#x27;, RandomForestRegressor())]),
                   n_iter=160, n_jobs=-1,
                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],
                                        &#x27;rf__ccp_alpha&#x27;: (0, 1e-08, 1e-05,
                                                          0.001),
                                        &#x27;rf__max_depth&#x27;: (None, 8, 32, 64, 128),
                                        &#x27;rf__n_estimators&#x27;: (50, 100, 300, 400,
                                                             500)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;, RandomForestRegressor())])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor()

best_rf = rf_gs.best_estimator_


best_rf



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)

best_rf.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)

The root mean squared errors for training and testing set are shown below:

The root mean squared errors for training and testing set are shown below:# Training Loss
np.sqrt(mean_squared_error(y_train,best_rf.predict(X_train)))


31.349066580782836


#Testing RMSE
np.sqrt(mean_squared_error(y_test,best_rf.predict(X_test)))


32.90546500222834


With Random Forest, we can see feature importance:

best_rf['rf'].feature_importances_


array([0.51617093, 0.04662939, 0.00114048, 0.00432427, 0.13790168,
       0.04360533, 0.0377016 , 0.02562464, 0.03328482, 0.01493993,
       0.05282551, 0.00782497, 0.01813578, 0.05989066])


X_train.columns
# Strike Price and Time to maturity seem to be the most important


Index(['K', 'hist_vol', 'q', 'r', 'T', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',
       't-6', 't-7', 't-8'],
      dtype='object')


The best hyperparamters are ccp_alpha=0, max_depth=8, n_estimators=400, and bootstrap = False.

K-Nearest Neighbors

In this model, we only tune a number of neighbors as a hyperparameter.

from sklearn.neighbors import KNeighborsRegressor


knn_rg = Pipeline([('std_scaler', StandardScaler()),
                  ('knn',KNeighborsRegressor())])
parameters = {'knn__n_neighbors': np.arange(5,100,5)}


knn_gs = GridSearchCV(estimator=knn_rg,param_grid = parameters, cv = 3,n_jobs = -1)
knn_gs.fit(X_train[:10000], y_train[:10000])



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;knn&#x27;, KNeighborsRegressor())]),
             n_jobs=-1,
             param_grid={&#x27;knn__n_neighbors&#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,
       90, 95])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;knn&#x27;, KNeighborsRegressor())]),
             n_jobs=-1,
             param_grid={&#x27;knn__n_neighbors&#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,
       90, 95])})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()

best_knn = knn_gs.best_estimator_
best_knn



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor(n_neighbors=15)

best_knn.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor(n_neighbors=15)

The root mean squared errors for training and testing set are shown below:

np.sqrt(mean_squared_error(y_train,best_knn.predict(X_train)))


31.92967572127278


np.sqrt(mean_squared_error(y_test,best_knn.predict(X_test)))


32.912277382176526


The best number of the neigbors is 15.

Multi-layer perceptron

For neutral network, we make experiments on three models, which are multilayer perceptron (with dropout and normalization, with relu as an activation function for the hidden layers and identity for the output layer), multilayer perceptron with tuned hyperparameters, and Convolutional Neural network.

from tensorflow.keras import Sequential, layers
import tensorflow as tf


2023-05-31 17:15:39.270253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.


model = tf.keras.Sequential([
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(30, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(30, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(20, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation= None)
])


callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50,restore_best_weights=True)
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train, y_train, epochs=1000,
                    validation_data=(X_valid, y_valid),callbacks=[callback])


Epoch 1/1000
2424/2424 [==============================] - 8s 2ms/step - loss: 2348.1985 - val_loss: 1538.4142
Epoch 2/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1858.8630 - val_loss: 1519.5778
Epoch 3/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1826.4266 - val_loss: 1481.3267
Epoch 4/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1821.6594 - val_loss: 1479.6355
Epoch 5/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1815.7463 - val_loss: 1452.8606
Epoch 6/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1803.6791 - val_loss: 1457.6923
Epoch 7/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1796.4032 - val_loss: 1453.8772
Epoch 8/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1796.3320 - val_loss: 1446.6718
Epoch 9/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1788.8895 - val_loss: 1437.1597
Epoch 10/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1762.4237 - val_loss: 1441.7910
Epoch 11/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1789.3596 - val_loss: 1461.8259
Epoch 12/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1782.5536 - val_loss: 1455.1730
Epoch 13/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1772.4648 - val_loss: 1450.3599
Epoch 14/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1753.4039 - val_loss: 1453.5958
Epoch 15/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1778.0695 - val_loss: 1447.4929
...
Epoch 106/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1699.8079 - val_loss: 1426.6115
Epoch 107/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1737.1375 - val_loss: 1446.8939
Epoch 108/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1710.6053 - val_loss: 1421.2778


model.summary()


Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 14)               56        
 ormalization)                                                   

 dropout (Dropout)           (None, 14)                0         

 dense (Dense)               (None, 30)                450       

 batch_normalization_1 (Batc  (None, 30)               120       
 hNormalization)                                                 

 dropout_1 (Dropout)         (None, 30)                0         

 dense_1 (Dense)             (None, 30)                930       

 batch_normalization_2 (Batc  (None, 30)               120       
 hNormalization)                                                 

 dropout_2 (Dropout)         (None, 30)                0         

 dense_2 (Dense)             (None, 20)                620       

 batch_normalization_3 (Batc  (None, 20)               80        
 hNormalization)                                                 

 dropout_3 (Dropout)         (None, 20)                0         

 dense_3 (Dense)             (None, 10)                210       

 batch_normalization_4 (Batc  (None, 10)               40        
 hNormalization)                                                 

 dropout_4 (Dropout)         (None, 10)                0         

 dense_4 (Dense)             (None, 1)                 11        

=================================================================
Total params: 2,637
Trainable params: 2,429
Non-trainable params: 208
_________________________________________________________________


The learning curve is shown below:

plt.plot(history.history['loss'],label = "loss")
plt.plot(history.history['val_loss'], label = "val_loss")
plt.legend()
plt.ylabel("MSE")
plt.xlabel("Epoch")


Text(0.5, 0, 'Epoch')




The root mean squared errors for training and testing set are shown below:

#trainig RMSE
np.sqrt(mean_squared_error(y_train,model.predict(X_train)))


2424/2424 [==============================] - 2s 714us/step





36.21230441027238


#testing RMSE
np.sqrt(mean_squared_error(y_test,model.predict(X_test)))


485/485 [==============================] - 0s 675us/step





35.71456726735933


The model seems to be underfitting.

Hyperparamter-tuned MLP

We use keras tuner library to tune the hyperparameters of the network. The hyperparameter space is shown below.

import keras_tuner as kt

def build_model(hp):
    n_hidden = hp.Int("n_hidden", min_value=1, max_value=8)
    n_neurons = hp.Int("n_neurons", min_value=1, max_value=100)
    learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2,
                             sampling="log")
    l2_rate = hp.Float("l2", min_value=1e-4, max_value=100,
                             sampling="log")
    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)

    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Normalization(input_shape=X_train.shape[1:]))
    for _ in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation="relu",kernel_initializer="he_normal",kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))
    model.add(tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))
    model.compile(loss="mse", optimizer=optimizer)
    return model


random_search_tuner = kt.RandomSearch(
    build_model, objective="val_loss", max_trials=20, seed=42)
random_search_tuner.search(X_train[:5000], y_train[:5000], epochs=150,
                           validation_data=(X_valid, y_valid))



Trial 20 Complete [00h 00m 53s]
val_loss: 1461.2305908203125

Best val_loss So Far: 1451.614013671875
Total elapsed time: 00h 15m 31s
INFO:tensorflow:Oracle triggered exit


random_search_tuner.get_best_hyperparameters()[0].values


{'n_hidden': 7,
 'n_neurons': 15,
 'learning_rate': 0.0006237028864858578,
 'l2': 0.0003065801184974072}


The best hyperparameters we found are 7 hidden layers with 15 neurons each, 0.0006237028864858578 learning rate, and l2 = 0.0003065801184974072 for the l2 regularization.

best_nn = random_search_tuner.get_best_models()[0]


callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100,restore_best_weights=True)
history_best_nn = best_nn.fit(X_train, y_train, epochs=1000,
                    validation_data=(X_valid, y_valid),callbacks=[callback])

Epoch 1/1000
2424/2424 [==============================] - 6s 1ms/step - loss: 1364.6182 - val_loss: 1498.1863
Epoch 2/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1363.1213 - val_loss: 1462.6725
Epoch 3/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1354.9094 - val_loss: 1493.1799
Epoch 4/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1349.7559 - val_loss: 1452.7467
Epoch 5/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1347.2516 - val_loss: 1499.4685
Epoch 6/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1341.6652 - val_loss: 1437.6245
Epoch 7/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.0056 - val_loss: 1423.4025
Epoch 8/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.5402 - val_loss: 1513.4679
Epoch 9/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.7520 - val_loss: 1569.9033
...
Epoch 556/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1242.6877 - val_loss: 1332.1714
Epoch 557/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1260.6245 - val_loss: 1501.7150
Epoch 558/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1245.4034 - val_loss: 1347.1127


The root mean squared errors for training and testing set are shown below:

# Traing RMSE
np.sqrt(mean_squared_error(y_train,best_nn.predict(X_train)))


2424/2424 [==============================] - 2s 597us/step

34.78459569413211


# Testing RMSE
np.sqrt(mean_squared_error(y_test,best_nn.predict(X_test)))


485/485 [==============================] - 0s 612us/step

34.821059741172114


The learning curve is shown below.

plt.plot(history_best_nn.history['loss'],label = "loss")
plt.plot(history_best_nn.history['val_loss'], label = "val_loss")
plt.legend()
plt.ylabel("MSE")
plt.xlabel("Epoch")


Text(0.5, 0, 'Epoch')




Convolutional Neural Network (CNN)

The structure of the convolutional network is shown below.

model_conv = tf.keras.Sequential([
    tf.keras.layers.Conv1D(32,2,activation = 'relu',input_shape = (14,1,), kernel_initializer="he_normal",padding = 'same'),
    tf.keras.layers.Conv1D(32,3,activation = 'relu', kernel_initializer="he_normal",padding = 'same'),
    tf.keras.layers.Conv1D(32,3,activation = 'relu', kernel_initializer="he_normal"),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(64, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(32, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(1, activation= None)
])




callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50,restore_best_weights=True)
model_conv.compile(loss="mse", optimizer="nadam")
history_cov = model_conv.fit(X_train, y_train, epochs=1000,
                    validation_data=(X_valid, y_valid),callbacks=[callback])


Epoch 1/1000
2424/2424 [==============================] - 7s 2ms/step - loss: 1632.2096 - val_loss: 1460.4231
Epoch 2/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1373.3662 - val_loss: 1505.1824
Epoch 3/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1366.6722 - val_loss: 1474.0969
Epoch 4/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1346.9469 - val_loss: 1475.6388
Epoch 5/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1334.1947 - val_loss: 1547.0897
Epoch 6/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1323.7288 - val_loss: 1439.5723
Epoch 7/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1310.3964 - val_loss: 1432.5355
Epoch 8/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1294.4939 - val_loss: 1449.5792
Epoch 9/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1290.2340 - val_loss: 1363.4564
...
Epoch 244/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1013.4796 - val_loss: 1174.7893
Epoch 245/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1002.9050 - val_loss: 1177.7996


model_conv.summary()


Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 14, 32)            96        

 conv1d_1 (Conv1D)           (None, 14, 32)            3104      

 conv1d_2 (Conv1D)           (None, 12, 32)            3104      

 flatten (Flatten)           (None, 384)               0         

 dense_8 (Dense)             (None, 128)               49280     

 dense_9 (Dense)             (None, 64)                8256      

 dense_10 (Dense)            (None, 32)                2080      

 dense_11 (Dense)            (None, 1)                 33        

=================================================================
Total params: 65,953
Trainable params: 65,953
Non-trainable params: 0
_________________________________________________________________


The learning curve is shown below.

plt.plot(history_cov.history['loss'],label = "loss")
plt.plot(history_cov.history['val_loss'], label = "val_loss")
plt.legend()
plt.ylabel("MSE")
plt.xlabel("Epoch")


Text(0.5, 0, 'Epoch')




The root mean squared errors for training and testing set are shown below:

# Training RMSE
np.sqrt(mean_squared_error(y_train,model_conv.predict(X_train)))


2424/2424 [==============================] - 2s 920us/step





31.66131974960281


# Testing RMSE
np.sqrt(mean_squared_error(y_test,model_conv.predict(X_test)))


485/485 [==============================] - 0s 814us/step





32.372653521773714


Result

The Root Mean Square Error for each machine learning models are shown below, together with the chosen hyperparamters.

res = pd.DataFrame({"Model": ["Binomial Tree",'Linear Regression',"Polynomial Regression","Ridge Regression","Lasso Regression","Linear SVR","Nonlinear SVR","Random Forest","KNN","Neural Network", "Neural Network (tuned)", " Convolutional Neutral Network"],
                   "RMSE(training)":[None,
38.43095625260875,
36.17182222027887,
38.43095969174494,
38.431561232353104,
38.547851070665025,
37.808085641917536,
31.349066580782836,
31.92967572127278,
36.21230441027238,
34.78459569413211,
31.66131974960281
],
                "RMSE(testing)":
                   [48.219430763590715,
                    37.97049534873681,
36.06896946882215,
37.97019255417378,
37.969146928057036,
38.03050061254464,
37.92867379033997,
32.90546500222834,
32.912277382176526,
35.71456726735933,
34.821059741172114,
32.372653521773714
],
"Hyperparameters" :
        ["29 steps","",
"degree = 2",
"alpha = 10",
"alpha = 0.01",
"C=10, epsilon=10",
"C=100, epsilon=1",
"ccp_alpha=0, max_depth=8, n_estimators=400, bootstrap = False",
         "neighbors = 15",
        "consider model summary above",
        "consider model summary above",
        "consider model summary above"]    
                   })
res





  
    
      
      Model
      RMSE(training)
      RMSE(testing)
      Hyperparameters
    
  
  
    
      0
      Binomial Tree
      NaN
      48.219431
      29 steps
    
    
      1
      Linear Regression
      38.430956
      37.970495
      
    
    
      2
      Polynomial Regression
      36.171822
      36.068969
      degree = 2
    
    
      3
      Ridge Regression
      38.430960
      37.970193
      alpha = 10
    
    
      4
      Lasso Regression
      38.431561
      37.969147
      alpha = 0.01
    
    
      5
      Linear SVR
      38.547851
      38.030501
      C=10, epsilon=10
    
    
      6
      Nonlinear SVR
      37.808086
      37.928674
      C=100, epsilon=1
    
    
      7
      Random Forest
      31.349067
      32.905465
      ccp_alpha=0, max_depth=8, n_estimators=400, bo...
    
    
      8
      KNN
      31.929676
      32.912277
      neighbors = 25
    
    
      9
      Neural Network
      36.212304
      35.714567
      consider model summary above
    
    
      10
      Neural Network (tuned)
      34.784596
      34.821060
      consider model summary above
    
    
      11
      Convolutional Neutral Network
      31.661320
      32.372654
      consider model summary above
    
  



Consider the RMSE for the testing set, the lower RMSE, the more accurate the model to compute option price. By this sole metric, all the machine learning model we used in this project performs better than the binomial tree.
By ranking the models by this value, we have that the Convolutional Neural Network performs the best, the random forest is the second best, and the K-Nearest Neighbors is the third.

The statistics for the absolute error for the best three models and the binomial tree are shown below.

res_error = pd.concat([pd.DataFrame({"binomial abs_error": np.abs(np.array(bm_list) -y_test.to_numpy().flatten())}).describe(),
                        np.abs(model_conv.predict(X_test).ravel() - y_test).describe(),
          np.abs((best_rf.predict(X_test).ravel()-y_test)).describe(),
                       np.abs(best_knn.predict(X_test).ravel() - y_test ).describe(),],axis = 1)


res_error.columns = ["Binomial Tree Absolute Error","CNN Absolute Error","Random Forest Absolute Error","KNN Absolute Error"]
res_error


485/485 [==============================] - 0s 908us/step





  
    
      
      Binomial Tree Absolute Error
      CNN Absolute Error
      Random Forest Absolute Error
      KNN Absolute Error
    
  
  
    
      count
      15508.000000
      15508.000000
      15508.000000
      15508.000000
    
    
      mean
      22.192882
      16.865260
      17.565473
      17.039533
    
    
      std
      42.810135
      27.633330
      27.825774
      28.158896
    
    
      min
      0.000000
      0.000103
      0.000018
      0.000000
    
    
      25%
      0.749303
      0.927289
      1.313031
      0.710667
    
    
      50%
      7.469478
      4.053897
      5.481253
      4.335000
    
    
      75%
      17.896939
      22.537766
      22.226330
      21.908333
    
    
      max
      369.079773
      350.456573
      338.299600
      341.098000
    
  



By considering the median for the absolute errors for each model, we can see that those of the machine learing models are smaller than that from the benchmark model. This implies that for at least half of the dataset, the absolute error from the machine learing models are smaller than that from the benchmark.
However, if we consider other quantitle, we can see that there are not much different. But for most part, the machine learning models have smaller errors.

Stock Price versus Predicted Call Price

By the RMSE on the testing set, we choose the CNN to be the best model.
We visualize $C_0$ as a function of strike price $K$ and the stock price $S_0$.

res_pt = pd.concat([X_test[['t0']].set_index(np.arange(0,len(X_test))).rename(columns = {"t0":"stock price"}),
                    X_test[['K']].set_index(np.arange(0,len(X_test))).rename(columns = {"K":"K"}),
            pd.DataFrame({"Call Price from the dataset": y_test.to_numpy().ravel()}),
           pd.DataFrame({"Call Price from Binomial Tree": bm_list}),
           pd.DataFrame({"Call Price from CNN": model_conv.predict(X_test).ravel()}),
          pd.DataFrame({"lower bound":
              [np.max([lb,0]) for lb in X_test['t0'] - X_test['K']*np.exp(-X_test['r'] *X_test['T'])]})], axis = 1)


485/485 [==============================] - 1s 1ms/step


# Percentage of call price greater than 120
np.sum(y_test.to_numpy() &gt;= 120)/len(y_test)


0.06970595821511479


fig,axes = plt.subplots(1,3, figsize = (21,8),subplot_kw={'projection': '3d'})
axes[0].scatter(res_pt['stock price'], res_pt["K"],res_pt["Call Price from the dataset"], s= 1)
axes[0].set_xlabel("$S_0$")
axes[0].set_ylabel("$K$")
axes[0].set_zlabel("$C_0$")
axes[0].view_init(elev=20., azim=145, roll=0)
axes[0].set_title("True")
axes[0].set_zlim([0,320])

axes[1].scatter(res_pt['stock price'], res_pt["K"],res_pt["Call Price from Binomial Tree"], s= 1)
axes[1].set_xlabel("$S_0$")
axes[1].set_ylabel("$K$")
axes[1].set_zlabel("$C_0 $")
axes[1].view_init(elev=20., azim=145, roll=0)
axes[1].set_title("Binomial Tree")
axes[1].set_zlim([0,320])


axes[2].scatter(res_pt['stock price'], res_pt["K"],res_pt["Call Price from CNN"], s= 1)
axes[2].set_xlabel("$S_0$")
axes[2].set_ylabel("$K$")
axes[2].set_zlabel("$C_0 $")
axes[2].set_title("CNN")
axes[2].view_init(elev=20., azim=145, roll=0)
axes[2].set_zlim([0,320])


plt.show()




By considering only two factors, $S_0$ and $K$, we can see some paterrns in the corresponding option price. All the three plots look similar, indicating the accuracy of the binomial tree and the Convolutional Neural Network to price options. However, in the convolutional neural network, we can see that the call prices are dense around the small values similar to the actual price, however it did not well capture high call prices. For the benchmark, the prices are more sparse compared to both actual price and the CNN.

Now, we visualize an option price as a function of stock price, fixing other variable.

We fix other variable by choosing those variables from the dataset.

Then, we compare an option price as a function of $S_0$ from Binomial Tree and the best model.

Remark that, the machine learning models require additional parameters which are stock prices 8 days in the past. We cannot fix these stock prices; otherwise, the underlying price is unrealistic (eg. stock price at 0 is 500, while those at -1, -2, are around 200).

For each $S_0$, we find relevant $S_{-1}, S_{-2}$, …, in the dataset.

We also compute a upperbound and lowerbound as shown below.

The upper and lower bound for American option (for non-dividend) can be computed by
\(\max(S_t - K e^{-r (T-t)},0)\leq C_t \leq S_t\)

# Choose only S_0_x (rounded) that we use to train model
S_0_x = np.unique(np.round(spy_use['t0']) )
binomial_vs_s0 = [american_call_price(xs0, sample_fixed['K'][0], sigma = sample_fixed['hist_vol'][0], t = sample_fixed['T'][0], r = sample_fixed['r'][0], q = sample_fixed['q'][0], N = 30 ) for xs0 in S_0_x ]
spy_extended = spy_use.copy()
spy_extended['round_t0'] = np.round(spy_use['t0'])
s0_dataset = []
for xs0 in S_0_x:
    s0_dataset.append(np.concatenate([sample_fixed.drop(columns = ["call_last",'t0']).to_numpy()[0,:5], spy_extended[spy_extended['round_t0'] == xs0].sample(1).iloc[0,:9].ravel()]))
s0_dataset = np.array(s0_dataset)
cnn_vs_s0 = model_conv.predict(s0_dataset).ravel()


7/7 [==============================] - 0s 1ms/step


max_bound = S_0_x
min_bound = [np.max([xs0 - sample_fixed['K'][0] * np.exp(-sample_fixed['r'][0]*sample_fixed['T'][0]),0]) for xs0 in S_0_x ]


plt.plot(S_0_x ,binomial_vs_s0,label = 'binomial')
plt.plot(S_0_x ,cnn_vs_s0,label = 'CNN')
plt.plot(S_0_x ,max_bound,label = 'max bound')
plt.plot(S_0_x ,min_bound,label = 'min bound',ls = 'dashed')
plt.ylim([0,100])
plt.legend()


&lt;matplotlib.legend.Legend at 0x7f844f12e160&gt;




The plot aligns with the previous plot, suggesting consistency. However, it appears that the CNN model does not accurately capture the high call prices. The generated call prices are observed to be outside the lower bound for high underlying price, creating the potential for arbitrage opportunities. Therefore, it is crucial to exercise caution and conduct a thorough examination of the model before implementing it. Additionally, fine-tuning or training the CNN model with a larger and more comprehensive dataset could potentially improve its performance and yield a better model.

Conclusion

In this project, we have utilized machine learning models to price American options based on the SPY dataset. Our findings indicate that all of the machine learning models outperform the traditional binomial model for both the testing and training sets. Among these models, CNN, random forest, and KNN show promise, as their testing and training losses are relatively low.

However, it is worth noting that even though CNN performs the best overall, it struggles with accurately computing high call prices. Therefore, further tuning and dataset preparation might be necessary to enhance its performance in this regard.

In conclusion, machine learning models demonstrate the ability to capture the relationship between financial information and option pricing. Through the utilization of the RMSE metric, these models outperform the traditional benchmark model. Nonetheless, additional tunin

Acknoledgement

Thanks all people who gathered the data and publicly publish them online. Thank you Dr. Kevin Lu for giving suggestions on American option pricing models.

References

Culkin, Robert, and Sanjiv R. Das. Machine Learning in Finance: The Case of Deep Learning for Option Pricing, 2 Aug. 2017, srdas.github.io/Papers/BlackScholesNN.pdf.

French, Kenneth R. Kenneth R. French - Data Library, mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html. Accessed 3 June 2023.

Graupe, Kyle. “$spy Option Chains - Q1 2020 - Q4 2022.” Kaggle, 14 Mar. 2023,
www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022.

Lu, Kevin. “Machine Learning for Finance Lectures.” CFRM 421:Machine Learning for Finance . Seattle, The University of Washington.

Lu, Kevin. “Introduction to Financial Markets Lectures.” CFRM 415: Introduction to Financial Markets. Seattle, The University of Washington.

Mooney, Kevin, director. Implementing the Binomial Option Pricing Model in Python, YouTube, 15 Feb. 2021, https://www.youtube.com/watch?v=d7wa16RNRCI. Accessed 5 June 2023.

SPY Dividend Yield, ycharts.com/companies/SPY/dividend_yield. Accessed 3 June 2023.
" />
<meta property="og:description" content="Wanchaloem Wunkaew 
leegarap@uw.edu 
University of Washington, Seattle, WA

This is an individual final project for CFRM 421/521: Machine Learning for Finance class at the University of Washington.

Note that this project is not peer-reviewed and that the project is for educational purpose. Please use it as your own risk.

The jupyter notebook of this project is accessible at this link

Abstract 

We proprose machine learning methods for regression including Linear Regression, Polynomial Regression, Support Vector Regressor Ridge and Lasso Regression, Random Forest Regressor, K-Nearest Neighbors Regression, Multi-layer perceptons, and Convolutional Neural Network to price American options. The methods are trained and tested on SPDR S&amp;P 500 ETF Trust call options, from 2021 to 2022.  The result shows that Convolutional Neural Network and the Random Forest performs better than the Binomial Tree, which we use as a benchmark, in the term of testing Root Mean Squared Errors.

Introduction

Option pricing is one of fields in financial engineering. The formalization of option pricing methods, such as the Black-Scholes equation, has greatly impactedthe field of financial economics. Among various types of options, the American Option is a distinct financial asset that grants its holder the right to buy or sell the underlying asset at any point up to, and including, its maturity date. Unlike European options, American options do not have a closed-form solution, so it requires the use of numerical methods for their pricing. The Binomial Tree and Monte Carlo simulations are two such numerical methods capable of pricing these options. One notable limitation in all option pricing methods is the unrealistic assumptions of underlying asset price models. For instance, the Geometric Brownian motion model, which is assumed in the Black-Scholes equation, does not account for heteroskedasticity and the non-normal log return. In addition, some parameters, such as $\sigma$ in the binomial tree, in these tradtional methods/models are hard to estimate.

In this project, we employ a range of machine learning regression models, including linear regression, polynomial regression, ridge regression, lasso regression, Support Vector Regressor, Random Forest Regressor, K-Nearest Neighbor regressor, Multilayer Perceptron regressor, and Convolutional Neural Network, to price American options. We anticipate that these models may unveil relationships between inputs (such as the strike price and stock prices from 8 days prior) and the output (the option price). As such, we expect these models to either outperform or match the performance of the traditional Binomial Tree model, which we have selected as our research benchmark. Additionally, we believe that some of these models can rectify the flaws of traditional models as outlined above.

We will divide this paper into distinct sections. In the next section, we will discuss our dataset and the Binomial Tree, which serves as our benchmark. The third section will be devoted to training machine learning models on this dataset. In the fourth section, we will summarize and discuss our findings. Lastly, in the fifth section, we will draw conclusions based on our results.

Data Preparation and Benchmark model

We will employ the SPDR S&amp;P 500 ETF Trust option chains from Q1 2020-Q4 2022 for our analysis. This data, which consists of more than three million options traded in markets, was downloads from Kaggle.  The dataset encompasses a wealth of information, including but not restricted to, the closing option price, the closing strike price, underlying asset price, bid and ask prices, and implied volatility. Despite the fact that the dataset includes put option data, our study will only concentrate on call options.

The features incorporated in this project consist of: strike price, dividend yield, risk-free rate, the time until the option’s maturity, historical volatility, and the underlying asset (adjusted closed) prices from seven days prior (including the closed price on the date that the option is traded). A majority of these features encompass parameters used to price options in the Binomial Tree model. Note that we assume no transaction fee.

The historical volatility was calculated by the standard deviation of the logarithmic return of the underlying asset over the five years preceding the date each option was observed. The risk-free rate was obtained from that Fama-French guy website. The stock prices were obtained from Yahoo Finance via yfinance as showed below, while the dividend yield was estimated from  Ycharts.

Despite the dataset’s size with more than three million option data entries, we will randomly sample 100,000 options for training and testing our models due to time limitation. Also, we will choose only first 10000 of the traning data for cross validation.

The output of the model is solely the corresponding call option price, denoted as “’ [C_LAST]’’ within the dataset.

%matplotlib inline


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
from pandas.plotting import scatter_matrix


# get data from https://www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022
df = pd.read_csv("./spy_2020_2022.csv", low_memory=False)


# Randomly chose 100000 samples for traning, validating, and testing
df = df.sample(100000,random_state = 42)


The columns of the option data are shown below:

df.columns


Index(['[QUOTE_UNIXTIME]', ' [QUOTE_READTIME]', ' [QUOTE_DATE]',
       ' [QUOTE_TIME_HOURS]', ' [UNDERLYING_LAST]', ' [EXPIRE_DATE]',
       ' [EXPIRE_UNIX]', ' [DTE]', ' [C_DELTA]', ' [C_GAMMA]', ' [C_VEGA]',
       ' [C_THETA]', ' [C_RHO]', ' [C_IV]', ' [C_VOLUME]', ' [C_LAST]',
       ' [C_SIZE]', ' [C_BID]', ' [C_ASK]', ' [STRIKE]', ' [P_BID]',
       ' [P_ASK]', ' [P_SIZE]', ' [P_LAST]', ' [P_DELTA]', ' [P_GAMMA]',
       ' [P_VEGA]', ' [P_THETA]', ' [P_RHO]', ' [P_IV]', ' [P_VOLUME]',
       ' [STRIKE_DISTANCE]', ' [STRIKE_DISTANCE_PCT]'],
      dtype='object')


# load option data
#df = pd.read_csv("./spy_20_21.csv")
#df = df.iloc[:,1:]
df[' [QUOTE_DATE]'] = pd.to_datetime(df[' [QUOTE_DATE]'], format = ' %Y-%m-%d')
df.set_index(" [QUOTE_DATE]", inplace = True)
df = df[[' [UNDERLYING_LAST]',' [EXPIRE_DATE]',' [C_IV]',' [C_LAST]',' [STRIKE]']]
df.columns = ['underlying_last','maturity','implied_vol','call_last', 'K']

# load stock data
#import yfinance as yf
# spy = yf.download('SPY', '2010-01-01', '2023-02-01')
# spy.to_csv('spy.csv')

spy = pd.read_csv('./spy.csv')
spy['Date'] = pd.to_datetime(spy['Date'], format = '%Y-%m-%d')
df['maturity']=pd.to_datetime(df['maturity'], format = ' %Y-%m-%d')
spy.set_index('Date', inplace = True)
# Choose only data and adjust close
spy = spy[['Adj Close']]

# Computing log return
spy = pd.DataFrame((np.log(spy['Adj Close'].shift(-1)) - np.log(spy['Adj Close'])).dropna())
spy.columns = ['ret']


Compute historical volatility $q$ by finding the std of the log return 1825 days (~5 years) in back in the past.

n_days_hist = 365 * 5
date_ls = list()
vol_ls = list()
spy_array = spy['ret'].to_numpy()
for i in range(n_days_hist,len(spy_array)):
    date_ls.append(spy.index[i])
    vol_ls.append(np.std(spy_array[i-n_days_hist:i+1])*np.sqrt(252))
hist_vol_df = pd.DataFrame({'Date': date_ls, "hist_vol":vol_ls})
hist_vol_df.set_index('Date', inplace = True)


df = df.join(hist_vol_df)


Dividend Yields are obtained and approximated from ychart website.

# Dividend Yield
# estimates from https://ycharts.com/companies/SPY/dividend_yield
# 2022 1.34%
# 2021 1.5
# 2020 1.7

d_yield = {2020: 1.7/100, 2021: 1.5/100, 2022: 1.34/100}
y_list = list()
for ind in df.index:
    y_list.append(d_yield[ind.year])
df['q'] = y_list


We obtain risk-free rate $r$ from Kenneth French website (Fama/French 3 Factors).

We approximate the risk-free on a specific date by chooing the risk on the first day of a month corresponding to the option data.

# Interest rate
# Note that the csv file below was preprocessed by removing unnecessary rows and columns that broke the read_csv
mf = pd.read_csv("F-F_Research_Data_Factors.CSV").iloc[:,:2]
mf.columns = ['Date','r']
mf['r'] /= 100
mf['Date'] = pd.to_datetime(mf['Date'], format = '%Y%m')                
mf.head()





  
    
      
      Date
      r
    
  
  
    
      0
      1926-07-01
      0.0296
    
    
      1
      1926-08-01
      0.0264
    
    
      2
      1926-09-01
      0.0036
    
    
      3
      1926-10-01
      -0.0324
    
    
      4
      1926-11-01
      0.0253
    
  



mf = pd.read_csv("F-F_Research_Data_Factors.CSV")
mf = mf[["Unnamed: 0",'RF']]
mf.columns = ['Date','r']

r_years = mf['Date'].apply(lambda x: int(str(x)[:4]))
r_months =  mf['Date'].apply(lambda x: int(str(x)[4:]))

mf['year'] = r_years
mf['month'] = r_months
mf.drop(['Date'],axis = 1,inplace = True)
mf = mf[2020 &lt;= mf['year']]
r_list = list()
for ind in tqdm(df.index):
    r_list.append(mf[(mf['year'] == ind.year) &amp; (mf['month'] == ind.month)]['r'].to_numpy()[0])
df['r'] = r_list


100%|█████████████████████████████████| 100000/100000 [00:42&lt;00:00, 2368.09it/s]


r_list = list()
r_i = 0

for ind in tqdm(df.index):
    while (mf.iloc[r_i,1] !=ind.year) or (mf.iloc[r_i,2]  != ind.month):
        r_i+=1
    r_list.append(mf.iloc[r_i,0])
df['r'] = r_list


100%|████████████████████████████████| 100000/100000 [00:05&lt;00:00, 18277.71it/s]


Now, we compute the time until maturity $T-t$ in a year.
This is computed by dividing a number of days between the day that the option data was observed and it maturity by 365.

We will denote this feature as $T$.

# Time til maturity
dd_list = list()
for days in (df['maturity']-df.index):
    dd_list.append(days.days/365)
df['T'] = dd_list


A scatter matrix which conclues the dataset are shown below:

scatter_matrix(df[["underlying_last","call_last", "K","hist_vol","q","r","T"]],figsize = (15,15))
plt.show()




Benchmark: American option

In the next subsection, we will discuss the traditional method of pricing American option: Binomial Tree.
Given an initial stock, in a next step, the stock price will either go up or go down, under some pre-defined multiplicative factors: u and d.
\(u = e^{\sigma \Delta t}\)
\(d = \frac{1}{u}\)
u and d depends solely on the volatility, so we used historical volatility to estimate this $\sigma$.

For stock (or index) price with continuous dividend, the risk-neutral probability is computed by
\(\hat{p} = \frac{e^{(r-q) \Delta t} - d}{u -d}\)

The price for American option at node i can be computed by
\(f_i = \max (e^{-r \Delta T} (\hat{p} f_{iu} + (1- \hat{p}) f_{id}, (s_i - K)^+)\)

where the payoff at the leaf nodes i are $ (s_i - K)^+$.

The prices of options, as observed from the market at a given time, represent the values that investors expect those options to have under the circumstances. Theorically, if we have all the necessary parameters to price an option, we can construct option prices in the market. For binomial tree, most of the parameters such as $r$, $q$,  and $S_0$ can be observed or estimated except for the $\sigma$ which is hard to estimate. One way is to construct that quantity by inversely solving the model given the option price and all other paramemters. This implied volaitlity is the volatility of the underlying asset that the market expects. When pricing option, this quantity is unknown but can be estimated. Due to volatitility smile, in many traditional model, it is hard to use the quantity in the model. Since this implied volatility can be used to reconstruct exact price of an individual option. We will not use this quantity in our model.

To estimate the volatility, werely on historical volatility of the underlying asset price. By analyzing the historical price movements of the asset, we can make an estimation of $\sigma$.

# American Option pricing using binomial tree
# adapted from Kevin Mooney (see reference)

def american_call_price(S0, K, sigma, t, r = 0, q = 0, N = 3 ):

    #delta t
    t = t / (N - 1)
    u = np.exp(sigma * np.sqrt(t))
    d = 1/u

    p = (np.exp((r-q) * t) - d) / (u - d)
    stock_prices = np.zeros( (N, N) )
    call_prices = np.zeros( (N, N) )

    stock_prices[0,0] = S0
    M = 0
    for i in range(1, N ):
        M = i + 1
        stock_prices[i, 0] = d * stock_prices[i-1, 0]
        for j in range(1, M ):
            stock_prices[i, j] = u * stock_prices[i - 1, j - 1]
    expiration = stock_prices[-1,:] - K
    expiration = np.exp(-q*t *(N-1))*stock_prices[-1,:] - K
    expiration.shape = (expiration.size, )
    expiration = np.where(expiration &gt;= 0, expiration, 0)
    call_prices[-1,:] =  expiration

    # backward computing value
    for i in range(N - 2,-1,-1):
        for j in range(i + 1):
            # American Payoff
            call_prices[i,j] = np.max([np.exp(-r * t) * ((1-p) * call_prices[i+1,j] + p * call_prices[i+1,j+1]),
                                      np.max([stock_prices[i, j] - K,0])])         
    return call_prices[0,0]


We use 10-step tree for option pricing.

# American Option
N = 10

bm_list = list()
for i in tqdm(range(len(df))):
    current_row = df.iloc[i,:]

    S0 = current_row['underlying_last']
    K = current_row['K']
    sigma = current_row['hist_vol']
    r = current_row['r']
    q = current_row['q']
    T = current_row['T']
    bm_list.append(american_call_price(S0, K, sigma = sigma, t = T, r = r, q = q, N = N ))
df['bm'] = bm_list


  0%|                                    | 126/100000 [00:00&lt;01:19, 1255.55it/s]/var/folders/6r/96ncs6hd5plcz0t1d7spstzr0000gn/T/ipykernel_47931/145875048.py:11: RuntimeWarning: invalid value encountered in double_scalars
  p = (np.exp((r-q) * t) - d) / (u - d)
100%|█████████████████████████████████| 100000/100000 [01:12&lt;00:00, 1377.00it/s]


df = df[df['call_last'] != " "]
df['call_last'] = np.double(df['call_last'])
df.dropna(inplace = True)


The table for option data are shown below.
Note that we are not going to use all the columns in the table.

df





  
    
      
      underlying_last
      maturity
      implied_vol
      call_last
      K
      hist_vol
      q
      r
      T
      bm
    
    
      [QUOTE_DATE]
      
      
      
      
      
      
      
      
      
      
    
  
  
    
      2020-01-02
      324.87
      2020-09-30
      0.199590
      33.40
      300.0
      0.128190
      0.0170
      0.13
      0.745205
      47.199011
    
    
      2020-01-02
      324.87
      2020-03-31
      0.557560
      100.30
      215.0
      0.128190
      0.0170
      0.13
      0.243836
      114.648588
    
    
      2020-01-02
      324.87
      2020-01-27
      0.130940
      9.09
      316.0
      0.128190
      0.0170
      0.13
      0.068493
      11.905250
    
    
      2020-01-02
      324.87
      2020-06-30
      0.286710
      70.85
      255.0
      0.128190
      0.0170
      0.13
      0.493151
      81.585331
    
    
      2020-01-02
      324.87
      2020-01-31
      0.118620
      6.17
      321.5
      0.128190
      0.0170
      0.13
      0.079452
      8.142697
    
    
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
      ...
    
    
      2022-12-30
      382.44
      2023-01-27
      0.169750
      0.38
      415.0
      0.189685
      0.0134
      0.33
      0.076712
      1.201606
    
    
      2022-12-30
      382.44
      2024-01-19
      0.167740
      1.86
      515.0
      0.189685
      0.0134
      0.33
      1.054795
      27.581163
    
    
      2022-12-30
      382.44
      2023-01-03
      
      0.00
      332.0
      0.189685
      0.0134
      0.33
      0.010959
      51.526183
    
    
      2022-12-30
      382.44
      2024-01-19
      0.193650
      0.16
      670.0
      0.189685
      0.0134
      0.33
      1.054795
      0.458140
    
    
      2022-12-30
      382.44
      2023-01-20
      0.681010
      0.01
      685.0
      0.189685
      0.0134
      0.33
      0.057534
      0.000000
    
  

96924 rows × 10 columns


The measure for quantiative models that are widely used in machine learning is the root mean square error.
As we use the binomial model as a benchmark, we will compute the RMSE for the benchmark binomial model.

# MSE
from sklearn.metrics import mean_squared_error


np.sqrt(mean_squared_error(df['call_last'], df['bm']))


51.301668827531735


The RMSE for the benchmark model is ~51 which is high.

The statistics for the absolute error is shown below. The median error is around 3.3.

plt.hist(np.abs(df['bm']-df['call_last']),bins = 50, density = True)
plt.title("A histogram of error (in absolute difference) of Binomial Model")
plt.ylabel("density")
plt.xlabel("Error")


Text(0.5, 0, 'Error')




Machine Learning Model

In addition to the (closed) underlying price, days-to-maturity, historical volatility, dividend yield, interest rate and strike price, we will also use the adjusted closed stock prices 8 days lag as inputs.

Tn = 8
spy = pd.read_csv('./spy.csv')
spy['Date'] = pd.to_datetime(spy['Date'], format = '%Y-%m-%d')
spy.set_index("Date",inplace= True)
spy = spy[['Adj Close']]
spy.rename(columns={"Adj Close": "t0"}, inplace = True)
spy_use = spy[spy.index &gt;= df.index[0]]

import warnings
warnings.filterwarnings("ignore")
for i in range(1,Tn+1):
    spy_use['t-'+str(i)] = spy['t0'].shift(i).iloc[-len(spy_use['t0']):]
df = df.join(spy_use)


# choose only numerical
df.drop(columns = ['maturity','implied_vol','bm','underlying_last'],inplace = True)


The first 5 rows of  final prepared dataset is shown below:

df.head()





  
    
      
      call_last
      K
      hist_vol
      q
      r
      T
      t0
      t-1
      t-2
      t-3
      t-4
      t-5
      t-6
      t-7
      t-8
    
  
  
    
      2020-01-02
      33.40
      300.0
      0.12819
      0.017
      0.13
      0.745205
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      100.30
      215.0
      0.12819
      0.017
      0.13
      0.243836
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      9.09
      316.0
      0.12819
      0.017
      0.13
      0.068493
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      70.85
      255.0
      0.12819
      0.017
      0.13
      0.493151
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
    
      2020-01-02
      6.17
      321.5
      0.12819
      0.017
      0.13
      0.079452
      308.517456
      305.658936
      304.918213
      306.608582
      306.684631
      305.060669
      305.051178
      304.585846
      303.256348
    
  



The original data is timestamped with the time that each option is observed. The chronological order of the data may have an effect on the model, so it is important to take the order into account. For this option pricing project, we assume that the chronological order does not have a significant effect on the models. We make an assumption that the most chornological effects are contained in the features such as historical volatility and the stock price lags.

We shuffle and split 80% for traning set, 16% for test set, 4% for validation sets.

from sklearn.model_selection import train_test_split
import datetime
# We split 80% for traning set, 16% for test set, 4% for validation sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['call_last']),df['call_last'], test_size=0.2)
X_test, X_valid, y_test, y_valid = train_test_split(X_test,y_test, test_size=0.2)


# If you believe the order affect, use the code below
#X_train = df[df.index &lt;= datetime.datetime(2022,9,1)].copy()
#y_train = X_train[['call_last']]
#X_train = X_train.drop(columns = ['call_last'])

#X_test = df[df.index &gt; datetime.datetime(2022,9,1)].copy()
#y_test = X_test[['call_last']]
#X_test = X_test.drop(columns = ['call_last'])

#X_valid = X_test.iloc[:1000,:]
#y_valid = y_test.iloc[:1000]

#X_test = X_test.iloc[1000:,:]
#y_test = y_test.iloc[1000:]


As we obtained the dataset, we compute the option price based on American binomial tree model on the test set in order to compare it to other models. Since a number of the test set is small, we can apply the tree for large number of steps. In this case, we use 30 steps.

Benchmark (Binomial Tree)

bm_list = list()
for i in tqdm(range(len(X_test))):
    current_row = X_test.iloc[i,:]

    S0 = current_row['t0']
    K = current_row['K']
    sigma = current_row['hist_vol']
    r = current_row['r']
    q = current_row['q']
    T = current_row['T']
    bm_list.append(american_call_price(S0, K, sigma = sigma, t = T, r = r, q = q, N = 30 ))


100%|████████████████████████████████████| 15508/15508 [01:25&lt;00:00, 180.46it/s]


np.sqrt(mean_squared_error(y_test,bm_list))


48.219430763590715


The root mean squared error for the binomial model is 48.219430763590715.

Linear Regression

We use multiple linear regression with stdard scaled input.

from sklearn.linear_model import LinearRegression,SGDRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler


lin_reg = Pipeline([("std_scaler", StandardScaler()),
                     ("LinReg", LinearRegression())])
lin_reg.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;LinReg&#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;LinReg&#x27;, LinearRegression())])StandardScalerStandardScaler()LinearRegressionLinearRegression()

The root mean squared errors for training and testing set are shown below:

np.sqrt(mean_squared_error(y_train,lin_reg.predict(X_train)))


38.43095625260875


np.sqrt(mean_squared_error(y_test,lin_reg.predict(X_test)))


37.97049534873681


The coefficients and intercept for the model are shown below:

lin_reg["LinReg"].coef_


array([-37.65425523,   2.524246  ,   1.51641187,  -0.54682013,
        13.45049636,   5.76041445,   3.59107429,   3.32804942,
         3.28542487,  -0.09017812,  -0.74559882,  -1.94913388,
         0.6794828 ,   9.51963281])


lin_reg["LinReg"].intercept_


31.72985981248147


Polynomial Regression

For the polynomial regression, we only consider the polynomial of degree 2 with standard scaled input. We have 14 features, and the polynomial features are going to be large.

from sklearn.preprocessing import PolynomialFeatures
from scipy.stats import randint
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV


PolyReg = Pipeline([("std_scaler", StandardScaler()),
          ("poly_feature", PolynomialFeatures(degree=2, include_bias=False)),
         ("LinReg",  LinearRegression())])


PolyReg.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;poly_feature&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;LinReg&#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;poly_feature&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;LinReg&#x27;, LinearRegression())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures(include_bias=False)LinearRegressionLinearRegression()

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,PolyReg.predict(X_train)))


36.17182222027887


# Testing Loss
np.sqrt(mean_squared_error(y_test,PolyReg.predict(X_test)))


36.06896946882215


Ridge and Lasso Regression

For Ridge and Lasso Regression, we also find the best hyperparameters $\alpha$ using Grid Search.
We perform cross validation only on the first 10000 training data with 3-fold cross validation.

from sklearn.linear_model import LinearRegression,Ridge,Lasso


alpha = (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)
ridge_reg = Pipeline([("std_scaler", StandardScaler()),
         ("ridge",  Ridge())])
parameters = {'ridge__alpha': alpha}
rr = GridSearchCV(estimator=ridge_reg,param_grid = parameters, cv = 3)
rr.fit(X_train[:10000], y_train[:10000])




GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;ridge&#x27;, Ridge())]),
             param_grid={&#x27;ridge__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;ridge&#x27;, Ridge())]),
             param_grid={&#x27;ridge__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge())])StandardScalerStandardScaler()RidgeRidge()

rr.best_estimator_



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])StandardScalerStandardScaler()RidgeRidge(alpha=10)

We found that $\alpha = 10$ is the best alpha.
We then fit this best estimator to the whole dataset.

best_ridge = rr.best_estimator_
best_ridge.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])StandardScalerStandardScaler()RidgeRidge(alpha=10)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_ridge.predict(X_train)))


38.43095969174494


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_ridge.predict(X_test)))


37.97019255417378


We did the same to Lasso.

alpha = (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)
lasso_reg = Pipeline([("std_scaler", StandardScaler()),
         ("lasso",  Lasso())])
parameters = {'lasso__alpha': alpha}
lr = GridSearchCV(estimator=lasso_reg, param_grid  = parameters, cv = 3)
lr.fit(X_train[:10000], y_train[:10000])



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso())])StandardScalerStandardScaler()LassoLasso()

lr.best_estimator_
best_lasso = lr.best_estimator_
best_lasso



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])StandardScalerStandardScaler()LassoLasso(alpha=0.01)

best_lasso.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])StandardScalerStandardScaler()LassoLasso(alpha=0.01)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_lasso.predict(X_train)))


38.431561232353104


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_lasso.predict(X_test)))


37.969146928057036


The best $\alpha$ is 0.01.

We can use Lasso for feature selection as it tries to minimize unimportant features coefficients to 0.

lr.best_estimator_['lasso'].coef_


array([-37.64057787,   2.49377087,   1.45139028,  -0.55584586,
        13.43936913,   5.72526309,   3.68045696,   3.23670677,
         2.1807758 ,   0.        ,  -0.        ,  -0.        ,
         0.        ,   8.50465773])


X_train.columns


Index(['K', 'hist_vol', 'q', 'r', 'T', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',
       't-6', 't-7', 't-8'],
      dtype='object')


Interestingly many of the lags of stock prices are not important.

Support Vector Regressor

For the linear, we tune the hyperparameter C and $\epsilon$ for linear SVR.
The other setting are the same as in previous.

from sklearn.svm import LinearSVR

lsvr = Pipeline([("std_scaler", StandardScaler()),
                 ('svr',LinearSVR())])
parameters = {'svr__epsilon':(0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000),
             'svr__C': (0.0001, 0.001, 0.01,0.1,1,10,100,500,1000,5000,10000)}
lsvr_gs = GridSearchCV(estimator=lsvr,param_grid = parameters, cv = 3)
lsvr_gs.fit(X_train[:10000], y_train[:10000])



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, LinearSVR())]),
             param_grid={&#x27;svr__C&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,
                                    1000, 5000, 10000),
                         &#x27;svr__epsilon&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, LinearSVR())]),
             param_grid={&#x27;svr__C&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,
                                    1000, 5000, 10000),
                         &#x27;svr__epsilon&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, LinearSVR())])StandardScalerStandardScaler()LinearSVRLinearSVR()

lsvr_gs.best_estimator_



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])StandardScalerStandardScaler()LinearSVRLinearSVR(C=10, epsilon=10)

best_lsvr = lsvr_gs.best_estimator_


best_lsvr.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])StandardScalerStandardScaler()LinearSVRLinearSVR(C=10, epsilon=10)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_lsvr.predict(X_train)))


38.547851070665025


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_lsvr.predict(X_test)))


38.03050061254464


The tuned c and epsilon are 10 and 10 respectively.

Now, we consider nonlinear SVR,in addtion to $\epsilon$ and $C$, we include a type of kernel as a hyperparamter. We choose between rbf and sigmoid. Note that we reduce search space for $\epsilon$ and $C$ to accelerate the computing time.

from sklearn.svm import SVR


nlsvr = Pipeline([("std_scaler", StandardScaler()),
                 ('svr',SVR())])
parameters = {'svr__kernel':('rbf', 'sigmoid'),
              'svr__epsilon':(0.001, 0.01,0.1,1,100,5000),
             'svr__C': (0.001, 0.01,0.1,1,100,5000)
              }
nlsvr_gs = GridSearchCV(estimator=nlsvr,param_grid = parameters, cv = 3,n_jobs = -1)
nlsvr_gs.fit(X_train[:10000].to_numpy(), y_train[:10000].to_numpy().ravel())



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, SVR())]),
             n_jobs=-1,
             param_grid={&#x27;svr__C&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__epsilon&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__kernel&#x27;: (&#x27;rbf&#x27;, &#x27;sigmoid&#x27;)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, SVR())]),
             n_jobs=-1,
             param_grid={&#x27;svr__C&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__epsilon&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__kernel&#x27;: (&#x27;rbf&#x27;, &#x27;sigmoid&#x27;)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, SVR())])StandardScalerStandardScaler()SVRSVR()

best_nlsvr = nlsvr_gs.best_estimator_
best_nlsvr



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])StandardScalerStandardScaler()SVRSVR(C=100, epsilon=1)

best_nlsvr.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])StandardScalerStandardScaler()SVRSVR(C=100, epsilon=1)

The root mean squared errors for training and testing set are shown below:

# Training Loss
np.sqrt(mean_squared_error(y_train,best_nlsvr.predict(X_train)))


37.808085641917536


# Testing Loss
np.sqrt(mean_squared_error(y_test,best_nlsvr.predict(X_test)))


37.92867379033997


Random Forest

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV


There are so many parameters, so we use random search instead of grid search. We randomly sample hyperparameters uniformly from the parameters lists for 100 samples. And we use first 10000 shuffled data entries for this tuning.

rf_rg = Pipeline([('std_scaler', StandardScaler()),
                     ('rf', RandomForestRegressor())])
parameters = {'rf__n_estimators': (50,100,300,400,500),
             'rf__max_depth':(None, 8,32,64,128),
             'rf__ccp_alpha':(0,0.00000001,0.00001,0.001),
             'rf__bootstrap': [True, False]}
rf_gs =RandomizedSearchCV(estimator=rf_rg,param_distributions = parameters, cv = 3,n_jobs = -1,n_iter = 100)


rf_gs.fit(X_train[:10000].to_numpy(), y_train[:10000].to_numpy().ravel())



RandomizedSearchCV(cv=3,
                   estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                             (&#x27;rf&#x27;, RandomForestRegressor())]),
                   n_iter=160, n_jobs=-1,
                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],
                                        &#x27;rf__ccp_alpha&#x27;: (0, 1e-08, 1e-05,
                                                          0.001),
                                        &#x27;rf__max_depth&#x27;: (None, 8, 32, 64, 128),
                                        &#x27;rf__n_estimators&#x27;: (50, 100, 300, 400,
                                                             500)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,
                   estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                             (&#x27;rf&#x27;, RandomForestRegressor())]),
                   n_iter=160, n_jobs=-1,
                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],
                                        &#x27;rf__ccp_alpha&#x27;: (0, 1e-08, 1e-05,
                                                          0.001),
                                        &#x27;rf__max_depth&#x27;: (None, 8, 32, 64, 128),
                                        &#x27;rf__n_estimators&#x27;: (50, 100, 300, 400,
                                                             500)})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;, RandomForestRegressor())])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor()

best_rf = rf_gs.best_estimator_


best_rf



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)

best_rf.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)

The root mean squared errors for training and testing set are shown below:

The root mean squared errors for training and testing set are shown below:# Training Loss
np.sqrt(mean_squared_error(y_train,best_rf.predict(X_train)))


31.349066580782836


#Testing RMSE
np.sqrt(mean_squared_error(y_test,best_rf.predict(X_test)))


32.90546500222834


With Random Forest, we can see feature importance:

best_rf['rf'].feature_importances_


array([0.51617093, 0.04662939, 0.00114048, 0.00432427, 0.13790168,
       0.04360533, 0.0377016 , 0.02562464, 0.03328482, 0.01493993,
       0.05282551, 0.00782497, 0.01813578, 0.05989066])


X_train.columns
# Strike Price and Time to maturity seem to be the most important


Index(['K', 'hist_vol', 'q', 'r', 'T', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',
       't-6', 't-7', 't-8'],
      dtype='object')


The best hyperparamters are ccp_alpha=0, max_depth=8, n_estimators=400, and bootstrap = False.

K-Nearest Neighbors

In this model, we only tune a number of neighbors as a hyperparameter.

from sklearn.neighbors import KNeighborsRegressor


knn_rg = Pipeline([('std_scaler', StandardScaler()),
                  ('knn',KNeighborsRegressor())])
parameters = {'knn__n_neighbors': np.arange(5,100,5)}


knn_gs = GridSearchCV(estimator=knn_rg,param_grid = parameters, cv = 3,n_jobs = -1)
knn_gs.fit(X_train[:10000], y_train[:10000])



GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;knn&#x27;, KNeighborsRegressor())]),
             n_jobs=-1,
             param_grid={&#x27;knn__n_neighbors&#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,
       90, 95])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;knn&#x27;, KNeighborsRegressor())]),
             n_jobs=-1,
             param_grid={&#x27;knn__n_neighbors&#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,
       90, 95])})estimator: PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()

best_knn = knn_gs.best_estimator_
best_knn



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor(n_neighbors=15)

best_knn.fit(X_train, y_train)



Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor(n_neighbors=15)

The root mean squared errors for training and testing set are shown below:

np.sqrt(mean_squared_error(y_train,best_knn.predict(X_train)))


31.92967572127278


np.sqrt(mean_squared_error(y_test,best_knn.predict(X_test)))


32.912277382176526


The best number of the neigbors is 15.

Multi-layer perceptron

For neutral network, we make experiments on three models, which are multilayer perceptron (with dropout and normalization, with relu as an activation function for the hidden layers and identity for the output layer), multilayer perceptron with tuned hyperparameters, and Convolutional Neural network.

from tensorflow.keras import Sequential, layers
import tensorflow as tf


2023-05-31 17:15:39.270253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.


model = tf.keras.Sequential([
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(30, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(30, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(20, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation= None)
])


callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50,restore_best_weights=True)
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train, y_train, epochs=1000,
                    validation_data=(X_valid, y_valid),callbacks=[callback])


Epoch 1/1000
2424/2424 [==============================] - 8s 2ms/step - loss: 2348.1985 - val_loss: 1538.4142
Epoch 2/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1858.8630 - val_loss: 1519.5778
Epoch 3/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1826.4266 - val_loss: 1481.3267
Epoch 4/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1821.6594 - val_loss: 1479.6355
Epoch 5/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1815.7463 - val_loss: 1452.8606
Epoch 6/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1803.6791 - val_loss: 1457.6923
Epoch 7/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1796.4032 - val_loss: 1453.8772
Epoch 8/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1796.3320 - val_loss: 1446.6718
Epoch 9/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1788.8895 - val_loss: 1437.1597
Epoch 10/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1762.4237 - val_loss: 1441.7910
Epoch 11/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1789.3596 - val_loss: 1461.8259
Epoch 12/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1782.5536 - val_loss: 1455.1730
Epoch 13/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1772.4648 - val_loss: 1450.3599
Epoch 14/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1753.4039 - val_loss: 1453.5958
Epoch 15/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1778.0695 - val_loss: 1447.4929
...
Epoch 106/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1699.8079 - val_loss: 1426.6115
Epoch 107/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1737.1375 - val_loss: 1446.8939
Epoch 108/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1710.6053 - val_loss: 1421.2778


model.summary()


Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 14)               56        
 ormalization)                                                   

 dropout (Dropout)           (None, 14)                0         

 dense (Dense)               (None, 30)                450       

 batch_normalization_1 (Batc  (None, 30)               120       
 hNormalization)                                                 

 dropout_1 (Dropout)         (None, 30)                0         

 dense_1 (Dense)             (None, 30)                930       

 batch_normalization_2 (Batc  (None, 30)               120       
 hNormalization)                                                 

 dropout_2 (Dropout)         (None, 30)                0         

 dense_2 (Dense)             (None, 20)                620       

 batch_normalization_3 (Batc  (None, 20)               80        
 hNormalization)                                                 

 dropout_3 (Dropout)         (None, 20)                0         

 dense_3 (Dense)             (None, 10)                210       

 batch_normalization_4 (Batc  (None, 10)               40        
 hNormalization)                                                 

 dropout_4 (Dropout)         (None, 10)                0         

 dense_4 (Dense)             (None, 1)                 11        

=================================================================
Total params: 2,637
Trainable params: 2,429
Non-trainable params: 208
_________________________________________________________________


The learning curve is shown below:

plt.plot(history.history['loss'],label = "loss")
plt.plot(history.history['val_loss'], label = "val_loss")
plt.legend()
plt.ylabel("MSE")
plt.xlabel("Epoch")


Text(0.5, 0, 'Epoch')




The root mean squared errors for training and testing set are shown below:

#trainig RMSE
np.sqrt(mean_squared_error(y_train,model.predict(X_train)))


2424/2424 [==============================] - 2s 714us/step





36.21230441027238


#testing RMSE
np.sqrt(mean_squared_error(y_test,model.predict(X_test)))


485/485 [==============================] - 0s 675us/step





35.71456726735933


The model seems to be underfitting.

Hyperparamter-tuned MLP

We use keras tuner library to tune the hyperparameters of the network. The hyperparameter space is shown below.

import keras_tuner as kt

def build_model(hp):
    n_hidden = hp.Int("n_hidden", min_value=1, max_value=8)
    n_neurons = hp.Int("n_neurons", min_value=1, max_value=100)
    learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2,
                             sampling="log")
    l2_rate = hp.Float("l2", min_value=1e-4, max_value=100,
                             sampling="log")
    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)

    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Normalization(input_shape=X_train.shape[1:]))
    for _ in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation="relu",kernel_initializer="he_normal",kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))
    model.add(tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))
    model.compile(loss="mse", optimizer=optimizer)
    return model


random_search_tuner = kt.RandomSearch(
    build_model, objective="val_loss", max_trials=20, seed=42)
random_search_tuner.search(X_train[:5000], y_train[:5000], epochs=150,
                           validation_data=(X_valid, y_valid))



Trial 20 Complete [00h 00m 53s]
val_loss: 1461.2305908203125

Best val_loss So Far: 1451.614013671875
Total elapsed time: 00h 15m 31s
INFO:tensorflow:Oracle triggered exit


random_search_tuner.get_best_hyperparameters()[0].values


{'n_hidden': 7,
 'n_neurons': 15,
 'learning_rate': 0.0006237028864858578,
 'l2': 0.0003065801184974072}


The best hyperparameters we found are 7 hidden layers with 15 neurons each, 0.0006237028864858578 learning rate, and l2 = 0.0003065801184974072 for the l2 regularization.

best_nn = random_search_tuner.get_best_models()[0]


callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100,restore_best_weights=True)
history_best_nn = best_nn.fit(X_train, y_train, epochs=1000,
                    validation_data=(X_valid, y_valid),callbacks=[callback])

Epoch 1/1000
2424/2424 [==============================] - 6s 1ms/step - loss: 1364.6182 - val_loss: 1498.1863
Epoch 2/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1363.1213 - val_loss: 1462.6725
Epoch 3/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1354.9094 - val_loss: 1493.1799
Epoch 4/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1349.7559 - val_loss: 1452.7467
Epoch 5/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1347.2516 - val_loss: 1499.4685
Epoch 6/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1341.6652 - val_loss: 1437.6245
Epoch 7/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.0056 - val_loss: 1423.4025
Epoch 8/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.5402 - val_loss: 1513.4679
Epoch 9/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.7520 - val_loss: 1569.9033
...
Epoch 556/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1242.6877 - val_loss: 1332.1714
Epoch 557/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1260.6245 - val_loss: 1501.7150
Epoch 558/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1245.4034 - val_loss: 1347.1127


The root mean squared errors for training and testing set are shown below:

# Traing RMSE
np.sqrt(mean_squared_error(y_train,best_nn.predict(X_train)))


2424/2424 [==============================] - 2s 597us/step

34.78459569413211


# Testing RMSE
np.sqrt(mean_squared_error(y_test,best_nn.predict(X_test)))


485/485 [==============================] - 0s 612us/step

34.821059741172114


The learning curve is shown below.

plt.plot(history_best_nn.history['loss'],label = "loss")
plt.plot(history_best_nn.history['val_loss'], label = "val_loss")
plt.legend()
plt.ylabel("MSE")
plt.xlabel("Epoch")


Text(0.5, 0, 'Epoch')




Convolutional Neural Network (CNN)

The structure of the convolutional network is shown below.

model_conv = tf.keras.Sequential([
    tf.keras.layers.Conv1D(32,2,activation = 'relu',input_shape = (14,1,), kernel_initializer="he_normal",padding = 'same'),
    tf.keras.layers.Conv1D(32,3,activation = 'relu', kernel_initializer="he_normal",padding = 'same'),
    tf.keras.layers.Conv1D(32,3,activation = 'relu', kernel_initializer="he_normal"),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(64, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(32, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(1, activation= None)
])




callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50,restore_best_weights=True)
model_conv.compile(loss="mse", optimizer="nadam")
history_cov = model_conv.fit(X_train, y_train, epochs=1000,
                    validation_data=(X_valid, y_valid),callbacks=[callback])


Epoch 1/1000
2424/2424 [==============================] - 7s 2ms/step - loss: 1632.2096 - val_loss: 1460.4231
Epoch 2/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1373.3662 - val_loss: 1505.1824
Epoch 3/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1366.6722 - val_loss: 1474.0969
Epoch 4/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1346.9469 - val_loss: 1475.6388
Epoch 5/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1334.1947 - val_loss: 1547.0897
Epoch 6/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1323.7288 - val_loss: 1439.5723
Epoch 7/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1310.3964 - val_loss: 1432.5355
Epoch 8/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1294.4939 - val_loss: 1449.5792
Epoch 9/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1290.2340 - val_loss: 1363.4564
...
Epoch 244/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1013.4796 - val_loss: 1174.7893
Epoch 245/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1002.9050 - val_loss: 1177.7996


model_conv.summary()


Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 14, 32)            96        

 conv1d_1 (Conv1D)           (None, 14, 32)            3104      

 conv1d_2 (Conv1D)           (None, 12, 32)            3104      

 flatten (Flatten)           (None, 384)               0         

 dense_8 (Dense)             (None, 128)               49280     

 dense_9 (Dense)             (None, 64)                8256      

 dense_10 (Dense)            (None, 32)                2080      

 dense_11 (Dense)            (None, 1)                 33        

=================================================================
Total params: 65,953
Trainable params: 65,953
Non-trainable params: 0
_________________________________________________________________


The learning curve is shown below.

plt.plot(history_cov.history['loss'],label = "loss")
plt.plot(history_cov.history['val_loss'], label = "val_loss")
plt.legend()
plt.ylabel("MSE")
plt.xlabel("Epoch")


Text(0.5, 0, 'Epoch')




The root mean squared errors for training and testing set are shown below:

# Training RMSE
np.sqrt(mean_squared_error(y_train,model_conv.predict(X_train)))


2424/2424 [==============================] - 2s 920us/step





31.66131974960281


# Testing RMSE
np.sqrt(mean_squared_error(y_test,model_conv.predict(X_test)))


485/485 [==============================] - 0s 814us/step





32.372653521773714


Result

The Root Mean Square Error for each machine learning models are shown below, together with the chosen hyperparamters.

res = pd.DataFrame({"Model": ["Binomial Tree",'Linear Regression',"Polynomial Regression","Ridge Regression","Lasso Regression","Linear SVR","Nonlinear SVR","Random Forest","KNN","Neural Network", "Neural Network (tuned)", " Convolutional Neutral Network"],
                   "RMSE(training)":[None,
38.43095625260875,
36.17182222027887,
38.43095969174494,
38.431561232353104,
38.547851070665025,
37.808085641917536,
31.349066580782836,
31.92967572127278,
36.21230441027238,
34.78459569413211,
31.66131974960281
],
                "RMSE(testing)":
                   [48.219430763590715,
                    37.97049534873681,
36.06896946882215,
37.97019255417378,
37.969146928057036,
38.03050061254464,
37.92867379033997,
32.90546500222834,
32.912277382176526,
35.71456726735933,
34.821059741172114,
32.372653521773714
],
"Hyperparameters" :
        ["29 steps","",
"degree = 2",
"alpha = 10",
"alpha = 0.01",
"C=10, epsilon=10",
"C=100, epsilon=1",
"ccp_alpha=0, max_depth=8, n_estimators=400, bootstrap = False",
         "neighbors = 15",
        "consider model summary above",
        "consider model summary above",
        "consider model summary above"]    
                   })
res





  
    
      
      Model
      RMSE(training)
      RMSE(testing)
      Hyperparameters
    
  
  
    
      0
      Binomial Tree
      NaN
      48.219431
      29 steps
    
    
      1
      Linear Regression
      38.430956
      37.970495
      
    
    
      2
      Polynomial Regression
      36.171822
      36.068969
      degree = 2
    
    
      3
      Ridge Regression
      38.430960
      37.970193
      alpha = 10
    
    
      4
      Lasso Regression
      38.431561
      37.969147
      alpha = 0.01
    
    
      5
      Linear SVR
      38.547851
      38.030501
      C=10, epsilon=10
    
    
      6
      Nonlinear SVR
      37.808086
      37.928674
      C=100, epsilon=1
    
    
      7
      Random Forest
      31.349067
      32.905465
      ccp_alpha=0, max_depth=8, n_estimators=400, bo...
    
    
      8
      KNN
      31.929676
      32.912277
      neighbors = 25
    
    
      9
      Neural Network
      36.212304
      35.714567
      consider model summary above
    
    
      10
      Neural Network (tuned)
      34.784596
      34.821060
      consider model summary above
    
    
      11
      Convolutional Neutral Network
      31.661320
      32.372654
      consider model summary above
    
  



Consider the RMSE for the testing set, the lower RMSE, the more accurate the model to compute option price. By this sole metric, all the machine learning model we used in this project performs better than the binomial tree.
By ranking the models by this value, we have that the Convolutional Neural Network performs the best, the random forest is the second best, and the K-Nearest Neighbors is the third.

The statistics for the absolute error for the best three models and the binomial tree are shown below.

res_error = pd.concat([pd.DataFrame({"binomial abs_error": np.abs(np.array(bm_list) -y_test.to_numpy().flatten())}).describe(),
                        np.abs(model_conv.predict(X_test).ravel() - y_test).describe(),
          np.abs((best_rf.predict(X_test).ravel()-y_test)).describe(),
                       np.abs(best_knn.predict(X_test).ravel() - y_test ).describe(),],axis = 1)


res_error.columns = ["Binomial Tree Absolute Error","CNN Absolute Error","Random Forest Absolute Error","KNN Absolute Error"]
res_error


485/485 [==============================] - 0s 908us/step





  
    
      
      Binomial Tree Absolute Error
      CNN Absolute Error
      Random Forest Absolute Error
      KNN Absolute Error
    
  
  
    
      count
      15508.000000
      15508.000000
      15508.000000
      15508.000000
    
    
      mean
      22.192882
      16.865260
      17.565473
      17.039533
    
    
      std
      42.810135
      27.633330
      27.825774
      28.158896
    
    
      min
      0.000000
      0.000103
      0.000018
      0.000000
    
    
      25%
      0.749303
      0.927289
      1.313031
      0.710667
    
    
      50%
      7.469478
      4.053897
      5.481253
      4.335000
    
    
      75%
      17.896939
      22.537766
      22.226330
      21.908333
    
    
      max
      369.079773
      350.456573
      338.299600
      341.098000
    
  



By considering the median for the absolute errors for each model, we can see that those of the machine learing models are smaller than that from the benchmark model. This implies that for at least half of the dataset, the absolute error from the machine learing models are smaller than that from the benchmark.
However, if we consider other quantitle, we can see that there are not much different. But for most part, the machine learning models have smaller errors.

Stock Price versus Predicted Call Price

By the RMSE on the testing set, we choose the CNN to be the best model.
We visualize $C_0$ as a function of strike price $K$ and the stock price $S_0$.

res_pt = pd.concat([X_test[['t0']].set_index(np.arange(0,len(X_test))).rename(columns = {"t0":"stock price"}),
                    X_test[['K']].set_index(np.arange(0,len(X_test))).rename(columns = {"K":"K"}),
            pd.DataFrame({"Call Price from the dataset": y_test.to_numpy().ravel()}),
           pd.DataFrame({"Call Price from Binomial Tree": bm_list}),
           pd.DataFrame({"Call Price from CNN": model_conv.predict(X_test).ravel()}),
          pd.DataFrame({"lower bound":
              [np.max([lb,0]) for lb in X_test['t0'] - X_test['K']*np.exp(-X_test['r'] *X_test['T'])]})], axis = 1)


485/485 [==============================] - 1s 1ms/step


# Percentage of call price greater than 120
np.sum(y_test.to_numpy() &gt;= 120)/len(y_test)


0.06970595821511479


fig,axes = plt.subplots(1,3, figsize = (21,8),subplot_kw={'projection': '3d'})
axes[0].scatter(res_pt['stock price'], res_pt["K"],res_pt["Call Price from the dataset"], s= 1)
axes[0].set_xlabel("$S_0$")
axes[0].set_ylabel("$K$")
axes[0].set_zlabel("$C_0$")
axes[0].view_init(elev=20., azim=145, roll=0)
axes[0].set_title("True")
axes[0].set_zlim([0,320])

axes[1].scatter(res_pt['stock price'], res_pt["K"],res_pt["Call Price from Binomial Tree"], s= 1)
axes[1].set_xlabel("$S_0$")
axes[1].set_ylabel("$K$")
axes[1].set_zlabel("$C_0 $")
axes[1].view_init(elev=20., azim=145, roll=0)
axes[1].set_title("Binomial Tree")
axes[1].set_zlim([0,320])


axes[2].scatter(res_pt['stock price'], res_pt["K"],res_pt["Call Price from CNN"], s= 1)
axes[2].set_xlabel("$S_0$")
axes[2].set_ylabel("$K$")
axes[2].set_zlabel("$C_0 $")
axes[2].set_title("CNN")
axes[2].view_init(elev=20., azim=145, roll=0)
axes[2].set_zlim([0,320])


plt.show()




By considering only two factors, $S_0$ and $K$, we can see some paterrns in the corresponding option price. All the three plots look similar, indicating the accuracy of the binomial tree and the Convolutional Neural Network to price options. However, in the convolutional neural network, we can see that the call prices are dense around the small values similar to the actual price, however it did not well capture high call prices. For the benchmark, the prices are more sparse compared to both actual price and the CNN.

Now, we visualize an option price as a function of stock price, fixing other variable.

We fix other variable by choosing those variables from the dataset.

Then, we compare an option price as a function of $S_0$ from Binomial Tree and the best model.

Remark that, the machine learning models require additional parameters which are stock prices 8 days in the past. We cannot fix these stock prices; otherwise, the underlying price is unrealistic (eg. stock price at 0 is 500, while those at -1, -2, are around 200).

For each $S_0$, we find relevant $S_{-1}, S_{-2}$, …, in the dataset.

We also compute a upperbound and lowerbound as shown below.

The upper and lower bound for American option (for non-dividend) can be computed by
\(\max(S_t - K e^{-r (T-t)},0)\leq C_t \leq S_t\)

# Choose only S_0_x (rounded) that we use to train model
S_0_x = np.unique(np.round(spy_use['t0']) )
binomial_vs_s0 = [american_call_price(xs0, sample_fixed['K'][0], sigma = sample_fixed['hist_vol'][0], t = sample_fixed['T'][0], r = sample_fixed['r'][0], q = sample_fixed['q'][0], N = 30 ) for xs0 in S_0_x ]
spy_extended = spy_use.copy()
spy_extended['round_t0'] = np.round(spy_use['t0'])
s0_dataset = []
for xs0 in S_0_x:
    s0_dataset.append(np.concatenate([sample_fixed.drop(columns = ["call_last",'t0']).to_numpy()[0,:5], spy_extended[spy_extended['round_t0'] == xs0].sample(1).iloc[0,:9].ravel()]))
s0_dataset = np.array(s0_dataset)
cnn_vs_s0 = model_conv.predict(s0_dataset).ravel()


7/7 [==============================] - 0s 1ms/step


max_bound = S_0_x
min_bound = [np.max([xs0 - sample_fixed['K'][0] * np.exp(-sample_fixed['r'][0]*sample_fixed['T'][0]),0]) for xs0 in S_0_x ]


plt.plot(S_0_x ,binomial_vs_s0,label = 'binomial')
plt.plot(S_0_x ,cnn_vs_s0,label = 'CNN')
plt.plot(S_0_x ,max_bound,label = 'max bound')
plt.plot(S_0_x ,min_bound,label = 'min bound',ls = 'dashed')
plt.ylim([0,100])
plt.legend()


&lt;matplotlib.legend.Legend at 0x7f844f12e160&gt;




The plot aligns with the previous plot, suggesting consistency. However, it appears that the CNN model does not accurately capture the high call prices. The generated call prices are observed to be outside the lower bound for high underlying price, creating the potential for arbitrage opportunities. Therefore, it is crucial to exercise caution and conduct a thorough examination of the model before implementing it. Additionally, fine-tuning or training the CNN model with a larger and more comprehensive dataset could potentially improve its performance and yield a better model.

Conclusion

In this project, we have utilized machine learning models to price American options based on the SPY dataset. Our findings indicate that all of the machine learning models outperform the traditional binomial model for both the testing and training sets. Among these models, CNN, random forest, and KNN show promise, as their testing and training losses are relatively low.

However, it is worth noting that even though CNN performs the best overall, it struggles with accurately computing high call prices. Therefore, further tuning and dataset preparation might be necessary to enhance its performance in this regard.

In conclusion, machine learning models demonstrate the ability to capture the relationship between financial information and option pricing. Through the utilization of the RMSE metric, these models outperform the traditional benchmark model. Nonetheless, additional tunin

Acknoledgement

Thanks all people who gathered the data and publicly publish them online. Thank you Dr. Kevin Lu for giving suggestions on American option pricing models.

References

Culkin, Robert, and Sanjiv R. Das. Machine Learning in Finance: The Case of Deep Learning for Option Pricing, 2 Aug. 2017, srdas.github.io/Papers/BlackScholesNN.pdf.

French, Kenneth R. Kenneth R. French - Data Library, mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html. Accessed 3 June 2023.

Graupe, Kyle. “$spy Option Chains - Q1 2020 - Q4 2022.” Kaggle, 14 Mar. 2023,
www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022.

Lu, Kevin. “Machine Learning for Finance Lectures.” CFRM 421:Machine Learning for Finance . Seattle, The University of Washington.

Lu, Kevin. “Introduction to Financial Markets Lectures.” CFRM 415: Introduction to Financial Markets. Seattle, The University of Washington.

Mooney, Kevin, director. Implementing the Binomial Option Pricing Model in Python, YouTube, 15 Feb. 2021, https://www.youtube.com/watch?v=d7wa16RNRCI. Accessed 5 June 2023.

SPY Dividend Yield, ycharts.com/companies/SPY/dividend_yield. Accessed 3 June 2023.
" />

<meta name="author" content="W Wunkaew" />


<meta property="og:title" content="Pricing American Options Using Machine Learning" />
<meta property="twitter:title" content="Pricing American Options Using Machine Learning" />



    <title>Pricing American Options Using Machine Learning – W Wunkaew – Omzin's portfolio and blog</title>

    <link rel="stylesheet" type="text/css" href="/style.css"/>
    <link rel="stylesheet" type="text/css" href="https://unpkg.com/ress/dist/ress.min.css"/>
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/image/favicon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/image/favicon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/image/favicon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/image/favicon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/image/favicon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/image/favicon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/image/favicon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/image/favicon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/image/favicon-180x180.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/image/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/image/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/assets/image/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/assets/image/favicon-192x192.png">
    <link rel="shortcut icon" type="image/x-icon" href="/assets/image/favicon.ico">
    <link rel="icon" type="image/x-icon" href="/assets/image/favicon.ico">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/assets/image/favicon-144x144.png">
    <meta name="msapplication-config" content="/assets/image/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <link rel="alternate" type="application/rss+xml" title="W Wunkaew - Omzin's portfolio and blog" href="/feed.xml"/>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css" integrity="sha256-XoaMnoYC5TH6/+ihMEnospgm0J1PM/nioxbOUdnM8HY=" crossorigin="anonymous" media="print" onload="this.media='all'"/>
    <link rel="stylesheet" href="https://cdn.plyr.io/3.6.5/plyr.css" media="print" onload="this.media='all'"/>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css" media="print" onload="this.media='all'"/>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css" integrity="sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET" crossorigin="anonymous" media="print" onload="this.media='all'"/>

  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="/assets/image/favicon.ico" width="70" height="70" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">W Wunkaew</a></h1>
            <p class="site-description">Omzin's portfolio and blog</p>
          </div>

          <nav class="nav-menu">
            
  <a class="nav-menu-item" href="/">Projects</a>

  <a class="nav-menu-item" href="/about">About</a>

  <a class="nav-menu-item" href="/search">Search</a>


          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Pricing American Options Using Machine Learning</h1>

  <div class="entry">
    <center>Wanchaloem Wunkaew </center>
<center>leegarap@uw.edu </center>
<center>University of Washington, Seattle, WA</center>

<p>This is an individual final project for CFRM 421/521: Machine Learning for Finance class at the University of Washington.</p>

<p>Note that this project is not peer-reviewed and that the project is for educational purpose. Please use it as your own risk.</p>

<p>The jupyter notebook of this project is accessible at <a href="https://github.com/middleOz/American-Option-Pricing-Machine-Learning/blob/main/American%20Option%20Pricing%20using%20Machine%20Learning.ipynb">this link</a></p>

<center><h4>Abstract</h4> </center>

<p>We proprose machine learning methods for regression including Linear Regression, Polynomial Regression, Support Vector Regressor Ridge and Lasso Regression, Random Forest Regressor, K-Nearest Neighbors Regression, Multi-layer perceptons, and Convolutional Neural Network to price American options. The methods are trained and tested on SPDR S&amp;P 500 ETF Trust call options, from 2021 to 2022.  The result shows that Convolutional Neural Network and the Random Forest performs better than the Binomial Tree, which we use as a benchmark, in the term of testing Root Mean Squared Errors.</p>

<h2 id="introduction">Introduction</h2>

<p>Option pricing is one of fields in financial engineering. The formalization of option pricing methods, such as the Black-Scholes equation, has greatly impactedthe field of financial economics. Among various types of options, the American Option is a distinct financial asset that grants its holder the right to buy or sell the underlying asset at any point up to, and including, its maturity date. Unlike European options, American options do not have a closed-form solution, so it requires the use of numerical methods for their pricing. The Binomial Tree and Monte Carlo simulations are two such numerical methods capable of pricing these options. One notable limitation in all option pricing methods is the unrealistic assumptions of underlying asset price models. For instance, the Geometric Brownian motion model, which is assumed in the Black-Scholes equation, does not account for heteroskedasticity and the non-normal log return. In addition, some parameters, such as $\sigma$ in the binomial tree, in these tradtional methods/models are hard to estimate.</p>

<p>In this project, we employ a range of machine learning regression models, including linear regression, polynomial regression, ridge regression, lasso regression, Support Vector Regressor, Random Forest Regressor, K-Nearest Neighbor regressor, Multilayer Perceptron regressor, and Convolutional Neural Network, to price American options. We anticipate that these models may unveil relationships between inputs (such as the strike price and stock prices from 8 days prior) and the output (the option price). As such, we expect these models to either outperform or match the performance of the traditional Binomial Tree model, which we have selected as our research benchmark. Additionally, we believe that some of these models can rectify the flaws of traditional models as outlined above.</p>

<p>We will divide this paper into distinct sections. In the next section, we will discuss our dataset and the Binomial Tree, which serves as our benchmark. The third section will be devoted to training machine learning models on this dataset. In the fourth section, we will summarize and discuss our findings. Lastly, in the fifth section, we will draw conclusions based on our results.</p>

<h2 id="data-preparation-and-benchmark-model">Data Preparation and Benchmark model</h2>

<p>We will employ the SPDR S&amp;P 500 ETF Trust option chains from Q1 2020-Q4 2022 for our analysis. This data, which consists of more than three million options traded in markets, was downloads from <a href="https://www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022">Kaggle</a>.  The dataset encompasses a wealth of information, including but not restricted to, the closing option price, the closing strike price, underlying asset price, bid and ask prices, and implied volatility. Despite the fact that the dataset includes put option data, our study will only concentrate on call options.</p>

<p>The features incorporated in this project consist of: strike price, dividend yield, risk-free rate, the time until the option’s maturity, historical volatility, and the underlying asset (adjusted closed) prices from seven days prior (including the closed price on the date that the option is traded). A majority of these features encompass parameters used to price options in the Binomial Tree model. Note that we assume no transaction fee.</p>

<p>The historical volatility was calculated by the standard deviation of the logarithmic return of the underlying asset over the five years preceding the date each option was observed. The risk-free rate was obtained from that Fama-French guy website. The stock prices were obtained from Yahoo Finance via yfinance as showed below, while the dividend yield was estimated from <a href="https://ycharts.com/companies/SPY/dividend_yield"> Ycharts</a>.</p>

<p>Despite the dataset’s size with more than three million option data entries, we will randomly sample 100,000 options for training and testing our models due to time limitation. Also, we will choose only first 10000 of the traning data for cross validation.</p>

<p>The output of the model is solely the corresponding call option price, denoted as “’ [C_LAST]’’ within the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">scatter_matrix</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get data from https://www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"./spy_2020_2022.csv"</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Randomly chose 100000 samples for traning, validating, and testing
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100000</span><span class="p">,</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>The columns of the option data are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">columns</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['[QUOTE_UNIXTIME]', ' [QUOTE_READTIME]', ' [QUOTE_DATE]',
       ' [QUOTE_TIME_HOURS]', ' [UNDERLYING_LAST]', ' [EXPIRE_DATE]',
       ' [EXPIRE_UNIX]', ' [DTE]', ' [C_DELTA]', ' [C_GAMMA]', ' [C_VEGA]',
       ' [C_THETA]', ' [C_RHO]', ' [C_IV]', ' [C_VOLUME]', ' [C_LAST]',
       ' [C_SIZE]', ' [C_BID]', ' [C_ASK]', ' [STRIKE]', ' [P_BID]',
       ' [P_ASK]', ' [P_SIZE]', ' [P_LAST]', ' [P_DELTA]', ' [P_GAMMA]',
       ' [P_VEGA]', ' [P_THETA]', ' [P_RHO]', ' [P_IV]', ' [P_VOLUME]',
       ' [STRIKE_DISTANCE]', ' [STRIKE_DISTANCE_PCT]'],
      dtype='object')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load option data
#df = pd.read_csv("./spy_20_21.csv")
#df = df.iloc[:,1:]
</span><span class="n">df</span><span class="p">[</span><span class="s">' [QUOTE_DATE]'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">' [QUOTE_DATE]'</span><span class="p">],</span> <span class="nb">format</span> <span class="o">=</span> <span class="s">' %Y-%m-%d'</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">" [QUOTE_DATE]"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">' [UNDERLYING_LAST]'</span><span class="p">,</span><span class="s">' [EXPIRE_DATE]'</span><span class="p">,</span><span class="s">' [C_IV]'</span><span class="p">,</span><span class="s">' [C_LAST]'</span><span class="p">,</span><span class="s">' [STRIKE]'</span><span class="p">]]</span>
<span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'underlying_last'</span><span class="p">,</span><span class="s">'maturity'</span><span class="p">,</span><span class="s">'implied_vol'</span><span class="p">,</span><span class="s">'call_last'</span><span class="p">,</span> <span class="s">'K'</span><span class="p">]</span>

<span class="c1"># load stock data
#import yfinance as yf
# spy = yf.download('SPY', '2010-01-01', '2023-02-01')
# spy.to_csv('spy.csv')
</span>
<span class="n">spy</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./spy.csv'</span><span class="p">)</span>
<span class="n">spy</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">spy</span><span class="p">[</span><span class="s">'Date'</span><span class="p">],</span> <span class="nb">format</span> <span class="o">=</span> <span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'maturity'</span><span class="p">]</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'maturity'</span><span class="p">],</span> <span class="nb">format</span> <span class="o">=</span> <span class="s">' %Y-%m-%d'</span><span class="p">)</span>
<span class="n">spy</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Date'</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Choose only data and adjust close
</span><span class="n">spy</span> <span class="o">=</span> <span class="n">spy</span><span class="p">[[</span><span class="s">'Adj Close'</span><span class="p">]]</span>

<span class="c1"># Computing log return
</span><span class="n">spy</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">spy</span><span class="p">[</span><span class="s">'Adj Close'</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">spy</span><span class="p">[</span><span class="s">'Adj Close'</span><span class="p">])).</span><span class="n">dropna</span><span class="p">())</span>
<span class="n">spy</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'ret'</span><span class="p">]</span>
</code></pre></div></div>

<p>Compute historical volatility $q$ by finding the std of the log return 1825 days (~5 years) in back in the past.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_days_hist</span> <span class="o">=</span> <span class="mi">365</span> <span class="o">*</span> <span class="mi">5</span>
<span class="n">date_ls</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">vol_ls</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">spy_array</span> <span class="o">=</span> <span class="n">spy</span><span class="p">[</span><span class="s">'ret'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_days_hist</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">spy_array</span><span class="p">)):</span>
    <span class="n">date_ls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">spy</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">vol_ls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">spy_array</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">n_days_hist</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">252</span><span class="p">))</span>
<span class="n">hist_vol_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'Date'</span><span class="p">:</span> <span class="n">date_ls</span><span class="p">,</span> <span class="s">"hist_vol"</span><span class="p">:</span><span class="n">vol_ls</span><span class="p">})</span>
<span class="n">hist_vol_df</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Date'</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">hist_vol_df</span><span class="p">)</span>
</code></pre></div></div>

<p>Dividend Yields are obtained and approximated from ychart website.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dividend Yield
# estimates from https://ycharts.com/companies/SPY/dividend_yield
# 2022 1.34%
# 2021 1.5
# 2020 1.7
</span>
<span class="n">d_yield</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2020</span><span class="p">:</span> <span class="mf">1.7</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2021</span><span class="p">:</span> <span class="mf">1.5</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2022</span><span class="p">:</span> <span class="mf">1.34</span><span class="o">/</span><span class="mi">100</span><span class="p">}</span>
<span class="n">y_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">y_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">d_yield</span><span class="p">[</span><span class="n">ind</span><span class="p">.</span><span class="n">year</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s">'q'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_list</span>
</code></pre></div></div>

<p>We obtain risk-free rate $r$ from <a href="https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html">Kenneth French website (Fama/French 3 Factors)</a>.</p>

<p>We approximate the risk-free on a specific date by chooing the risk on the first day of a month corresponding to the option data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Interest rate
# Note that the csv file below was preprocessed by removing unnecessary rows and columns that broke the read_csv
</span><span class="n">mf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"F-F_Research_Data_Factors.CSV"</span><span class="p">).</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">mf</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Date'</span><span class="p">,</span><span class="s">'r'</span><span class="p">]</span>
<span class="n">mf</span><span class="p">[</span><span class="s">'r'</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">100</span>
<span class="n">mf</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">mf</span><span class="p">[</span><span class="s">'Date'</span><span class="p">],</span> <span class="nb">format</span> <span class="o">=</span> <span class="s">'%Y%m'</span><span class="p">)</span>                
<span class="n">mf</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>r</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1926-07-01</td>
      <td>0.0296</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1926-08-01</td>
      <td>0.0264</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1926-09-01</td>
      <td>0.0036</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1926-10-01</td>
      <td>-0.0324</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1926-11-01</td>
      <td>0.0253</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"F-F_Research_Data_Factors.CSV"</span><span class="p">)</span>
<span class="n">mf</span> <span class="o">=</span> <span class="n">mf</span><span class="p">[[</span><span class="s">"Unnamed: 0"</span><span class="p">,</span><span class="s">'RF'</span><span class="p">]]</span>
<span class="n">mf</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Date'</span><span class="p">,</span><span class="s">'r'</span><span class="p">]</span>

<span class="n">r_years</span> <span class="o">=</span> <span class="n">mf</span><span class="p">[</span><span class="s">'Date'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)[:</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">r_months</span> <span class="o">=</span>  <span class="n">mf</span><span class="p">[</span><span class="s">'Date'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">4</span><span class="p">:]))</span>

<span class="n">mf</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">=</span> <span class="n">r_years</span>
<span class="n">mf</span><span class="p">[</span><span class="s">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">r_months</span>
<span class="n">mf</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">mf</span> <span class="o">=</span> <span class="n">mf</span><span class="p">[</span><span class="mi">2020</span> <span class="o">&lt;=</span> <span class="n">mf</span><span class="p">[</span><span class="s">'year'</span><span class="p">]]</span>
<span class="n">r_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">):</span>
    <span class="n">r_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mf</span><span class="p">[(</span><span class="n">mf</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">==</span> <span class="n">ind</span><span class="p">.</span><span class="n">year</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">mf</span><span class="p">[</span><span class="s">'month'</span><span class="p">]</span> <span class="o">==</span> <span class="n">ind</span><span class="p">.</span><span class="n">month</span><span class="p">)][</span><span class="s">'r'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s">'r'</span><span class="p">]</span> <span class="o">=</span> <span class="n">r_list</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|█████████████████████████████████| 100000/100000 [00:42&lt;00:00, 2368.09it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">r_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">r_i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">):</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">mf</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">r_i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span><span class="n">ind</span><span class="p">.</span><span class="n">year</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">mf</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">r_i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>  <span class="o">!=</span> <span class="n">ind</span><span class="p">.</span><span class="n">month</span><span class="p">):</span>
        <span class="n">r_i</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">r_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mf</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">r_i</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s">'r'</span><span class="p">]</span> <span class="o">=</span> <span class="n">r_list</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|████████████████████████████████| 100000/100000 [00:05&lt;00:00, 18277.71it/s]
</code></pre></div></div>

<p>Now, we compute the time until maturity $T-t$ in a year.
This is computed by dividing a number of days between the day that the option data was observed and it maturity by 365.</p>

<p>We will denote this feature as $T$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Time til maturity
</span><span class="n">dd_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">days</span> <span class="ow">in</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'maturity'</span><span class="p">]</span><span class="o">-</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">):</span>
    <span class="n">dd_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">days</span><span class="p">.</span><span class="n">days</span><span class="o">/</span><span class="mi">365</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'T'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dd_list</span>
</code></pre></div></div>

<p>A scatter matrix which conclues the dataset are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">"underlying_last"</span><span class="p">,</span><span class="s">"call_last"</span><span class="p">,</span> <span class="s">"K"</span><span class="p">,</span><span class="s">"hist_vol"</span><span class="p">,</span><span class="s">"q"</span><span class="p">,</span><span class="s">"r"</span><span class="p">,</span><span class="s">"T"</span><span class="p">]],</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/image/output_24_0.png" alt="png" /></p>

<h3 id="benchmark-american-option">Benchmark: American option</h3>

<p>In the next subsection, we will discuss the traditional method of pricing American option: Binomial Tree.
Given an initial stock, in a next step, the stock price will either go up or go down, under some pre-defined multiplicative factors: u and d.
\(u = e^{\sigma \Delta t}\)
\(d = \frac{1}{u}\)
u and d depends solely on the volatility, so we used historical volatility to estimate this $\sigma$.</p>

<p>For stock (or index) price with continuous dividend, the risk-neutral probability is computed by
\(\hat{p} = \frac{e^{(r-q) \Delta t} - d}{u -d}\)</p>

<p>The price for American option at node i can be computed by
\(f_i = \max (e^{-r \Delta T} (\hat{p} f_{iu} + (1- \hat{p}) f_{id}, (s_i - K)^+)\)</p>

<p>where the payoff at the leaf nodes i are $ (s_i - K)^+$.</p>

<p>The prices of options, as observed from the market at a given time, represent the values that investors expect those options to have under the circumstances. Theorically, if we have all the necessary parameters to price an option, we can construct option prices in the market. For binomial tree, most of the parameters such as $r$, $q$,  and $S_0$ can be observed or estimated except for the $\sigma$ which is hard to estimate. One way is to construct that quantity by inversely solving the model given the option price and all other paramemters. This implied volaitlity is the volatility of the underlying asset that the market expects. When pricing option, this quantity is unknown but can be estimated. Due to volatitility smile, in many traditional model, it is hard to use the quantity in the model. Since this implied volatility can be used to reconstruct exact price of an individual option. We will not use this quantity in our model.</p>

<p>To estimate the volatility, werely on historical volatility of the underlying asset price. By analyzing the historical price movements of the asset, we can make an estimation of $\sigma$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># American Option pricing using binomial tree
# adapted from Kevin Mooney (see reference)
</span>
<span class="k">def</span> <span class="nf">american_call_price</span><span class="p">(</span><span class="n">S0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">):</span>

    <span class="c1">#delta t
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
    <span class="n">d</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">u</span>

    <span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="n">r</span><span class="o">-</span><span class="n">q</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">stock_prices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="p">)</span>
    <span class="n">call_prices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">stock_prices</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">S0</span>
    <span class="n">M</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="p">):</span>
        <span class="n">M</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">stock_prices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span> <span class="o">*</span> <span class="n">stock_prices</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span> <span class="p">):</span>
            <span class="n">stock_prices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">stock_prices</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">expiration</span> <span class="o">=</span> <span class="n">stock_prices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">K</span>
    <span class="n">expiration</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">q</span><span class="o">*</span><span class="n">t</span> <span class="o">*</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">stock_prices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">K</span>
    <span class="n">expiration</span><span class="p">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">expiration</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">expiration</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">expiration</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">expiration</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">call_prices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span>  <span class="n">expiration</span>

    <span class="c1"># backward computing value
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># American Payoff
</span>            <span class="n">call_prices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">call_prices</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="n">call_prices</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">([</span><span class="n">stock_prices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">K</span><span class="p">,</span><span class="mi">0</span><span class="p">])])</span>         
    <span class="k">return</span> <span class="n">call_prices</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>We use 10-step tree for option pricing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># American Option
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">bm_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))):</span>
    <span class="n">current_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>

    <span class="n">S0</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'underlying_last'</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'K'</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'hist_vol'</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'r'</span><span class="p">]</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'q'</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'T'</span><span class="p">]</span>
    <span class="n">bm_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">american_call_price</span><span class="p">(</span><span class="n">S0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">r</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s">'bm'</span><span class="p">]</span> <span class="o">=</span> <span class="n">bm_list</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|                                    | 126/100000 [00:00&lt;01:19, 1255.55it/s]/var/folders/6r/96ncs6hd5plcz0t1d7spstzr0000gn/T/ipykernel_47931/145875048.py:11: RuntimeWarning: invalid value encountered in double_scalars
  p = (np.exp((r-q) * t) - d) / (u - d)
100%|█████████████████████████████████| 100000/100000 [01:12&lt;00:00, 1377.00it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'call_last'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">" "</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s">'call_last'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">double</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'call_last'</span><span class="p">])</span>
<span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>The table for option data are shown below.
Note that we are not going to use all the columns in the table.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>underlying_last</th>
      <th>maturity</th>
      <th>implied_vol</th>
      <th>call_last</th>
      <th>K</th>
      <th>hist_vol</th>
      <th>q</th>
      <th>r</th>
      <th>T</th>
      <th>bm</th>
    </tr>
    <tr>
      <th>[QUOTE_DATE]</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>324.87</td>
      <td>2020-09-30</td>
      <td>0.199590</td>
      <td>33.40</td>
      <td>300.0</td>
      <td>0.128190</td>
      <td>0.0170</td>
      <td>0.13</td>
      <td>0.745205</td>
      <td>47.199011</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>324.87</td>
      <td>2020-03-31</td>
      <td>0.557560</td>
      <td>100.30</td>
      <td>215.0</td>
      <td>0.128190</td>
      <td>0.0170</td>
      <td>0.13</td>
      <td>0.243836</td>
      <td>114.648588</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>324.87</td>
      <td>2020-01-27</td>
      <td>0.130940</td>
      <td>9.09</td>
      <td>316.0</td>
      <td>0.128190</td>
      <td>0.0170</td>
      <td>0.13</td>
      <td>0.068493</td>
      <td>11.905250</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>324.87</td>
      <td>2020-06-30</td>
      <td>0.286710</td>
      <td>70.85</td>
      <td>255.0</td>
      <td>0.128190</td>
      <td>0.0170</td>
      <td>0.13</td>
      <td>0.493151</td>
      <td>81.585331</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>324.87</td>
      <td>2020-01-31</td>
      <td>0.118620</td>
      <td>6.17</td>
      <td>321.5</td>
      <td>0.128190</td>
      <td>0.0170</td>
      <td>0.13</td>
      <td>0.079452</td>
      <td>8.142697</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2022-12-30</th>
      <td>382.44</td>
      <td>2023-01-27</td>
      <td>0.169750</td>
      <td>0.38</td>
      <td>415.0</td>
      <td>0.189685</td>
      <td>0.0134</td>
      <td>0.33</td>
      <td>0.076712</td>
      <td>1.201606</td>
    </tr>
    <tr>
      <th>2022-12-30</th>
      <td>382.44</td>
      <td>2024-01-19</td>
      <td>0.167740</td>
      <td>1.86</td>
      <td>515.0</td>
      <td>0.189685</td>
      <td>0.0134</td>
      <td>0.33</td>
      <td>1.054795</td>
      <td>27.581163</td>
    </tr>
    <tr>
      <th>2022-12-30</th>
      <td>382.44</td>
      <td>2023-01-03</td>
      <td></td>
      <td>0.00</td>
      <td>332.0</td>
      <td>0.189685</td>
      <td>0.0134</td>
      <td>0.33</td>
      <td>0.010959</td>
      <td>51.526183</td>
    </tr>
    <tr>
      <th>2022-12-30</th>
      <td>382.44</td>
      <td>2024-01-19</td>
      <td>0.193650</td>
      <td>0.16</td>
      <td>670.0</td>
      <td>0.189685</td>
      <td>0.0134</td>
      <td>0.33</td>
      <td>1.054795</td>
      <td>0.458140</td>
    </tr>
    <tr>
      <th>2022-12-30</th>
      <td>382.44</td>
      <td>2023-01-20</td>
      <td>0.681010</td>
      <td>0.01</td>
      <td>685.0</td>
      <td>0.189685</td>
      <td>0.0134</td>
      <td>0.33</td>
      <td>0.057534</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
<p>96924 rows × 10 columns</p>
</div>

<p>The measure for quantiative models that are widely used in machine learning is the root mean square error.
As we use the binomial model as a benchmark, we will compute the RMSE for the benchmark binomial model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># MSE
</span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'call_last'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'bm'</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>51.301668827531735
</code></pre></div></div>

<p>The RMSE for the benchmark model is ~51 which is high.</p>

<p>The statistics for the absolute error is shown below. The median error is around 3.3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'bm'</span><span class="p">]</span><span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="s">'call_last'</span><span class="p">]),</span><span class="n">bins</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"A histogram of error (in absolute difference) of Binomial Model"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"density"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Error"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'Error')
</code></pre></div></div>

<p><img src="/assets/image/output_37_1.png" alt="png" /></p>

<h2 id="machine-learning-model">Machine Learning Model</h2>

<p>In addition to the (closed) underlying price, days-to-maturity, historical volatility, dividend yield, interest rate and strike price, we will also use the adjusted closed stock prices 8 days lag as inputs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Tn</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">spy</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./spy.csv'</span><span class="p">)</span>
<span class="n">spy</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">spy</span><span class="p">[</span><span class="s">'Date'</span><span class="p">],</span> <span class="nb">format</span> <span class="o">=</span> <span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">spy</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">"Date"</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">spy</span> <span class="o">=</span> <span class="n">spy</span><span class="p">[[</span><span class="s">'Adj Close'</span><span class="p">]]</span>
<span class="n">spy</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">"Adj Close"</span><span class="p">:</span> <span class="s">"t0"</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">spy_use</span> <span class="o">=</span> <span class="n">spy</span><span class="p">[</span><span class="n">spy</span><span class="p">.</span><span class="n">index</span> <span class="o">&gt;=</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">Tn</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">spy_use</span><span class="p">[</span><span class="s">'t-'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">spy</span><span class="p">[</span><span class="s">'t0'</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">spy_use</span><span class="p">[</span><span class="s">'t0'</span><span class="p">]):]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">spy_use</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># choose only numerical
</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'maturity'</span><span class="p">,</span><span class="s">'implied_vol'</span><span class="p">,</span><span class="s">'bm'</span><span class="p">,</span><span class="s">'underlying_last'</span><span class="p">],</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>The first 5 rows of  final prepared dataset is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>call_last</th>
      <th>K</th>
      <th>hist_vol</th>
      <th>q</th>
      <th>r</th>
      <th>T</th>
      <th>t0</th>
      <th>t-1</th>
      <th>t-2</th>
      <th>t-3</th>
      <th>t-4</th>
      <th>t-5</th>
      <th>t-6</th>
      <th>t-7</th>
      <th>t-8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>33.40</td>
      <td>300.0</td>
      <td>0.12819</td>
      <td>0.017</td>
      <td>0.13</td>
      <td>0.745205</td>
      <td>308.517456</td>
      <td>305.658936</td>
      <td>304.918213</td>
      <td>306.608582</td>
      <td>306.684631</td>
      <td>305.060669</td>
      <td>305.051178</td>
      <td>304.585846</td>
      <td>303.256348</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>100.30</td>
      <td>215.0</td>
      <td>0.12819</td>
      <td>0.017</td>
      <td>0.13</td>
      <td>0.243836</td>
      <td>308.517456</td>
      <td>305.658936</td>
      <td>304.918213</td>
      <td>306.608582</td>
      <td>306.684631</td>
      <td>305.060669</td>
      <td>305.051178</td>
      <td>304.585846</td>
      <td>303.256348</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>9.09</td>
      <td>316.0</td>
      <td>0.12819</td>
      <td>0.017</td>
      <td>0.13</td>
      <td>0.068493</td>
      <td>308.517456</td>
      <td>305.658936</td>
      <td>304.918213</td>
      <td>306.608582</td>
      <td>306.684631</td>
      <td>305.060669</td>
      <td>305.051178</td>
      <td>304.585846</td>
      <td>303.256348</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>70.85</td>
      <td>255.0</td>
      <td>0.12819</td>
      <td>0.017</td>
      <td>0.13</td>
      <td>0.493151</td>
      <td>308.517456</td>
      <td>305.658936</td>
      <td>304.918213</td>
      <td>306.608582</td>
      <td>306.684631</td>
      <td>305.060669</td>
      <td>305.051178</td>
      <td>304.585846</td>
      <td>303.256348</td>
    </tr>
    <tr>
      <th>2020-01-02</th>
      <td>6.17</td>
      <td>321.5</td>
      <td>0.12819</td>
      <td>0.017</td>
      <td>0.13</td>
      <td>0.079452</td>
      <td>308.517456</td>
      <td>305.658936</td>
      <td>304.918213</td>
      <td>306.608582</td>
      <td>306.684631</td>
      <td>305.060669</td>
      <td>305.051178</td>
      <td>304.585846</td>
      <td>303.256348</td>
    </tr>
  </tbody>
</table>
</div>

<p>The original data is timestamped with the time that each option is observed. The chronological order of the data may have an effect on the model, so it is important to take the order into account. For this option pricing project, we assume that the chronological order does not have a significant effect on the models. We make an assumption that the most chornological effects are contained in the features such as historical volatility and the stock price lags.</p>

<p>We shuffle and split 80% for traning set, 16% for test set, 4% for validation sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="c1"># We split 80% for traning set, 16% for test set, 4% for validation sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'call_last'</span><span class="p">]),</span><span class="n">df</span><span class="p">[</span><span class="s">'call_last'</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># If you believe the order affect, use the code below
#X_train = df[df.index &lt;= datetime.datetime(2022,9,1)].copy()
#y_train = X_train[['call_last']]
#X_train = X_train.drop(columns = ['call_last'])
</span>
<span class="c1">#X_test = df[df.index &gt; datetime.datetime(2022,9,1)].copy()
#y_test = X_test[['call_last']]
#X_test = X_test.drop(columns = ['call_last'])
</span>
<span class="c1">#X_valid = X_test.iloc[:1000,:]
#y_valid = y_test.iloc[:1000]
</span>
<span class="c1">#X_test = X_test.iloc[1000:,:]
#y_test = y_test.iloc[1000:]
</span></code></pre></div></div>

<p>As we obtained the dataset, we compute the option price based on American binomial tree model on the test set in order to compare it to other models. Since a number of the test set is small, we can apply the tree for large number of steps. In this case, we use 30 steps.</p>

<h3 id="benchmark-binomial-tree">Benchmark (Binomial Tree)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bm_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))):</span>
    <span class="n">current_row</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>

    <span class="n">S0</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'t0'</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'K'</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'hist_vol'</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'r'</span><span class="p">]</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'q'</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">current_row</span><span class="p">[</span><span class="s">'T'</span><span class="p">]</span>
    <span class="n">bm_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">american_call_price</span><span class="p">(</span><span class="n">S0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">r</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">30</span> <span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|████████████████████████████████████| 15508/15508 [01:25&lt;00:00, 180.46it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">bm_list</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>48.219430763590715
</code></pre></div></div>

<p>The root mean squared error for the binomial model is 48.219430763590715.</p>

<h3 id="linear-regression">Linear Regression</h3>

<p>We use multiple linear regression with stdard scaled input.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span><span class="n">SGDRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lin_reg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"std_scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s">"LinReg"</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
<span class="n">lin_reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;LinReg&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" /><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;LinReg&#x27;, LinearRegression())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" /><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" /><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">lin_reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>38.43095625260875
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">lin_reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>37.97049534873681
</code></pre></div></div>

<p>The coefficients and intercept for the model are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lin_reg</span><span class="p">[</span><span class="s">"LinReg"</span><span class="p">].</span><span class="n">coef_</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([-37.65425523,   2.524246  ,   1.51641187,  -0.54682013,
        13.45049636,   5.76041445,   3.59107429,   3.32804942,
         3.28542487,  -0.09017812,  -0.74559882,  -1.94913388,
         0.6794828 ,   9.51963281])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lin_reg</span><span class="p">[</span><span class="s">"LinReg"</span><span class="p">].</span><span class="n">intercept_</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>31.72985981248147
</code></pre></div></div>

<h3 id="polynomial-regression">Polynomial Regression</h3>

<p>For the polynomial regression, we only consider the polynomial of degree 2 with standard scaled input. We have 14 features, and the polynomial features are going to be large.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span><span class="p">,</span><span class="n">GridSearchCV</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PolyReg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"std_scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
          <span class="p">(</span><span class="s">"poly_feature"</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)),</span>
         <span class="p">(</span><span class="s">"LinReg"</span><span class="p">,</span>  <span class="n">LinearRegression</span><span class="p">())])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PolyReg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;poly_feature&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;LinReg&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" /><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;poly_feature&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;LinReg&#x27;, LinearRegression())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" /><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" /><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures(include_bias=False)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" /><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">PolyReg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>36.17182222027887
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">PolyReg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>36.06896946882215
</code></pre></div></div>

<h3 id="ridge-and-lasso-regression">Ridge and Lasso Regression</h3>

<p>For Ridge and Lasso Regression, we also find the best hyperparameters $\alpha$ using Grid Search.
We perform cross validation only on the first 10000 training data with 3-fold cross validation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span><span class="n">Ridge</span><span class="p">,</span><span class="n">Lasso</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">5000</span><span class="p">,</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"std_scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s">"ridge"</span><span class="p">,</span>  <span class="n">Ridge</span><span class="p">())])</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'ridge__alpha'</span><span class="p">:</span> <span class="n">alpha</span><span class="p">}</span>
<span class="n">rr</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">ridge_reg</span><span class="p">,</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">rr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">])</span>

</code></pre></div></div>

<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;ridge&#x27;, Ridge())]),
             param_grid={&#x27;ridge__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" /><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;ridge&#x27;, Ridge())]),
             param_grid={&#x27;ridge__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" /><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" /><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" /><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge()</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rr</span><span class="p">.</span><span class="n">best_estimator_</span>
</code></pre></div></div>

<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" /><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" /><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-14" type="checkbox" /><label for="sk-estimator-id-14" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge(alpha=10)</pre></div></div></div></div></div></div></div>

<p>We found that $\alpha = 10$ is the best alpha.
We then fit this best estimator to the whole dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_ridge</span> <span class="o">=</span> <span class="n">rr</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-15" type="checkbox" /><label for="sk-estimator-id-15" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;ridge&#x27;, Ridge(alpha=10))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-16" type="checkbox" /><label for="sk-estimator-id-16" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-17" type="checkbox" /><label for="sk-estimator-id-17" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge(alpha=10)</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_ridge</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>38.43095969174494
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_ridge</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>37.97019255417378
</code></pre></div></div>

<p>We did the same to Lasso.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">5000</span><span class="p">,</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">lasso_reg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"std_scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s">"lasso"</span><span class="p">,</span>  <span class="n">Lasso</span><span class="p">())])</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'lasso__alpha'</span><span class="p">:</span> <span class="n">alpha</span><span class="p">}</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">lasso_reg</span><span class="p">,</span> <span class="n">param_grid</span>  <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">])</span>
</code></pre></div></div>

<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-18" type="checkbox" /><label for="sk-estimator-id-18" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-19" type="checkbox" /><label for="sk-estimator-id-19" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-20" type="checkbox" /><label for="sk-estimator-id-20" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-21" type="checkbox" /><label for="sk-estimator-id-21" class="sk-toggleable__label sk-toggleable__label-arrow">Lasso</label><div class="sk-toggleable__content"><pre>Lasso()</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_lasso</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_lasso</span>
</code></pre></div></div>

<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-22" type="checkbox" /><label for="sk-estimator-id-22" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-23" type="checkbox" /><label for="sk-estimator-id-23" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-24" type="checkbox" /><label for="sk-estimator-id-24" class="sk-toggleable__label sk-toggleable__label-arrow">Lasso</label><div class="sk-toggleable__content"><pre>Lasso(alpha=0.01)</pre></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_lasso</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-25" type="checkbox" /><label for="sk-estimator-id-25" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso(alpha=0.01))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-26" type="checkbox" /><label for="sk-estimator-id-26" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-27" type="checkbox" /><label for="sk-estimator-id-27" class="sk-toggleable__label sk-toggleable__label-arrow">Lasso</label><div class="sk-toggleable__content"><pre>Lasso(alpha=0.01)</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_lasso</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>38.431561232353104
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_lasso</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>37.969146928057036
</code></pre></div></div>

<p>The best $\alpha$ is 0.01.</p>

<p>We can use Lasso for feature selection as it tries to minimize unimportant features coefficients to 0.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="s">'lasso'</span><span class="p">].</span><span class="n">coef_</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([-37.64057787,   2.49377087,   1.45139028,  -0.55584586,
        13.43936913,   5.72526309,   3.68045696,   3.23670677,
         2.1807758 ,   0.        ,  -0.        ,  -0.        ,
         0.        ,   8.50465773])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">.</span><span class="n">columns</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['K', 'hist_vol', 'q', 'r', 'T', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',
       't-6', 't-7', 't-8'],
      dtype='object')
</code></pre></div></div>

<p>Interestingly many of the lags of stock prices are not important.</p>

<h3 id="support-vector-regressor">Support Vector Regressor</h3>

<p>For the linear, we tune the hyperparameter C and $\epsilon$ for linear SVR.
The other setting are the same as in previous.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVR</span>

<span class="n">lsvr</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"std_scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                 <span class="p">(</span><span class="s">'svr'</span><span class="p">,</span><span class="n">LinearSVR</span><span class="p">())])</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'svr__epsilon'</span><span class="p">:(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">5000</span><span class="p">,</span><span class="mi">10000</span><span class="p">),</span>
             <span class="s">'svr__C'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">5000</span><span class="p">,</span><span class="mi">10000</span><span class="p">)}</span>
<span class="n">lsvr_gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">lsvr</span><span class="p">,</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">lsvr_gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">])</span>
</code></pre></div></div>

<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, LinearSVR())]),
             param_grid={&#x27;svr__C&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,
                                    1000, 5000, 10000),
                         &#x27;svr__epsilon&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-28" type="checkbox" /><label for="sk-estimator-id-28" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, LinearSVR())]),
             param_grid={&#x27;svr__C&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500,
                                    1000, 5000, 10000),
                         &#x27;svr__epsilon&#x27;: (0.0001, 0.001, 0.01, 0.1, 1, 10, 100,
                                          500, 1000, 5000, 10000)})</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-29" type="checkbox" /><label for="sk-estimator-id-29" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, LinearSVR())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-30" type="checkbox" /><label for="sk-estimator-id-30" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-31" type="checkbox" /><label for="sk-estimator-id-31" class="sk-toggleable__label sk-toggleable__label-arrow">LinearSVR</label><div class="sk-toggleable__content"><pre>LinearSVR()</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lsvr_gs</span><span class="p">.</span><span class="n">best_estimator_</span>
</code></pre></div></div>

<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-10" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-32" type="checkbox" /><label for="sk-estimator-id-32" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-33" type="checkbox" /><label for="sk-estimator-id-33" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-34" type="checkbox" /><label for="sk-estimator-id-34" class="sk-toggleable__label sk-toggleable__label-arrow">LinearSVR</label><div class="sk-toggleable__content"><pre>LinearSVR(C=10, epsilon=10)</pre></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_lsvr</span> <span class="o">=</span> <span class="n">lsvr_gs</span><span class="p">.</span><span class="n">best_estimator_</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_lsvr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-11" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-35" type="checkbox" /><label for="sk-estimator-id-35" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, LinearSVR(C=10, epsilon=10))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-36" type="checkbox" /><label for="sk-estimator-id-36" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-37" type="checkbox" /><label for="sk-estimator-id-37" class="sk-toggleable__label sk-toggleable__label-arrow">LinearSVR</label><div class="sk-toggleable__content"><pre>LinearSVR(C=10, epsilon=10)</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_lsvr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>38.547851070665025
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_lsvr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>38.03050061254464
</code></pre></div></div>

<p>The tuned c and epsilon are 10 and 10 respectively.</p>

<p>Now, we consider nonlinear SVR,in addtion to $\epsilon$ and $C$, we include a type of kernel as a hyperparamter. We choose between rbf and sigmoid. Note that we reduce search space for $\epsilon$ and $C$ to accelerate the computing time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nlsvr</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">"std_scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                 <span class="p">(</span><span class="s">'svr'</span><span class="p">,</span><span class="n">SVR</span><span class="p">())])</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'svr__kernel'</span><span class="p">:(</span><span class="s">'rbf'</span><span class="p">,</span> <span class="s">'sigmoid'</span><span class="p">),</span>
              <span class="s">'svr__epsilon'</span><span class="p">:(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5000</span><span class="p">),</span>
             <span class="s">'svr__C'</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5000</span><span class="p">)</span>
              <span class="p">}</span>
<span class="n">nlsvr_gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">nlsvr</span><span class="p">,</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">nlsvr_gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">().</span><span class="n">ravel</span><span class="p">())</span>
</code></pre></div></div>

<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-12" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, SVR())]),
             n_jobs=-1,
             param_grid={&#x27;svr__C&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__epsilon&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__kernel&#x27;: (&#x27;rbf&#x27;, &#x27;sigmoid&#x27;)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-38" type="checkbox" /><label for="sk-estimator-id-38" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;svr&#x27;, SVR())]),
             n_jobs=-1,
             param_grid={&#x27;svr__C&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__epsilon&#x27;: (0.001, 0.01, 0.1, 1, 100, 5000),
                         &#x27;svr__kernel&#x27;: (&#x27;rbf&#x27;, &#x27;sigmoid&#x27;)})</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-39" type="checkbox" /><label for="sk-estimator-id-39" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, SVR())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-40" type="checkbox" /><label for="sk-estimator-id-40" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-41" type="checkbox" /><label for="sk-estimator-id-41" class="sk-toggleable__label sk-toggleable__label-arrow">SVR</label><div class="sk-toggleable__content"><pre>SVR()</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_nlsvr</span> <span class="o">=</span> <span class="n">nlsvr_gs</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_nlsvr</span>
</code></pre></div></div>

<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-13" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-42" type="checkbox" /><label for="sk-estimator-id-42" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-43" type="checkbox" /><label for="sk-estimator-id-43" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-44" type="checkbox" /><label for="sk-estimator-id-44" class="sk-toggleable__label sk-toggleable__label-arrow">SVR</label><div class="sk-toggleable__content"><pre>SVR(C=100, epsilon=1)</pre></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_nlsvr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-14" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-45" type="checkbox" /><label for="sk-estimator-id-45" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;svr&#x27;, SVR(C=100, epsilon=1))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-46" type="checkbox" /><label for="sk-estimator-id-46" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-47" type="checkbox" /><label for="sk-estimator-id-47" class="sk-toggleable__label sk-toggleable__label-arrow">SVR</label><div class="sk-toggleable__content"><pre>SVR(C=100, epsilon=1)</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_nlsvr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>37.808085641917536
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_nlsvr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>37.92867379033997
</code></pre></div></div>

<h3 id="random-forest">Random Forest</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</code></pre></div></div>

<p>There are so many parameters, so we use random search instead of grid search. We randomly sample hyperparameters uniformly from the parameters lists for 100 samples. And we use first 10000 shuffled data entries for this tuning.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rf_rg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">'std_scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s">'rf'</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">())])</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'rf__n_estimators'</span><span class="p">:</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">500</span><span class="p">),</span>
             <span class="s">'rf__max_depth'</span><span class="p">:(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">),</span>
             <span class="s">'rf__ccp_alpha'</span><span class="p">:(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.00000001</span><span class="p">,</span><span class="mf">0.00001</span><span class="p">,</span><span class="mf">0.001</span><span class="p">),</span>
             <span class="s">'rf__bootstrap'</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]}</span>
<span class="n">rf_gs</span> <span class="o">=</span><span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">rf_rg</span><span class="p">,</span><span class="n">param_distributions</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rf_gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">().</span><span class="n">ravel</span><span class="p">())</span>
</code></pre></div></div>

<style>#sk-container-id-51 {color: black;background-color: white;}#sk-container-id-51 pre{padding: 0;}#sk-container-id-51 div.sk-toggleable {background-color: white;}#sk-container-id-51 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-51 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-51 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-51 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-51 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-51 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-51 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-51 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-51 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-51 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-51 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-51 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-51 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-51 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-51 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-51 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-51 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-51 div.sk-item {position: relative;z-index: 1;}#sk-container-id-51 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-51 div.sk-item::before, #sk-container-id-51 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-51 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-51 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-51 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-51 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-51 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-51 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-51 div.sk-label-container {text-align: center;}#sk-container-id-51 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-51 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-51" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomizedSearchCV(cv=3,
                   estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                             (&#x27;rf&#x27;, RandomForestRegressor())]),
                   n_iter=160, n_jobs=-1,
                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],
                                        &#x27;rf__ccp_alpha&#x27;: (0, 1e-08, 1e-05,
                                                          0.001),
                                        &#x27;rf__max_depth&#x27;: (None, 8, 32, 64, 128),
                                        &#x27;rf__n_estimators&#x27;: (50, 100, 300, 400,
                                                             500)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-167" type="checkbox" /><label for="sk-estimator-id-167" class="sk-toggleable__label sk-toggleable__label-arrow">RandomizedSearchCV</label><div class="sk-toggleable__content"><pre>RandomizedSearchCV(cv=3,
                   estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                             (&#x27;rf&#x27;, RandomForestRegressor())]),
                   n_iter=160, n_jobs=-1,
                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],
                                        &#x27;rf__ccp_alpha&#x27;: (0, 1e-08, 1e-05,
                                                          0.001),
                                        &#x27;rf__max_depth&#x27;: (None, 8, 32, 64, 128),
                                        &#x27;rf__n_estimators&#x27;: (50, 100, 300, 400,
                                                             500)})</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-168" type="checkbox" /><label for="sk-estimator-id-168" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;, RandomForestRegressor())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-169" type="checkbox" /><label for="sk-estimator-id-169" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-170" type="checkbox" /><label for="sk-estimator-id-170" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_rf</span> <span class="o">=</span> <span class="n">rf_gs</span><span class="p">.</span><span class="n">best_estimator_</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_rf</span>
</code></pre></div></div>

<style>#sk-container-id-52 {color: black;background-color: white;}#sk-container-id-52 pre{padding: 0;}#sk-container-id-52 div.sk-toggleable {background-color: white;}#sk-container-id-52 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-52 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-52 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-52 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-52 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-52 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-52 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-52 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-52 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-52 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-52 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-52 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-52 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-52 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-52 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-52 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-52 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-52 div.sk-item {position: relative;z-index: 1;}#sk-container-id-52 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-52 div.sk-item::before, #sk-container-id-52 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-52 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-52 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-52 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-52 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-52 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-52 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-52 div.sk-label-container {text-align: center;}#sk-container-id-52 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-52 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-52" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-171" type="checkbox" /><label for="sk-estimator-id-171" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-172" type="checkbox" /><label for="sk-estimator-id-172" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-173" type="checkbox" /><label for="sk-estimator-id-173" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)</pre></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-53 {color: black;background-color: white;}#sk-container-id-53 pre{padding: 0;}#sk-container-id-53 div.sk-toggleable {background-color: white;}#sk-container-id-53 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-53 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-53 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-53 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-53 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-53 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-53 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-53 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-53 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-53 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-53 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-53 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-53 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-53 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-53 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-53 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-53 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-53 div.sk-item {position: relative;z-index: 1;}#sk-container-id-53 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-53 div.sk-item::before, #sk-container-id-53 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-53 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-53 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-53 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-53 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-53 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-53 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-53 div.sk-label-container {text-align: center;}#sk-container-id-53 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-53 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-53" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-174" type="checkbox" /><label for="sk-estimator-id-174" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;rf&#x27;,
                 RandomForestRegressor(ccp_alpha=0, max_depth=8,
                                       n_estimators=400))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-175" type="checkbox" /><label for="sk-estimator-id-175" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-176" type="checkbox" /><label for="sk-estimator-id-176" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(ccp_alpha=0, max_depth=8, n_estimators=400)</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">The</span> <span class="n">root</span> <span class="n">mean</span> <span class="n">squared</span> <span class="n">errors</span> <span class="k">for</span> <span class="n">training</span> <span class="ow">and</span> <span class="n">testing</span> <span class="nb">set</span> <span class="n">are</span> <span class="n">shown</span> <span class="n">below</span><span class="p">:</span><span class="c1"># Training Loss
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>31.349066580782836
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Testing RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>32.90546500222834
</code></pre></div></div>

<p>With Random Forest, we can see feature importance:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_rf</span><span class="p">[</span><span class="s">'rf'</span><span class="p">].</span><span class="n">feature_importances_</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.51617093, 0.04662939, 0.00114048, 0.00432427, 0.13790168,
       0.04360533, 0.0377016 , 0.02562464, 0.03328482, 0.01493993,
       0.05282551, 0.00782497, 0.01813578, 0.05989066])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">.</span><span class="n">columns</span>
<span class="c1"># Strike Price and Time to maturity seem to be the most important
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['K', 'hist_vol', 'q', 'r', 'T', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',
       't-6', 't-7', 't-8'],
      dtype='object')
</code></pre></div></div>

<p>The best hyperparamters are ccp_alpha=0, max_depth=8, n_estimators=400, and bootstrap = False.</p>

<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>

<p>In this model, we only tune a number of neighbors as a hyperparameter.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">knn_rg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">'std_scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                  <span class="p">(</span><span class="s">'knn'</span><span class="p">,</span><span class="n">KNeighborsRegressor</span><span class="p">())])</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'knn__n_neighbors'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">)}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">knn_gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">knn_rg</span><span class="p">,</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">knn_gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">10000</span><span class="p">])</span>
</code></pre></div></div>

<style>#sk-container-id-44 {color: black;background-color: white;}#sk-container-id-44 pre{padding: 0;}#sk-container-id-44 div.sk-toggleable {background-color: white;}#sk-container-id-44 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-44 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-44 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-44 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-44 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-44 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-44 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-44 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-44 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-44 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-44 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-44 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-44 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-44 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-44 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-44 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-44 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-44 div.sk-item {position: relative;z-index: 1;}#sk-container-id-44 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-44 div.sk-item::before, #sk-container-id-44 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-44 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-44 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-44 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-44 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-44 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-44 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-44 div.sk-label-container {text-align: center;}#sk-container-id-44 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-44 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-44" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;knn&#x27;, KNeighborsRegressor())]),
             n_jobs=-1,
             param_grid={&#x27;knn__n_neighbors&#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,
       90, 95])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-145" type="checkbox" /><label for="sk-estimator-id-145" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=3,
             estimator=Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                                       (&#x27;knn&#x27;, KNeighborsRegressor())]),
             n_jobs=-1,
             param_grid={&#x27;knn__n_neighbors&#x27;: array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,
       90, 95])})</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-146" type="checkbox" /><label for="sk-estimator-id-146" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-147" type="checkbox" /><label for="sk-estimator-id-147" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-148" type="checkbox" /><label for="sk-estimator-id-148" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_knn</span> <span class="o">=</span> <span class="n">knn_gs</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_knn</span>
</code></pre></div></div>

<style>#sk-container-id-45 {color: black;background-color: white;}#sk-container-id-45 pre{padding: 0;}#sk-container-id-45 div.sk-toggleable {background-color: white;}#sk-container-id-45 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-45 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-45 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-45 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-45 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-45 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-45 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-45 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-45 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-45 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-45 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-45 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-45 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-45 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-45 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-45 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-45 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-45 div.sk-item {position: relative;z-index: 1;}#sk-container-id-45 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-45 div.sk-item::before, #sk-container-id-45 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-45 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-45 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-45 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-45 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-45 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-45 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-45 div.sk-label-container {text-align: center;}#sk-container-id-45 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-45 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-45" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-149" type="checkbox" /><label for="sk-estimator-id-149" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-150" type="checkbox" /><label for="sk-estimator-id-150" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-151" type="checkbox" /><label for="sk-estimator-id-151" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor(n_neighbors=15)</pre></div></div></div></div></div></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_knn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<style>#sk-container-id-55 {color: black;background-color: white;}#sk-container-id-55 pre{padding: 0;}#sk-container-id-55 div.sk-toggleable {background-color: white;}#sk-container-id-55 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-55 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-55 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-55 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-55 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-55 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-55 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-55 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-55 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-55 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-55 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-55 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-55 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-55 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-55 div.sk-item {position: relative;z-index: 1;}#sk-container-id-55 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-55 div.sk-item::before, #sk-container-id-55 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-55 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-55 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-55 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-55 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-55 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-55 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-55 div.sk-label-container {text-align: center;}#sk-container-id-55 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-55 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-55" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-180" type="checkbox" /><label for="sk-estimator-id-180" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;std_scaler&#x27;, StandardScaler()),
                (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-181" type="checkbox" /><label for="sk-estimator-id-181" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-182" type="checkbox" /><label for="sk-estimator-id-182" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor(n_neighbors=15)</pre></div></div></div></div></div></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_knn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>31.92967572127278
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_knn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>32.912277382176526
</code></pre></div></div>

<p>The best number of the neigbors is 15.</p>

<h3 id="multi-layer-perceptron">Multi-layer perceptron</h3>

<p>For neutral network, we make experiments on three models, which are multilayer perceptron (with dropout and normalization, with relu as an activation function for the hidden layers and identity for the output layer), multilayer perceptron with tuned hyperparameters, and Convolutional Neural network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2023-05-31 17:15:39.270253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
     <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
     <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
     <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
     <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
     <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">callback</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mse"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"nadam"</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/1000
2424/2424 [==============================] - 8s 2ms/step - loss: 2348.1985 - val_loss: 1538.4142
Epoch 2/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1858.8630 - val_loss: 1519.5778
Epoch 3/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1826.4266 - val_loss: 1481.3267
Epoch 4/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1821.6594 - val_loss: 1479.6355
Epoch 5/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1815.7463 - val_loss: 1452.8606
Epoch 6/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1803.6791 - val_loss: 1457.6923
Epoch 7/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1796.4032 - val_loss: 1453.8772
Epoch 8/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1796.3320 - val_loss: 1446.6718
Epoch 9/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1788.8895 - val_loss: 1437.1597
Epoch 10/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1762.4237 - val_loss: 1441.7910
Epoch 11/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1789.3596 - val_loss: 1461.8259
Epoch 12/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1782.5536 - val_loss: 1455.1730
Epoch 13/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1772.4648 - val_loss: 1450.3599
Epoch 14/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1753.4039 - val_loss: 1453.5958
Epoch 15/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1778.0695 - val_loss: 1447.4929
...
Epoch 106/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1699.8079 - val_loss: 1426.6115
Epoch 107/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1737.1375 - val_loss: 1446.8939
Epoch 108/1000
2424/2424 [==============================] - 4s 2ms/step - loss: 1710.6053 - val_loss: 1421.2778
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 14)               56        
 ormalization)                                                   

 dropout (Dropout)           (None, 14)                0         

 dense (Dense)               (None, 30)                450       

 batch_normalization_1 (Batc  (None, 30)               120       
 hNormalization)                                                 

 dropout_1 (Dropout)         (None, 30)                0         

 dense_1 (Dense)             (None, 30)                930       

 batch_normalization_2 (Batc  (None, 30)               120       
 hNormalization)                                                 

 dropout_2 (Dropout)         (None, 30)                0         

 dense_2 (Dense)             (None, 20)                620       

 batch_normalization_3 (Batc  (None, 20)               80        
 hNormalization)                                                 

 dropout_3 (Dropout)         (None, 20)                0         

 dense_3 (Dense)             (None, 10)                210       

 batch_normalization_4 (Batc  (None, 10)               40        
 hNormalization)                                                 

 dropout_4 (Dropout)         (None, 10)                0         

 dense_4 (Dense)             (None, 1)                 11        

=================================================================
Total params: 2,637
Trainable params: 2,429
Non-trainable params: 208
_________________________________________________________________
</code></pre></div></div>

<p>The learning curve is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s">"loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"val_loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Epoch"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'Epoch')
</code></pre></div></div>

<p><img src="/assets/image/output_141_1.png" alt="png" /></p>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#trainig RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2424/2424 [==============================] - 2s 714us/step





36.21230441027238
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#testing RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>485/485 [==============================] - 0s 675us/step





35.71456726735933
</code></pre></div></div>

<p>The model seems to be underfitting.</p>

<h3 id="hyperparamter-tuned-mlp">Hyperparamter-tuned MLP</h3>

<p>We use keras tuner library to tune the hyperparameters of the network. The hyperparameter space is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">keras_tuner</span> <span class="k">as</span> <span class="n">kt</span>

<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">hp</span><span class="p">):</span>
    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">Int</span><span class="p">(</span><span class="s">"n_hidden"</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">n_neurons</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">Int</span><span class="p">(</span><span class="s">"n_neurons"</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">Float</span><span class="p">(</span><span class="s">"learning_rate"</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
                             <span class="n">sampling</span><span class="o">=</span><span class="s">"log"</span><span class="p">)</span>
    <span class="n">l2_rate</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">Float</span><span class="p">(</span><span class="s">"l2"</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                             <span class="n">sampling</span><span class="o">=</span><span class="s">"log"</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Nadam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Normalization</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_rate</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_rate</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mse"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">random_search_tuner</span> <span class="o">=</span> <span class="n">kt</span><span class="p">.</span><span class="n">RandomSearch</span><span class="p">(</span>
    <span class="n">build_model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s">"val_loss"</span><span class="p">,</span> <span class="n">max_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_search_tuner</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
                           <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Trial 20 Complete [00h 00m 53s]
val_loss: 1461.2305908203125

Best val_loss So Far: 1451.614013671875
Total elapsed time: 00h 15m 31s
INFO:tensorflow:Oracle triggered exit
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">random_search_tuner</span><span class="p">.</span><span class="n">get_best_hyperparameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'n_hidden': 7,
 'n_neurons': 15,
 'learning_rate': 0.0006237028864858578,
 'l2': 0.0003065801184974072}
</code></pre></div></div>

<p>The best hyperparameters we found are 7 hidden layers with 15 neurons each, 0.0006237028864858578 learning rate, and l2 = 0.0003065801184974072 for the l2 regularization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_nn</span> <span class="o">=</span> <span class="n">random_search_tuner</span><span class="p">.</span><span class="n">get_best_models</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">callback</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">history_best_nn</span> <span class="o">=</span> <span class="n">best_nn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback</span><span class="p">])</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/1000
2424/2424 [==============================] - 6s 1ms/step - loss: 1364.6182 - val_loss: 1498.1863
Epoch 2/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1363.1213 - val_loss: 1462.6725
Epoch 3/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1354.9094 - val_loss: 1493.1799
Epoch 4/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1349.7559 - val_loss: 1452.7467
Epoch 5/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1347.2516 - val_loss: 1499.4685
Epoch 6/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1341.6652 - val_loss: 1437.6245
Epoch 7/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.0056 - val_loss: 1423.4025
Epoch 8/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.5402 - val_loss: 1513.4679
Epoch 9/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1340.7520 - val_loss: 1569.9033
...
Epoch 556/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1242.6877 - val_loss: 1332.1714
Epoch 557/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1260.6245 - val_loss: 1501.7150
Epoch 558/1000
2424/2424 [==============================] - 3s 1ms/step - loss: 1245.4034 - val_loss: 1347.1127
</code></pre></div></div>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Traing RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">best_nn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2424/2424 [==============================] - 2s 597us/step

34.78459569413211
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">best_nn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>485/485 [==============================] - 0s 612us/step

34.821059741172114
</code></pre></div></div>

<p>The learning curve is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_best_nn</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s">"loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_best_nn</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"val_loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Epoch"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'Epoch')
</code></pre></div></div>

<p><img src="/assets/image/output_157_1.png" alt="png" /></p>

<h3 id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3>

<p>The structure of the convolutional network is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_conv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">1</span><span class="p">,),</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">,</span><span class="n">padding</span> <span class="o">=</span> <span class="s">'same'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">,</span><span class="n">padding</span> <span class="o">=</span> <span class="s">'same'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
<span class="p">])</span>


</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">callback</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model_conv</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mse"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"nadam"</span><span class="p">)</span>
<span class="n">history_cov</span> <span class="o">=</span> <span class="n">model_conv</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/1000
2424/2424 [==============================] - 7s 2ms/step - loss: 1632.2096 - val_loss: 1460.4231
Epoch 2/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1373.3662 - val_loss: 1505.1824
Epoch 3/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1366.6722 - val_loss: 1474.0969
Epoch 4/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1346.9469 - val_loss: 1475.6388
Epoch 5/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1334.1947 - val_loss: 1547.0897
Epoch 6/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1323.7288 - val_loss: 1439.5723
Epoch 7/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1310.3964 - val_loss: 1432.5355
Epoch 8/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1294.4939 - val_loss: 1449.5792
Epoch 9/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1290.2340 - val_loss: 1363.4564
...
Epoch 244/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1013.4796 - val_loss: 1174.7893
Epoch 245/1000
2424/2424 [==============================] - 5s 2ms/step - loss: 1002.9050 - val_loss: 1177.7996
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_conv</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 14, 32)            96        

 conv1d_1 (Conv1D)           (None, 14, 32)            3104      

 conv1d_2 (Conv1D)           (None, 12, 32)            3104      

 flatten (Flatten)           (None, 384)               0         

 dense_8 (Dense)             (None, 128)               49280     

 dense_9 (Dense)             (None, 64)                8256      

 dense_10 (Dense)            (None, 32)                2080      

 dense_11 (Dense)            (None, 1)                 33        

=================================================================
Total params: 65,953
Trainable params: 65,953
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<p>The learning curve is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_cov</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s">"loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_cov</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"val_loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Epoch"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'Epoch')
</code></pre></div></div>

<p><img src="/assets/image/output_164_1.png" alt="png" /></p>

<p>The root mean squared errors for training and testing set are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">model_conv</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2424/2424 [==============================] - 2s 920us/step





31.66131974960281
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing RMSE
</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">model_conv</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>485/485 [==============================] - 0s 814us/step





32.372653521773714
</code></pre></div></div>

<h2 id="result">Result</h2>

<p>The Root Mean Square Error for each machine learning models are shown below, together with the chosen hyperparamters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"Model"</span><span class="p">:</span> <span class="p">[</span><span class="s">"Binomial Tree"</span><span class="p">,</span><span class="s">'Linear Regression'</span><span class="p">,</span><span class="s">"Polynomial Regression"</span><span class="p">,</span><span class="s">"Ridge Regression"</span><span class="p">,</span><span class="s">"Lasso Regression"</span><span class="p">,</span><span class="s">"Linear SVR"</span><span class="p">,</span><span class="s">"Nonlinear SVR"</span><span class="p">,</span><span class="s">"Random Forest"</span><span class="p">,</span><span class="s">"KNN"</span><span class="p">,</span><span class="s">"Neural Network"</span><span class="p">,</span> <span class="s">"Neural Network (tuned)"</span><span class="p">,</span> <span class="s">" Convolutional Neutral Network"</span><span class="p">],</span>
                   <span class="s">"RMSE(training)"</span><span class="p">:[</span><span class="bp">None</span><span class="p">,</span>
<span class="mf">38.43095625260875</span><span class="p">,</span>
<span class="mf">36.17182222027887</span><span class="p">,</span>
<span class="mf">38.43095969174494</span><span class="p">,</span>
<span class="mf">38.431561232353104</span><span class="p">,</span>
<span class="mf">38.547851070665025</span><span class="p">,</span>
<span class="mf">37.808085641917536</span><span class="p">,</span>
<span class="mf">31.349066580782836</span><span class="p">,</span>
<span class="mf">31.92967572127278</span><span class="p">,</span>
<span class="mf">36.21230441027238</span><span class="p">,</span>
<span class="mf">34.78459569413211</span><span class="p">,</span>
<span class="mf">31.66131974960281</span>
<span class="p">],</span>
                <span class="s">"RMSE(testing)"</span><span class="p">:</span>
                   <span class="p">[</span><span class="mf">48.219430763590715</span><span class="p">,</span>
                    <span class="mf">37.97049534873681</span><span class="p">,</span>
<span class="mf">36.06896946882215</span><span class="p">,</span>
<span class="mf">37.97019255417378</span><span class="p">,</span>
<span class="mf">37.969146928057036</span><span class="p">,</span>
<span class="mf">38.03050061254464</span><span class="p">,</span>
<span class="mf">37.92867379033997</span><span class="p">,</span>
<span class="mf">32.90546500222834</span><span class="p">,</span>
<span class="mf">32.912277382176526</span><span class="p">,</span>
<span class="mf">35.71456726735933</span><span class="p">,</span>
<span class="mf">34.821059741172114</span><span class="p">,</span>
<span class="mf">32.372653521773714</span>
<span class="p">],</span>
<span class="s">"Hyperparameters"</span> <span class="p">:</span>
        <span class="p">[</span><span class="s">"29 steps"</span><span class="p">,</span><span class="s">""</span><span class="p">,</span>
<span class="s">"degree = 2"</span><span class="p">,</span>
<span class="s">"alpha = 10"</span><span class="p">,</span>
<span class="s">"alpha = 0.01"</span><span class="p">,</span>
<span class="s">"C=10, epsilon=10"</span><span class="p">,</span>
<span class="s">"C=100, epsilon=1"</span><span class="p">,</span>
<span class="s">"ccp_alpha=0, max_depth=8, n_estimators=400, bootstrap = False"</span><span class="p">,</span>
         <span class="s">"neighbors = 15"</span><span class="p">,</span>
        <span class="s">"consider model summary above"</span><span class="p">,</span>
        <span class="s">"consider model summary above"</span><span class="p">,</span>
        <span class="s">"consider model summary above"</span><span class="p">]</span>    
                   <span class="p">})</span>
<span class="n">res</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>RMSE(training)</th>
      <th>RMSE(testing)</th>
      <th>Hyperparameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Binomial Tree</td>
      <td>NaN</td>
      <td>48.219431</td>
      <td>29 steps</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Linear Regression</td>
      <td>38.430956</td>
      <td>37.970495</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Regression</td>
      <td>36.171822</td>
      <td>36.068969</td>
      <td>degree = 2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Ridge Regression</td>
      <td>38.430960</td>
      <td>37.970193</td>
      <td>alpha = 10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Lasso Regression</td>
      <td>38.431561</td>
      <td>37.969147</td>
      <td>alpha = 0.01</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Linear SVR</td>
      <td>38.547851</td>
      <td>38.030501</td>
      <td>C=10, epsilon=10</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Nonlinear SVR</td>
      <td>37.808086</td>
      <td>37.928674</td>
      <td>C=100, epsilon=1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Random Forest</td>
      <td>31.349067</td>
      <td>32.905465</td>
      <td>ccp_alpha=0, max_depth=8, n_estimators=400, bo...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN</td>
      <td>31.929676</td>
      <td>32.912277</td>
      <td>neighbors = 25</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Neural Network</td>
      <td>36.212304</td>
      <td>35.714567</td>
      <td>consider model summary above</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Neural Network (tuned)</td>
      <td>34.784596</td>
      <td>34.821060</td>
      <td>consider model summary above</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Convolutional Neutral Network</td>
      <td>31.661320</td>
      <td>32.372654</td>
      <td>consider model summary above</td>
    </tr>
  </tbody>
</table>
</div>

<p>Consider the RMSE for the testing set, the lower RMSE, the more accurate the model to compute option price. By this sole metric, all the machine learning model we used in this project performs better than the binomial tree.
By ranking the models by this value, we have that the Convolutional Neural Network performs the best, the random forest is the second best, and the K-Nearest Neighbors is the third.</p>

<p>The statistics for the absolute error for the best three models and the binomial tree are shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res_error</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"binomial abs_error"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">bm_list</span><span class="p">)</span> <span class="o">-</span><span class="n">y_test</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">().</span><span class="n">flatten</span><span class="p">())}).</span><span class="n">describe</span><span class="p">(),</span>
                        <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">model_conv</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">).</span><span class="n">describe</span><span class="p">(),</span>
          <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">best_rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span><span class="o">-</span><span class="n">y_test</span><span class="p">)).</span><span class="n">describe</span><span class="p">(),</span>
                       <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">best_knn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span> <span class="o">-</span> <span class="n">y_test</span> <span class="p">).</span><span class="n">describe</span><span class="p">(),],</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">res_error</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Binomial Tree Absolute Error"</span><span class="p">,</span><span class="s">"CNN Absolute Error"</span><span class="p">,</span><span class="s">"Random Forest Absolute Error"</span><span class="p">,</span><span class="s">"KNN Absolute Error"</span><span class="p">]</span>
<span class="n">res_error</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>485/485 [==============================] - 0s 908us/step
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Binomial Tree Absolute Error</th>
      <th>CNN Absolute Error</th>
      <th>Random Forest Absolute Error</th>
      <th>KNN Absolute Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>15508.000000</td>
      <td>15508.000000</td>
      <td>15508.000000</td>
      <td>15508.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>22.192882</td>
      <td>16.865260</td>
      <td>17.565473</td>
      <td>17.039533</td>
    </tr>
    <tr>
      <th>std</th>
      <td>42.810135</td>
      <td>27.633330</td>
      <td>27.825774</td>
      <td>28.158896</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000103</td>
      <td>0.000018</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.749303</td>
      <td>0.927289</td>
      <td>1.313031</td>
      <td>0.710667</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>7.469478</td>
      <td>4.053897</td>
      <td>5.481253</td>
      <td>4.335000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>17.896939</td>
      <td>22.537766</td>
      <td>22.226330</td>
      <td>21.908333</td>
    </tr>
    <tr>
      <th>max</th>
      <td>369.079773</td>
      <td>350.456573</td>
      <td>338.299600</td>
      <td>341.098000</td>
    </tr>
  </tbody>
</table>
</div>

<p>By considering the median for the absolute errors for each model, we can see that those of the machine learing models are smaller than that from the benchmark model. This implies that for at least half of the dataset, the absolute error from the machine learing models are smaller than that from the benchmark.
However, if we consider other quantitle, we can see that there are not much different. But for most part, the machine learning models have smaller errors.</p>

<h3 id="stock-price-versus-predicted-call-price">Stock Price versus Predicted Call Price</h3>

<p>By the RMSE on the testing set, we choose the CNN to be the best model.
We visualize $C_0$ as a function of strike price $K$ and the stock price $S_0$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res_pt</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">[[</span><span class="s">'t0'</span><span class="p">]].</span><span class="n">set_index</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))).</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s">"t0"</span><span class="p">:</span><span class="s">"stock price"</span><span class="p">}),</span>
                    <span class="n">X_test</span><span class="p">[[</span><span class="s">'K'</span><span class="p">]].</span><span class="n">set_index</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))).</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s">"K"</span><span class="p">:</span><span class="s">"K"</span><span class="p">}),</span>
            <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"Call Price from the dataset"</span><span class="p">:</span> <span class="n">y_test</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">().</span><span class="n">ravel</span><span class="p">()}),</span>
           <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"Call Price from Binomial Tree"</span><span class="p">:</span> <span class="n">bm_list</span><span class="p">}),</span>
           <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"Call Price from CNN"</span><span class="p">:</span> <span class="n">model_conv</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="n">ravel</span><span class="p">()}),</span>
          <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"lower bound"</span><span class="p">:</span>
              <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">([</span><span class="n">lb</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">lb</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">[</span><span class="s">'t0'</span><span class="p">]</span> <span class="o">-</span> <span class="n">X_test</span><span class="p">[</span><span class="s">'K'</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X_test</span><span class="p">[</span><span class="s">'r'</span><span class="p">]</span> <span class="o">*</span><span class="n">X_test</span><span class="p">[</span><span class="s">'T'</span><span class="p">])]})],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>485/485 [==============================] - 1s 1ms/step
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Percentage of call price greater than 120
</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">120</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.06970595821511479
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">21</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'projection'</span><span class="p">:</span> <span class="s">'3d'</span><span class="p">})</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">scatter</span><span class="p">(</span><span class="n">res_pt</span><span class="p">[</span><span class="s">'stock price'</span><span class="p">],</span> <span class="n">res_pt</span><span class="p">[</span><span class="s">"K"</span><span class="p">],</span><span class="n">res_pt</span><span class="p">[</span><span class="s">"Call Price from the dataset"</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$S_0$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$K$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"$C_0$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">145</span><span class="p">,</span> <span class="n">roll</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"True"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_zlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">320</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">scatter</span><span class="p">(</span><span class="n">res_pt</span><span class="p">[</span><span class="s">'stock price'</span><span class="p">],</span> <span class="n">res_pt</span><span class="p">[</span><span class="s">"K"</span><span class="p">],</span><span class="n">res_pt</span><span class="p">[</span><span class="s">"Call Price from Binomial Tree"</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$S_0$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$K$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"$C_0 $"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">145</span><span class="p">,</span> <span class="n">roll</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Binomial Tree"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_zlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">320</span><span class="p">])</span>


<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">scatter</span><span class="p">(</span><span class="n">res_pt</span><span class="p">[</span><span class="s">'stock price'</span><span class="p">],</span> <span class="n">res_pt</span><span class="p">[</span><span class="s">"K"</span><span class="p">],</span><span class="n">res_pt</span><span class="p">[</span><span class="s">"Call Price from CNN"</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$S_0$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$K$"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"$C_0 $"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"CNN"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">145</span><span class="p">,</span> <span class="n">roll</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">set_zlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">320</span><span class="p">])</span>


<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/image/output_178_0.png" alt="png" /></p>

<p>By considering only two factors, $S_0$ and $K$, we can see some paterrns in the corresponding option price. All the three plots look similar, indicating the accuracy of the binomial tree and the Convolutional Neural Network to price options. However, in the convolutional neural network, we can see that the call prices are dense around the small values similar to the actual price, however it did not well capture high call prices. For the benchmark, the prices are more sparse compared to both actual price and the CNN.</p>

<p>Now, we visualize an option price as a function of stock price, fixing other variable.</p>

<p>We fix other variable by choosing those variables from the dataset.</p>

<p>Then, we compare an option price as a function of $S_0$ from Binomial Tree and the best model.</p>

<p>Remark that, the machine learning models require additional parameters which are stock prices 8 days in the past. We cannot fix these stock prices; otherwise, the underlying price is unrealistic (eg. stock price at 0 is 500, while those at -1, -2, are around 200).</p>

<p>For each $S_0$, we find relevant $S_{-1}, S_{-2}$, …, in the dataset.</p>

<p>We also compute a upperbound and lowerbound as shown below.</p>

<p>The upper and lower bound for American option (for non-dividend) can be computed by
\(\max(S_t - K e^{-r (T-t)},0)\leq C_t \leq S_t\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Choose only S_0_x (rounded) that we use to train model
</span><span class="n">S_0_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">spy_use</span><span class="p">[</span><span class="s">'t0'</span><span class="p">])</span> <span class="p">)</span>
<span class="n">binomial_vs_s0</span> <span class="o">=</span> <span class="p">[</span><span class="n">american_call_price</span><span class="p">(</span><span class="n">xs0</span><span class="p">,</span> <span class="n">sample_fixed</span><span class="p">[</span><span class="s">'K'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sample_fixed</span><span class="p">[</span><span class="s">'hist_vol'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">t</span> <span class="o">=</span> <span class="n">sample_fixed</span><span class="p">[</span><span class="s">'T'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span> <span class="o">=</span> <span class="n">sample_fixed</span><span class="p">[</span><span class="s">'r'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">q</span> <span class="o">=</span> <span class="n">sample_fixed</span><span class="p">[</span><span class="s">'q'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">30</span> <span class="p">)</span> <span class="k">for</span> <span class="n">xs0</span> <span class="ow">in</span> <span class="n">S_0_x</span> <span class="p">]</span>
<span class="n">spy_extended</span> <span class="o">=</span> <span class="n">spy_use</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">spy_extended</span><span class="p">[</span><span class="s">'round_t0'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">spy_use</span><span class="p">[</span><span class="s">'t0'</span><span class="p">])</span>
<span class="n">s0_dataset</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">xs0</span> <span class="ow">in</span> <span class="n">S_0_x</span><span class="p">:</span>
    <span class="n">s0_dataset</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">sample_fixed</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">"call_last"</span><span class="p">,</span><span class="s">'t0'</span><span class="p">]).</span><span class="n">to_numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">5</span><span class="p">],</span> <span class="n">spy_extended</span><span class="p">[</span><span class="n">spy_extended</span><span class="p">[</span><span class="s">'round_t0'</span><span class="p">]</span> <span class="o">==</span> <span class="n">xs0</span><span class="p">].</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">9</span><span class="p">].</span><span class="n">ravel</span><span class="p">()]))</span>
<span class="n">s0_dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">s0_dataset</span><span class="p">)</span>
<span class="n">cnn_vs_s0</span> <span class="o">=</span> <span class="n">model_conv</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s0_dataset</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>7/7 [==============================] - 0s 1ms/step
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_bound</span> <span class="o">=</span> <span class="n">S_0_x</span>
<span class="n">min_bound</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">([</span><span class="n">xs0</span> <span class="o">-</span> <span class="n">sample_fixed</span><span class="p">[</span><span class="s">'K'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">sample_fixed</span><span class="p">[</span><span class="s">'r'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">sample_fixed</span><span class="p">[</span><span class="s">'T'</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">xs0</span> <span class="ow">in</span> <span class="n">S_0_x</span> <span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">S_0_x</span> <span class="p">,</span><span class="n">binomial_vs_s0</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">'binomial'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">S_0_x</span> <span class="p">,</span><span class="n">cnn_vs_s0</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">'CNN'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">S_0_x</span> <span class="p">,</span><span class="n">max_bound</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">'max bound'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">S_0_x</span> <span class="p">,</span><span class="n">min_bound</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">'min bound'</span><span class="p">,</span><span class="n">ls</span> <span class="o">=</span> <span class="s">'dashed'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7f844f12e160&gt;
</code></pre></div></div>

<p><img src="/assets/image/output_184_1.png" alt="png" /></p>

<p>The plot aligns with the previous plot, suggesting consistency. However, it appears that the CNN model does not accurately capture the high call prices. The generated call prices are observed to be outside the lower bound for high underlying price, creating the potential for arbitrage opportunities. Therefore, it is crucial to exercise caution and conduct a thorough examination of the model before implementing it. Additionally, fine-tuning or training the CNN model with a larger and more comprehensive dataset could potentially improve its performance and yield a better model.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this project, we have utilized machine learning models to price American options based on the SPY dataset. Our findings indicate that all of the machine learning models outperform the traditional binomial model for both the testing and training sets. Among these models, CNN, random forest, and KNN show promise, as their testing and training losses are relatively low.</p>

<p>However, it is worth noting that even though CNN performs the best overall, it struggles with accurately computing high call prices. Therefore, further tuning and dataset preparation might be necessary to enhance its performance in this regard.</p>

<p>In conclusion, machine learning models demonstrate the ability to capture the relationship between financial information and option pricing. Through the utilization of the RMSE metric, these models outperform the traditional benchmark model. Nonetheless, additional tunin</p>

<h2 id="acknoledgement">Acknoledgement</h2>

<p>Thanks all people who gathered the data and publicly publish them online. Thank you Dr. Kevin Lu for giving suggestions on American option pricing models.</p>

<h2 id="references">References</h2>

<p>Culkin, Robert, and Sanjiv R. Das. Machine Learning in Finance: The Case of Deep Learning for Option Pricing, 2 Aug. 2017, srdas.github.io/Papers/BlackScholesNN.pdf.</p>

<p>French, Kenneth R. Kenneth R. French - Data Library, mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html. Accessed 3 June 2023.</p>

<p>Graupe, Kyle. “$spy Option Chains - Q1 2020 - Q4 2022.” Kaggle, 14 Mar. 2023,
www.kaggle.com/datasets/kylegraupe/spy-daily-eod-options-quotes-2020-2022.</p>

<p>Lu, Kevin. “Machine Learning for Finance Lectures.” CFRM 421:Machine Learning for Finance . Seattle, The University of Washington.</p>

<p>Lu, Kevin. “Introduction to Financial Markets Lectures.” CFRM 415: Introduction to Financial Markets. Seattle, The University of Washington.</p>

<p>Mooney, Kevin, director. Implementing the Binomial Option Pricing Model in Python, YouTube, 15 Feb. 2021, https://www.youtube.com/watch?v=d7wa16RNRCI. Accessed 5 June 2023.</p>

<p>SPY Dividend Yield, ycharts.com/companies/SPY/dividend_yield. Accessed 3 June 2023.</p>

  </div>

  <div class="date">
    <i><span style="color: #b8b8b8;">&nbsp;Posted on  &nbsp;June  6, 2023</span> </i>
  </div>
  <div class="disclaimer-note">
      Please note that this blog post has not been peer-reviewed. If you have any concerns or suggestions, please do not hesitate to contact me.
  </div>
</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer"><div class="social-icons"><a class="social-icon" href="https://www.linkedin.com/in/wanchaloem-omzin-wunkaew/"><i class="fa fa-linkedin fa-2x" title="Linkedin"></i></a><a class="social-icon" href="https://www.facebook.com/people.live.in.the.world.of.agree/"><i class="fa fa-facebook fa-2x" title="Facebook"></i></a><a class="social-icon" href="https://www.instagram.com/__middleman/"><i class="fa fa-instagram fa-2x" title="Instagram"></i></a><a class="social-icon" href="https://github.com/middleOz"><i class="fa fa-github-square fa-2x" title="GitHub"></i></a></div><div class="copyright">
            
            <p>&copy; 2023 W Wunkaew. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/forever-jekyll/forever-jekyll" rel="nofollow">Forever Jekyll</a>.</p>
            
          </div>
        </footer>
      </div>
    </div>

    <script defer src="https://cdn.plyr.io/3.6.5/plyr.js"></script>
    <script>
      const player = new Plyr("#player", { noCookie: true, rel: 0, showinfo: 0, iv_load_policy: 3, modestbranding: 1, playsinline: 1 , enablejsapi: 1, autoplay: 0, focused: true, global: false });
      window.player = player;
    </script>

    <script src="https://cdn.jsdelivr.net/gh/mcstudios/glightbox/dist/js/glightbox.min.js"></script>
    <script type="text/javascript">
      const lightbox = GLightbox({
          touchNavigation: true,
          loop: true,
          autoplayVideos: true
      });
    </script>

    <script defer src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js" integrity="sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body,{
                    delimiters: [
                    { left: '$$',  right: '$$',  display: true  },
                    { left: '$',   right: '$',   display: false },
                    { left: '\\[', right: '\\]', display: true  },
                    { left: '\\(', right: '\\)', display: false }
                    ]});">
    </script>

    <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
    <script>
      function addDarkmodeWidget() {
        new Darkmode().showWidget();
        window.addEventListener('load', addDarkmodeWidget);
      }
      const options = {
        label: '🌓',
      }
      const darkmode = new Darkmode(options);
            darkmode.showWidget();
    </script>

  </body>
</html>
